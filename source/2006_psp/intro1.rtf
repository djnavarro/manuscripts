{\rtf1\ansi \deflang1033\deff0{\fonttbl
{\f0\froman \fcharset0 \fprq2 Times New Roman;}}{\colortbl;\red0\green0\blue0;\red255\green255\blue0;}
{\stylesheet{\fs20 \snext0 Normal;}
{\s1 \qj QuickFormat1;}
}\notabind\margl1440\margr1440\hyphhotz936\ftnbj\fet2\ftnrstpg\aftnnar\viewkind1\lytprtmet\subfontbysize \sectd \sbknone\pgndec\headery1440\footery1440\endnhere\endnhere 
{\header {
\posxr\nowrap {\field{\*\fldinst { PAGE  }}}\par}
\par}
{\*\pnseclvl1\pndec\pnstart1{\pntxta .}}
{\*\pnseclvl2\pnlcltr\pnstart1{\pntxta .}}
{\*\pnseclvl3\pnlcrm\pnstart1{\pntxta .}}
{\*\pnseclvl4\pndec\pnstart1{\pntxtb (}{\pntxta )}}
{\*\pnseclvl5\pnlcltr\pnstart1{\pntxtb (}{\pntxta )}}
{\*\pnseclvl6\pnlcrm\pnstart1{\pntxtb (}{\pntxta )}}
{\*\pnseclvl7\pndec\pnstart1{\pntxta .}}
{\*\pnseclvl8\pnlcltr\pnstart1{\pntxta .}}
{\*\pnseclvl9\pnlcrm\pnstart1}

{\field{\*\fldinst {\lang4105  SEQ CHAPTER \\h \\r 1}}{\fldrslt }}\pard \fs24
Imagine you are a medical doctor. How comfortable would you be prescribing a newly developed \softline
medication for which there was minimal knowledge of how it works or of its side effects? An \softline
analogous situation exists today in the psychological sciences in modeling human behavior. \softline
Quantitative modeling has evolved into an influential and increasingly popular research tool and \softline
method of inquiry, yet our understanding of model behavior is too often quite limited (e.g., why \softline
the model behaves in a particular way and the range of other behaviors it exhibits). In fact, model \softline
behavior can be down right mysterious (Dawson & Shamanski, 1994; McCloskey, 1991). This \softline
observation is not a criticism about models or modeling, but a comment about the absence of \softline
methods for studying them.\par
\par
Modelers need to understand the consequences of their design choices in model construction. \softline
They also need to be able to make well-informed decisions when choosing between competing \softline
models. The computational power and precision of quantitative models of human behavior \softline
requires correspondingly sophisticated analysis tools. The purpose of this paper is to introduce \softline
and demonstrate such a tool.\par
\par
{\plain \fs24 \b Current Methods of Model Analysis}{\plain \fs24 \par
}{\plain \fs24 Much of present-day model evaluation and comparison assess what we have recently dubbed \softline
Local Model Analysis (LMA; Navarro, Pitt, & Myung, in press). The term refers to evaluation in \softline
a very specific context The most frequent form of LMA is testing model performance against \softline
human performance, for example, by fitting a mathematical model to the data or simulating the \softline
human pattern with a connectionist model. Because data are a reflection of the psychological \softline
process underlie study, a good fit to the data is a necessary condition a model must satisfy to be \softline
taken seriously. A good fit determines how well a model passes the sufficiency test of mimicking  \softline
human performance. It is especially useful in the early stages of model development as a quick \softline
and easy check on sufficiency. Quantitative measures of fit include percent variance accounted \softline
for, root mean square deviation, and maximum likelihood (). \par
}{\plain \fs24 \par
}{\plain \fs24 As with any quantitative measure, it is important to be aware of the limitations of goodness-of-fit \softline
(GOF) measures. For example, they can be biased when assumptions are violated (normality in \softline
the case of root mean square deviation). What we can learn from them is also limited. A good fit \softline
makes a model a member of the class of possible contenders. The problem is that this class will \softline
almost always be large. If two models that one is comparing fit the data similarly well, other \softline
analysis methods are needed to choose between them.\par
}{\plain \fs24 \par
}{\plain \fs24 Another LMA method that is useful for probing model behavior more deeply is a sensitivity \softline
analysis, in which a model{\u8217\'92}s parameters are varied around its best-fitting values to learn how \softline
robust model behavior is to slight variations in those parameters. If a good fit reflects a \softline
fundamental property of the model, then this behavior should be stable across reasonable \softline
variation in the relevant parameter(s). Another reason a model should satisfy this criterion is that \softline
human data are noisy. A model should not be so sensitive that its behavior changes noticeably \softline
when noise is encountered. Cross validation, in which a model is fit to the second of two data \softline
sets using the best fitting parameter values from fitting the first data set, is a fit-based approach to 
quantifying this sensitivity.\par
}{\plain \fs24 \par
}{\plain \fs24 A drawback of all LMA methods is that they are local.  For example, each fit provides a view of \softline
model performance in a specific experimental setting or (e.g., design). Each view is also always \softline
in relation to how humans perform, which not only restricts what we learn about model \softline
performance but also makes it difficult to get a sense of overall model behavior when these views \softline
are combined , let alone the similarities and differences between models, For these reasons, we \softline
have been interested in developing Global Model Analysis (GMA) techniques. They are intended \softline
to complement LMA, not replace it. The idea is that by stepping back from a particular data \softline
sample and obtaining a broader view of how a model performs, we can gain a deeper \softline
understanding of model behavior and how it compares with competing models. \par
}{\plain \fs24 \par
}{\plain \fs24 One GMA measure is a model{\u8217\'92}s complexity, which quantifies its inherent data-fitting ability in \softline
an experimental setting (Grunwald et al, in press; Pitt, Myung, & Zhang, 2002; Rissanen, 1996; \softline
2001).  The method is mathematically rigorous, as will be elaborated later, but a few limitations \softline
restrict its application to the diverse range of models in psychology. One is that it is currently \softline
only applicable to statistical (parametric, algebraically formulated) models. The other is that it \softline
generates only a single value. Although a useful starting point for exploration, by itself the \softline
measure cannot provide enough information about model behavior to meet our needs.\par
}{\plain \fs24 \par
}{\plain \fs24 Recently, Navarro et al (in press; Navarro et al, 2003; Pitt & Navarro, in press; see also \softline
Wagenmakers et al, 2004) developed a GMA method called }{\plain \fs24 \i Landscaping}{\plain \fs24  that attempts to fulfill \softline
this goal while still maintaining ties with the complexity measure. A landscape is a plot of the \softline
relative fits of two models to a range of data sets generated by those models in a particular \softline
experimental design. Two examples are shown in Figure 1. The diagonal line represents the \softline
location of equal fit by the two models, with the distribution of points above the line indicating \softline
better fits by model A. The reverse is true of points below the line. The high degree of overlap of \softline
the distributions in the left panel indicates the models fit all data sets equally well. The separation \softline
of the distributions in the plot on the right indicates the models could be discriminable, most \softline
decisively if data were generated in the regions furthest from the criterion line, where one model \softline
would provide a far superior fit than the other.\par
}{\plain \fs24 \par
}{\plain \fs24 Landscapes are relatively easy to create. In addition to learning about the types of data that can \softline
distinguish models, data collected in past studies can be overplotted on a landscape to assess their \softline
informativeness in distinguishing models. In short, the landscape provides a broader context in \softline
which to understand the relationship between two models and their fits to data.\par
}{\plain \fs24 \par
}\sect \sectd \sbknone\pgndec\headery1440\footery1440\endnhere\endnhere 
{\header {
\posxr\nowrap \plain \fs24 {\field{\*\fldinst { PAGE  }}}\par}
\par}
{\*\pnseclvl1\pndec\pnstart1{\pntxta .}}
{\*\pnseclvl2\pnlcltr\pnstart1{\pntxta .}}
{\*\pnseclvl3\pnlcrm\pnstart1{\pntxta .}}
{\*\pnseclvl4\pndec\pnstart1{\pntxtb (}{\pntxta )}}
{\*\pnseclvl5\pnlcltr\pnstart1{\pntxtb (}{\pntxta )}}
{\*\pnseclvl6\pnlcrm\pnstart1{\pntxtb (}{\pntxta )}}
{\*\pnseclvl7\pndec\pnstart1{\pntxta .}}
{\*\pnseclvl8\pnlcltr\pnstart1{\pntxta .}}
{\*\pnseclvl9\pnlcrm\pnstart1}

\pard \fs24
{\plain \fs24 Landscaping has two drawbacks, however. Like the complexity measure mentioned above, it too \softline
is restricted to statistical models. More importantly, comparison of fits cannot tell us much about \softline
the functional relationship between models. Are they virtually isomorphic in a particular \softline
experimental setup, generating the same number and range of data patterns,  or are they distinct,  \softline
with only a small subset of overlapping patterns between them? Goodness-of-fit measures might \softline
be relatively insensitive to these two situations. Even if such differences were detectable by fit 
}{\plain \fs24 quality, the reason for the differences could not be understood or taken advantage of without \softline
dissecting them further.\par
}{\plain \fs24 \par
}{\plain \fs24  Exactly how to perform such analyses to address these issues is a nontrivial problem precisely \softline
because most models are complex, with many parameters that combine nonlinearly. We \softline
introduce a general-purpose version of landscaping that overcomes these two remaining hurdles \softline
and achieves this goal. It is called }{\plain \fs24 \i Parameter Space Partitioning }{\plain \fs24 (PSP), and involves doing \softline
exactly what the name implies: A model{\u8217\'92}s parameter space is literally partitioned into regions \softline
that correspond to the data patterns that could be generated in an experiment. These partitions \softline
can be studied to learn about a model{\u8217\'92}s behavior and compared across models to assess their \softline
similarities.\par
}{\plain \fs24 \par
}{\plain \fs24 \b Parameter space partitioning:}{\plain \fs24  }{\plain \fs24 \b Peering into the black box of model behavior}{\plain \fs24 \par
}{\plain \fs24 To illustrate PSP approach, consider a visual word recognition experiment in which participants \softline
are asked to categorize stimuli as words or  nonwords and response time to words is measured as \softline
the dependent variable across  three experimental conditions, A, B and C. Suppose we are \softline
interested in the ordinal relationship (fastest to slowest) across conditions. In this case, there are \softline
13 possible orderings (including equalities) that can be observed across the three conditions (e.g., \softline
A > B > C, A > B = C, B > C = A, etc). Each of these orderings defines a data pattern. Suppose \softline
that mean participant performance yielded the pattern B > C > A. \par
}{\plain \fs24 \par
}{\plain \fs24 Now consider two hypothetical models, M}{\plain \fs24 \sub 1}{\plain \fs24  and M}{\plain \fs24 \sub 2}{\plain \fs24 , of word recognition, each with two \softline
parameters. Using PSP, we can answer the following questions about the relationship between \softline
the models and the human data generated in the experiment: How many of the thirteen data \softline
patterns each model can produce? What part of the parameter space includes the human pattern, \softline
how much of the space is occupied by the human pattern? What data patterns are found in the \softline
rest of the parameter space?\par
}{\plain \fs24 \par
}{\plain \fs24 Figure 2 shows the parameter space of each model partitioned into the data patterns it can \softline
generate. Model M}{\plain \fs24 \sub 1}{\plain \fs24  produces three, one of which is the }{\plain \fs24 human pattern. Note how the human \softline
pattern is central to the model, occupying the largest portion of the parameter space. Even though \softline
the model generates two other patterns, they differ minimally from the human pattern. In \softline
contrast, M}{\plain \fs24 \sub 2}{\plain \fs24  can produce nine of the thirteen patterns. Although one is the human pattern, M}{\plain \fs24 \sub 2}{\plain \fs24 's \softline
performance is not impressive because it can mimic almost any pattern that can be observed in \softline
the experimental design; its predictive power is too great. Indeed, that M}{\plain \fs24 \sub 2}{\plain \fs24  mimics human \softline
performance seems almost incidental because the region of the parameter space that corresponds \softline
to the human pattern is small and some of the larger regions are produced by patterns that are \softline
unlike human data (e.g., C>A>B).\par
}{\plain \fs24 \par
}{\plain \fs24 \ul Challenges in Implementing PSP}{\plain \fs24 \par
}{\plain \fs24 The preceding example illustrates the gist of the method and describes some of what can be \softline
learned with it. Implementation of the method requires solving two non-trivial problems. One is \softline
how to define a data pattern, and the other is how to devise an efficient search algorithm to find 
the data patterns. Our solutions are described next.\par
}{\plain \fs24  \ul0 \tab \ul0 \tab \ul0 \tab \ul0 \tab \ul0 \tab \ul0 \tab \ul0 \tab \ul0 \tab \ul0 \tab \par
}{\plain \fs24 Each set of a model{\u8217\'92}s parameter values generates a model output, but not every model output is a \softline
distinct data pattern. How many qualitatively {\u8220\'93}different{\u8221\'94} outputs can a model produce? \softline
Answering this question, which is what PSP enables us to do, depends critically on how a data \softline
pattern is defined. This is something which will vary from experiment to experiment, and indeed \softline
may vary within an experiment depending upon what a researcher wants to learn. Although there \softline
is no general solution, the experimentalist usually has a very good idea about what is going on in \softline
their data and what patterns should be found in order to confirm or disconfirm a model. Most of \softline
the time, one is testing ordinal predictions. In this case, a {\u8220\'93}natural{\u8221\'94} definition of a data pattern is \softline
the order relationship of model outputs, as in the above example.   Nevertheless, it is a good idea \softline
to try out a couple of different definitions and to perform sensitivity analyses to ascertain if and \softline
to what extent conclusions obtained under one definition hold across others. An example of this \softline
is presented in the comparison of TRACE and Merge.\par
}{\plain \fs24 \par
}{\plain \fs24 Once a data pattern is defined, the next challenge is to find all patterns a model can simulate. The \softline
data space by definition is made up of all patterns that can be observed in an experiment, \softline
however improbable. The space may contain a huge number of patterns, only a small fraction of \softline
which correspond to a model{\u8217\'92}s predictions, so it is essential to use an efficient search algorithm \softline
that finds all of the patterns the model can generate in a reasonable amount of time.\par
}{\plain \fs24 \par
}{\plain \fs24 \ul The PSP Algorithm}{\plain \fs24 \par
}{\plain \fs24 The search problem to be solved is to partition a multi-dimensional parameter space into an \softline
unknown number of regions, each of which by definition corresponds to a unique data pattern. \softline
Given the dimensionality of the parameter space, brute force methods such as Simple Monte \softline
Carlo (SMC; a random search procedure) will not work. For each parameter set, we run the \softline
model, see what pattern it produces and keep track of only those that are unique. However, SMC \softline
is inefficient precisely because the search is random, not guided. Markov Chain Monte Carlo \softline
(MCMC; Gilk et al, 1996) is a much more powerful sampling method that we encorporated into \softline
an algorithm that efficiently finds all of the regions.\par
}{\plain \fs24 \par
}\pard \fs24\s1 
{\plain \fs24 Application of the algorithm  begins with a set of parameter values at which the model can \softline
generate a valid data pattern. This initial set can be supplied by the modeler or from an \softline
exploratory run using SMC. Given the parameter set and the corresponding data pattern \softline
generated by the model, the algorithm samples nearby points in the parameter space to map the \softline
region that defines the data pattern. MCMC is }{\plain \fs24 used to approximate quickly and accurately the \softline
shape of this region. \par
}\pard \fs24
{\plain \fs24 \par
}{\plain \fs24 Figure 3 illustrates how the algorithm works in the space of a two-parameter model.  The process \softline
begins with the initial parameter set serving as the current point in the parameter space (filled \softline
point in panel a). A }{\plain \fs24 \i candidate}{\plain \fs24  sample point (open circle) is drawn from a small, predefined \softline
region, called a  jumping distribution, centered at the current point.  The model is then run with \softline
the candidate parameter values and its output is evaluated to determine if the data pattern is the 
same as that generated by the initial point. If so, the candidate point is accepted as the next point \softline
from which another candidate point is drawn. If the new candidate point does not yield the same \softline
data pattern as the initial one, it is rejected as belonging to the current region. Another jump from \softline
the initial point is attempted, accepting those points that yield the same data pattern. The \softline
sequence of all accepted points recorded across all trials is called the }{\plain \fs24 \i Markov chain \softline
}{\plain \fs24 corresponding to the current data pattern. This sample of points is used to estimate the size of the \softline
region occupied by the data pattern (panel b). The theory of MCMC guarantees that the sample of \softline
accepted points will eventually be distributed uniformly over the region. This feature of MCMC \softline
allows us to estimate accurately the volume occupied by the region, regardless of its size. \par
}{\plain \fs24 \par
}{\plain \fs24 Every rejected point, which must be outside the current region, is checked to see if it generates a \softline
new valid data pattern. If so, a new Markov chain corresponding to the newly discovered pattern \softline
is started to define this new region. In effect, accepted points are used to shift the jumping \softline
distribution around inside the current region to map it completely, whereas rejected ones are used \softline
to discover new regions. Over time, as many search processes as there are unique data patterns \softline
(probable or not) will be run. Additional details about the algorithm are described in Appendix \softline
???. [Insert fuller description as well as the Table detailing the steps in the Appendix]\par
}{\plain \fs24 \par
}{\plain \fs24 \b Algorithm Evaluation}{\plain \fs24 \par
}{\plain \fs24 We tested the accuracy of the algorithm by measuring its ability to find all of the data patterns in \softline
a model that was simple enough for us to specify and manipulate their frequency and structure. \softline
The efficiency of the PSP algorithm was measured by comparing it to SMC (random search). \par
}{\plain \fs24 \par
}{\plain \fs24 The model was a hypercube whose dimensionality }{\plain \fs24 \i d}{\plain \fs24  (i.e., number of parameters) was 5, 10, or \softline
15.To illustrate the evaluation method, a two-dimensional model (d = 2) is depicted in Figure 4 \softline
that contains twenty data regions (outlined in bold) that the algorithm had to find. Note that a \softline
large portion of the space does not produce any valid data patterns. Also note that the sizes of the \softline
data regions vary a great deal. Some are elicited by a wide range of parameter values whereas \softline
others can be produced only by a small ranges of values. This contrast grows exponentially as the \softline
dimensionality of the model increases, and was purposefully introduced into the test to make the \softline
search difficult and approximate the complexities of nonlinear models.\par
}{\plain \fs24 \par
}{\plain \fs24 Figure 5 shows performance of the PSP algorithm for a search problem in which there were one \softline
hundred regions embedded in a 10-dimensional hypercube (d = 10). The two curves are based on \softline
the average of 10 independent replications }{\plain \fs24 \highlight2 [Woojae: Figure should include standard deviations \softline
every 30 seconds.] }{\plain \fs24 The PSP algorithm found all the regions and did so in nine minutes. SMC \softline
found only about 23 patterns in nine minutes, and given its sluggish performance, it seems \softline
doubtful that SMC would find all of them in a reasonable amount of time.\par
}{\plain \fs24 \par
}{\plain \fs24 Table 1 summarizes results from a series of tests comparing the two search processes, PSP and \softline
SMC. The dimensionality of the model was extend to 15 to make it comparable to models found \softline
in cognitive science. The number of data patterns that had to be found was deliberately made \softline
large (20 - 500) to make the test of the algorithm comprehensive and challenging. In addition, ten 
independent runs of each search method were carried out to assess the reliability of algorithm \softline
performance.\par
}{\plain \fs24 \par
}{\plain \fs24 The mean proportion of patterns found is listed in each cell. The results are clear and consistent. \softline
The PSP algorithm almost always found all of the patterns whereas SMC failed to do so in every \softline
condition. Most noteworthy is the success of the PSP algorithm (and its contrast with SMC) \softline
when there were many parameters and data patterns (lower right corner). The near perfect \softline
success of the PSP algorithm suggests it is likely to perform admirably in other testing situations. \softline
In the remainder of this paper we describe its applications to models in the fields of category \softline
learning and speech perception.\par
}{\plain \fs24 \par
}{\plain \fs24 \b Evaluating Alcove}{\plain \fs24 \par
}{\plain \fs24 Need to clearly describe what we mean and do not mean by {\u8220\'93}experimental design{\u8221\'94}\par
}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 \b Comparing localist connectionist models of phoneme perception}{\plain \fs24 \par
}{\plain \fs24 In addition to learning about the behavior of a single model, PSP analyses can be used to \softline
compare models. As mentioned in the introduction, PSP is applicable to a varieties of \softline
computational models (e.g., mathematical, connectionist). In this section, we demonstrate its \softline
application to comparing two localist connectionist models. The purpose of the comparison was \softline
to learn the extent to which slight architectural differences between the models make them \softline
functionally different. Connectionist models were also chosen to show how PSP analyses can \softline
help make inroads into understanding the behaviors of highly nonlinear models that results from \softline
the interconnectedness among their parts. The localist variety was chosen because of its \softline
popularity in some content areas (e.g., language; Grainger & Jacobs, 1998) and because we had \softline
previously worked with them (Brunsman, Myung, & Pitt, 1999).\par
}{\plain \fs24 \par
}{\plain \fs24 TRACE (McClelland & Elman, 1986) and Merge (Norris, McQueen, & Cutler, 2001), two \softline
models of phoneme perception, were compared. Schematic diagrams of the fundamental design \softline
properties of each are shown in Figure ???. They are similar in many ways. Both have a \softline
phonemic input stage and a word stage. There are excitatory connections from the phoneme input \softline
to word stages, and inhibitory connections within the word stage. They differ in how prior \softline
knowledge is combined with phonemic input to yield a phonemic percept. In TRACE, word \softline
(prior) knowledge can directly influence sensory processing of  phonemes. This is represented by \softline
direct excitatory connections from the word back down to the phoneme stage. Note also that this \softline
means the  phonemic input stage also functions as a phoneme decision stage. In Merge, these two \softline
duties are purposefully separated into two stages to prevent word information from affecting \softline
phoneme registration. Instead, word knowledge affects phoneme decision making via \softline
connections from the word to phoneme-decision stage. In contrast to the direct interaction \softline
between phoneme and word levels in TRACE, these two sources of information are integrated at \softline
this additional phoneme decision stage in Merge.\par
}{\plain \fs24 \par
}{\plain \fs24 Although not visible in the diagram in Figure ???, the models differ in another important way. In 
keeping with the belief that bottom-up information takes priority, activation of a phoneme \softline
decision node in Merge must be initiated by phoneme input before lexical input can influence \softline
decision making. TRACE contains no such constraint.\par
}{\plain \fs24 \par
}{\plain \fs24 A goal of our investigation was to assess the impact of these two design differences (architectural \softline
and processing) on }{\plain \fs24 \i global }{\plain \fs24 model behavior. [FN: Because the debate over lexical feedback has at \softline
times become heated, we want to be clear that this study was not undertaken to favor one model \softline
or argue for superiority of one architecture. We are interested only in their behavioral \softline
similarities.] Norris et al (2001) proposed Merge as an alternative to TRACE because they felt \softline
the evidence from the experimental literature did not warrant direct word-to-phoneme feedback. \softline
Integration of phoneme and word information at a later, decision stage is, in their view, more in \softline
line with the data. The adequacy of the Merge architecture was demonstrated in simulation tests \softline
in which it performed just as well as, if not slightly better than, TRACE in reproducing key \softline
experimental findings of how word knowledge affects phoneme processing.  PSP analyses of the \softline
models{\u8217\'92} behavior were carried out in two of these experimental settings. In the first, the \softline
subcategorical mismatch study by }{\plain \fs24 Marslen-Wilson and  Warren (1994)}{\plain \fs24 , the consequences of \softline
splitting  phoneme processing into separate input and decision stages was evaluated. In the \softline
second, the indirect inhibition experiment by Frauenfelder, Segui, and Dijkstra (1990), the \softline
contribution of the bottom-up priority rule was examined.\par
}{\plain \fs24 \par
}{\plain \fs24 To compare the two models, it was necessary to equate them in every way possible to ensure that \softline
differences in performance were attributable only to design differences, not other factors, such as \softline
the size of the lexicon. In essence, we wanted to compare the fundamental structural and \softline
functional properties that define the models, nothing else. We did this by first implementing the \softline
version of Merge described in Norris et al (2001), and then making the necessary changes to \softline
Merge to turn it into TRACE (Norris et al essentially did this as well). When finished, the source \softline
code for the two models, written in Matlab, was identical except for the sections that \softline
corresponded to their design differences. TRACE required ??? fewer parameters than Merge (??? \softline
vs. ???) because the phoneme input and decision stages were combined. The names of the \softline
parameters, along with other model details, are in Appendix ???. \par
}{\plain \fs24 \par
}{\plain \fs24 \ul One phoneme stage vs. two}{\plain \fs24 \par
}{\plain \fs24 The first comparison of the two models was performed in the context of the subcategorical \softline
mismatch experiment of Marslen-Wilson and Warren (1994, Experiment 1; McQueen, Norris, & \softline
Cutler, 1999, Experiment 3).  It was chosen because of the large number of conditions and the \softline
variety of response alternatives, which together permitted detailed analyses of model behavior at \softline
both the phonemic and lexical levels.\par
}{\plain \fs24 \par
}{\plain \fs24 In the experiment, listeners heard one-syllable utterances and then had to classify them as words \softline
or nonwords (lexical decision task) or categorize the final phoneme (phonetic decision task). The \softline
stimuli were made by appending a phoneme (e.g., /b/) that was excised from the end of a word \softline
(e.g.,}{\plain \fs24 \i  job}{\plain \fs24 ) or nonword (e.g., }{\plain \fs24 \i smob}{\plain \fs24 ) to three preceding contexts, to yield six experimental \softline
conditions (listed in Table ???). The first context was a new token of those same items but with 
the final consonant removed (e.g., }{\plain \fs24 \i jo }{\plain \fs24 and }{\plain \fs24 \i smo}{\plain \fs24 ), to create cross-spliced versions of  }{\plain \fs24 \i job }{\plain \fs24 and }{\plain \fs24 \i smob}{\plain \fs24 . \softline
The second consisted of equivalent stretches of speech from two other words that differed only in \softline
the final consonant (e.g,, }{\plain \fs24 \i jo }{\plain \fs24 from }{\plain \fs24 \i jog }{\plain \fs24 and }{\plain \fs24 \i smo }{\plain \fs24 from }{\plain \fs24 \i smog}{\plain \fs24 ). The third was the same as the second \softline
except that the initial parts were excised from nonwords (e.g., }{\plain \fs24 \i jo }{\plain \fs24 from }{\plain \fs24 \i jod }{\plain \fs24 and }{\plain \fs24 \i smo }{\plain \fs24 from }{\plain \fs24 \i smod}{\plain \fs24 ). \par
}{\plain \fs24 \par
}{\plain \fs24 Because cues to phoneme identity overlap in time (due to coarticulation), a consequence of cross-splicing is that cues to the identity of the final consonant will conflict when the first word ends in \softline
a consonant different from the second. For example,  }{\plain \fs24 \i jo }{\plain \fs24 from }{\plain \fs24 \i jog}{\plain \fs24  contains cues to /g/ at the end \softline
of the vowel, which will be present in the resulting stimulus when combined with the /b/ from \softline
}{\plain \fs24 \i job}{\plain \fs24  (W2W1 condition in Table ???).}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 Marslen-Wilson and Warren (1994) were interested in how such stimuli affect phoneme and \softline
lexical processing. They found that in the lexical decision task, reaction times slowed when \softline
listeners heard cross-spliced stimuli, but responding was not affected by these conflicting cues \softline
(responses in the W2W1 and N3W1 conditions were equivalent).  Phoneme categorization, in \softline
contrast, was sensitive to the subtle variation in phonetic detail, but only when the stimulus itself \softline
formed a nonword (e.g., }{\plain \fs24 \i smob}{\plain \fs24 ).\par
}{\plain \fs24 \par
}{\plain \fs24 Merge can simulate this complex pattern of results. TRACE can as well if the number of \softline
processing cycles is increased from one to 15 (Norris et al, 2001). Just as it did with Alcove, a \softline
PSP analysis will determine what other data patterns these models can generate in the particular \softline
experimental setup. To the extent that the patterns differ across models, they are more than likely \softline
a result of the structural differences between them (one phoneme stage vs two). The use of cross-spliced phonemes nullifies any effect of the bottom-up priority rule, which could otherwise \softline
further differentiates Merge from TRACE (see Norris et al, 2001, for details).\par
}{\plain \fs24 \par
}{\plain \fs24 In sum, we analyzed the data generated from partitioning each model{\u8217\'92}s parameter space in the \softline
experimental design. By comparing how many data patterns the models can produce, how similar \softline
these patterns are across models and to the human data pattern, and the centrality of the human \softline
data in model performance, we can assess the similarities of the models and the consequences of \softline
splitting the phoneme level in two. \par
}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 \ul Details of PSP analysis}{\plain \fs24 \par
}{\plain \fs24 Define data pattern. Mention how many there are.\par
}{\plain \fs24 Explain purpose of and define use of weak and strong constraints. \par
}{\plain \fs24 State how long (in hours) the searches took.\par
}{\plain \fs24 \par
}{\plain \fs24 \ul Results and Discussion}{\plain \fs24 \par
}{\plain \fs24 We began by analyzing classification phoneme and lexical performance, followed by analysis of \softline
the RT patterns.\par
}{\plain \fs24 \par
}{\plain \fs24 Must explain the purpose of each analysis. Why is it being performed? What will it tell us?  This 
is especially important because it is the first time they are introduced.\par
}{\plain \fs24 \par
}{\plain \fs24 complexity - pattern count\par
}{\plain \fs24 volume ratios (}{\plain \fs24 \i shared }{\plain \fs24 vs unique; magnitude of human pattern)\par
}{\plain \fs24 volume correlations (shared only; ranking of human pattern, size+frequency of other patterns)\par
}{\plain \fs24 mismatch (error) distributions (similarity of competing patterns with human pattern)\par
}{\plain \fs24 mismatch vs volume correlations \par
}{\plain \fs24 micro analysis of mismatches (types of errors) - consistency analysis of errors.\par
}{\plain \fs24 RT analysis (consistency of ordinal predictions between conditions)\par
}{\plain \fs24 Parameter analyses\par
}{\plain \fs24 \par
}{\plain \fs24 \ul Bottom-up priority}{\plain \fs24 \par
}{\plain \fs24 This second comparison of TRACE and Merge was performed to determine how the addition \softline
of a bottom-up priority rule in Merge distinguishes it from TRACE. Recall that the two models \softline
differ in the sources of information that can initiate phoneme decision making. In Merge, for a \softline
phoneme decision node to become activated (Figure ???), excitation must be initiated from the \softline
phoneme input stage (i.e., evidence for the phoneme must be in the acoustic signal). In TRACE, \softline
this is not required. Initial activation via top-down connections from the word stage is possible in \softline
some circumstances. For example, having been presented with }{\plain \fs24 \i jo}{\plain \fs24  in }{\plain \fs24 \i job}{\plain \fs24 , excitation from the }{\plain \fs24 \i job \softline
}{\plain \fs24 word node will feedback and excite the }{\plain \fs24 \i b}{\plain \fs24  phoneme node. Interconnectivity between phoneme \softline
nodes results in indirect word-to-phoneme inhibition, because the activated }{\plain \fs24 \i b}{\plain \fs24  node will in turn \softline
inhibit competing phoneme nodes (e.g.,  }{\plain \fs24 \i d, g)}{\plain \fs24 . Thus, word nodes have the ability to excite \softline
directly and inhibit indirectly phoneme nodes.\par
}{\plain \fs24 \par
}{\plain \fs24 Frauenfelder et al (1990, Experiment ???) tested TRACE{\u8217\'92}s prediction of indirect word-to-phoneme inhibition.  described above. Listeners had to monitor for phonemes that occurred late \softline
in multisyllabic words and pseudowords. Of interest was whether reaction times to the target \softline
phoneme would slow when presented stimuli that should caused indirect phoneme inhibition. \softline
RTs were compared across three conditions. A word condition (e.g., }{\plain \fs24 \i cabine}{\plain \fs24 \i\ul t}{\plain \fs24 , with }{\plain \fs24 \i t}{\plain \fs24  as the target \softline
phoneme) served as a lower baseline in which lexical and phonemic information should combine \softline
to yield fast RTs. A control pseudoword condition (e.g., }{\plain \fs24 \i vabine}{\plain \fs24 \i\ul l}{\plain \fs24 , with }{\plain \fs24 \i l }{\plain \fs24 as the target phoneme) \softline
served as a likely upper limit on RTs. Because }{\plain \fs24 \i vabinel }{\plain \fs24 is not a word, there should be no top-down lexical facilitation or inhibition when responding to /l/; responses should be based on \softline
sensory input alone. In the third, inhibitory condition, listeners heard pseudowords like }{\plain \fs24 \i cabinel}{\plain \fs24 , \softline
with }{\plain \fs24 \i l }{\plain \fs24 as the target phoneme. A slowdown in RTs relative to the word condition is expected if \softline
there is in fact inhibition. This is because the first part of the stimulus, }{\plain \fs24 \i cabine}{\plain \fs24  will activate \softline
}{\plain \fs24 \i cabinet}{\plain \fs24 , whose activation should then feed back down and excite /t/, which will then inhibit /l/.\par
}{\plain \fs24 \par
}{\plain \fs24 The RT slowdown in the inhibition condition was small and not reliable, which argues against \softline
word-to-phoneme excitation in TRACE. However, in simulations of indirect inhibition, \softline
TRACE{\u8217\'92}s behavior was not cut and dry, with inhibition being more likely with longer than \softline
shorter words (Norris et al, 2001}{\plain \fs24 ).  In contrast, the bottom-up priority rule in Merge ensured that \softline
it produced consistent simulation esults that never yielded inhibition.\par
}{\plain \fs24 A PSP analysis was performed on TRACE and Merge in the Frauenfelder et al (1990) \softline
experimental setup to develop a broader understanding of how the priority rule distinguishes \softline
Merge from TRACE{\u8217\'92}s. TRACE{\u8217\'92}s variable behavior may be a sign that it can produce more data \softline
patterns than Merge. If this is the case, is the priority rule the reason why model behavior is \softline
constrained? Or is it also due to the structural differences between the models?\par
}{\plain \fs24 \par
}{\plain \fs24 To answer these question, we ran PSP analyses on both models with and without the priority rule \softline
in what yieded a 2x2 factorial design, shown in Table ???.  The lower left and upper right cells \softline
represent the models as originally formulated. This comparison serves as a reference from which \softline
to understand the contribution of the rule and the models{\u8217\'92} structures in affecting behavior. \softline
Comparisons of results between columns (i.e., models) neutralizes the effects of the priority rule. \softline
If the rule is primarily responsible for differences in model behavior, then the results for both \softline
models should be quite similar when the rule is and is not operational. If differences still remain \softline
in this circumstance, then their different architectures are also contributing to their different \softline
behaviors. In short, these analyses will tell us whether Merge, without its priority rule, behaves \softline
like TRACE, and whether TRACE, with the priority rule,  behaves like MERGE. \par
}{\plain \fs24 \par
}{\plain \fs24 \ul Details of PSP implementation}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 Mention the experimental design is much simpler than the subcat exp, so there were fewer \softline
patterns overall.\par
}{\plain \fs24 \par
}{\plain \fs24 Mention that we included a lexical level response in the simulations because humans perceive the \softline
items as a word or pwords, even though participants made no lexical decision response. \softline
Imposition of this requirement narrows the parameter space because of the lexical restriction. \softline
This additional constraint on model performance should increase the opportunity to find \softline
differences between them because the region containing human data will be smaller. (Without \softline
this constraint, do the model perform identically? i.e., same # of patterns?)\par
}{\plain \fs24 \par
}{\plain \fs24 State how long (in hours) the search took. Discuss reliability of results if search were run again.\par
}{\plain \fs24 \par
}{\plain \fs24 Need to mention this somewhere:\par
}{\plain \fs24 Even though the words were seven phonemes in length, they were not long enough to generate \softline
enough phoneme-level inhibition in TRACE to make it produce the wrong pattern of results. It \softline
still was able to produce the human RT pattern, which is why the models mimic each other so \softline
closely (To address the inconsistency, remind readers that Norris et al did not find indirect \softline
inhibition with short words, only with longer words. Is this with weak constraints only, not \softline
strong constraints?)\par
}{\plain \fs24 \par
}{\plain \fs24 \ul Results}{\plain \fs24 \par
}{\plain \fs24 # of TRACE patterns shrinks across thresholds. # of Merge patterns drops only minimally. This \softline
is what was found in the subcat analysis.\par
}{\plain \fs24 \par
}{\plain \fs24 Is the reason that both TRACE and Merge produced the human pattern is because the strings \softline
were not long enough to produce inhibition in TRACE?\par
}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 \par
}{\plain \fs24 model equivalence in this testing situation (recognition threshold dependent).\par
}{\plain \fs24 \par
}{\plain \fs24 \b General Discussion}{\plain \fs24 \par
}{\plain \fs24 In the discussion, point out that PSP analyses like this can us determine the generality of a \softline
particular behavior. In the case of TRACE and Merge, findings generated in the past have been \softline
shown not to generalize fully to new situations, be they new words or a scaled-up version of the \softline
model. PSP analyses can assess the robustness of findings, and thereby provide researchers with \softline
a clearer understanding of what a model can and cannot do, identifying incidental as well as \softline
central patterns of the model.\par
}{\plain \fs24 \par
}}