{\rtf1\ansi\deff0\adeflang1025
{\fonttbl{\f0\froman\fprq2\fcharset0 Times New Roman;}{\f1\froman\fprq2\fcharset0 Times New Roman;}{\f2\froman\fprq2\fcharset0 Palatino Linotype;}{\f3\fnil\fprq0\fcharset2 StarSymbol;}{\f4\fnil\fprq2\fcharset0 Lucida Sans Unicode;}{\f5\fmodern\fprq1\fcharset0 Courier New;}{\f6\froman\fprq2\fcharset0 Times New Roman Greek;}{\f7\fnil\fprq2\fcharset0 Tahoma;}{\f8\fnil\fprq0\fcharset0 Tahoma;}}
{\colortbl;\red0\green0\blue0;\red128\green128\blue128;}
{\stylesheet{\s1\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs24\lang255\ltrch\dbch\af4\afs24\langfe255\loch\f0\fs24\lang1033\snext1 Default;}
{\s2\sa120\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs24\lang255\ltrch\dbch\af4\afs24\langfe255\loch\f0\fs24\lang1033\sbasedon1\snext2 Text body;}
{\s3\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af8\afs24\lang255\ltrch\dbch\af4\afs24\langfe255\loch\f0\fs24\lang1033\sbasedon2\snext3 List;}
{\s4\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs24\lang255\ltrch\dbch\af4\afs24\langfe255\loch\f0\fs24\lang1033\sbasedon2\snext4 Table Contents;}
{\s5\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs24\lang255\ab\ltrch\dbch\af4\afs24\langfe255\ab\loch\f0\fs24\lang1033\b\sbasedon4\snext5 Table Heading;}
{\s6\sb120\sa120\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af8\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f0\fs20\lang1033\i\sbasedon1\snext6 Caption;}
{\s7\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af8\afs24\lang255\ltrch\dbch\af4\afs24\langfe255\loch\f0\fs24\lang1033\sbasedon1\snext7 Index;}
{\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033\sbasedon1\snext8 Normal;}
{\s9\cf0\qj{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs24\lang255\ltrch\dbch\af4\afs24\langfe255\loch\f0\fs24\lang1033\sbasedon8\snext8 QuickFormat1;}
{\*\cs11\cf0\rtlch\af7\afs24\lang255\ltrch\dbch\af4\afs24\langfe255\loch\f0\fs24\lang1033 Numbering Symbols;}
{\*\cs12\cf0\rtlch\af3\afs18\lang255\ltrch\dbch\af3\afs18\langfe255\loch\f3\fs18\lang1033 Bullets;}
}
{\info{\creatim\yr2004\mo8\dy11\hr13\min8}{\revtim\yr2004\mo8\dy13\hr13\min55}{\printim\yr2004\mo8\dy13\hr13\min27}{\comment StarWriter}{\vern6450}}\deftab709
{\*\pgdsctbl
{\pgdsc0\pgdscuse195\pgwsxn12240\pghsxn15840\marglsxn1134\margrsxn1134\margtsxn1134\margbsxn1134\pgdscnxt0 Default;}}
\paperh15840\paperw12240\margl1134\margr1134\margt1134\margb1134\sectd\sbknone\pgwsxn12240\pghsxn15840\marglsxn1134\margrsxn1134\margtsxn1134\margbsxn1134\ftnbj\ftnstart1\ftnrstcont\ftnnar\aenddoc\aftnrstcont\aftnstart1\aftnnrlc
\pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Quantitative modeling has evolved into an influential and increasingly popular research tool and method of inquiry, yet our understanding of model behavior is frequently quite limited. It is often not clear why a model behaves in a particular way, and the 
full range of its behavior is rarely known. In fact, model behavior can be downright mysterious (Dawson & Shamanski, 1994; McCloskey, 1991). This observation is not a criticism about models or modeling, but a comment about the absence of methods for studyi
ng them. Nevertheless, modeling will be most fruitful when the consequences of a modeler\rquote s choices are understood. Similarly, a thorough understanding of model behavior is necessary in order to compare models to one another in a meaningful way. For the mos
t part, however, current model analysis tools do not provide this kind of insight. In this paper, we aim to help fill this gap by introducing a powerful, general-purpose tool for model analysis and selection.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b\b Methods of Model Analysis}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\fs20\f2 In surveying the state of model evaluation tools, there are two useful distinctions to make, between local and global analyses on the one hand, and between quantitative and qualitative analyses on the other. These two dimensions are largely independent, as
 illustrated schematically in Figure 1. We will use these distinctions to structure our discussion. We begin with a discussion of quantitative methods.}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Quantitative Model Analysis}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 When one performs a statistical analysis, one is almost always referring to a quantitative analysis. However, a quantitative method may be either local or global. Most model evaluations are local, in the sense that they consider only the behavior of a mode
l at the best-fitting parameter values, or in a small neighborhood around those parameters. In contrast, global methods aim to account for the model's behavior across the full range of its parameter values. Not surprisingly, the two approaches are compleme
ntary.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Local Quantitative Methods}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The most common form of local analysis is {\i model fitting,} in which a model is tested by seeing how closely it can approximate human data. Because data are a reflection of the psychological process under study, a good fit to the data is a necessary condition
 a model must satisfy to be taken seriously. A good fit determines how well a model passes the sufficiency test of mimicking human performance. It is especially useful in the early stages of model development as a quick and easy check on sufficiency. Quant
itative measures of fit include percent variance accounted for, root mean square deviation, and maximum likelihood. These measures can be very meaningful, but it is important to be aware of their limitations. For example, they can be inaccurate when assump
tions are violated (normality in the case of root mean square deviation). Although a good fit makes a model a member of the class of possible contenders, this class will almost always be quite large. If two models that one is comparing fit the data similar
ly well, other analysis methods are needed to choose between them.}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Another local method that is useful for probing model behavior more deeply is a {\i sensitivity analysis,} in which a model\rquote s parameters are varied around its best-fitting values to learn how robust model behavior is to slight variations of those parameters. If
 a good fit reflects a fundamental property of the model, then this behavior should be stable across reasonable parameter variation. Another reason a model should satisfy this criterion is that human data are noisy. A model should not be so sensitive that 
its behavior changes noticeably when noise is encountered.{\i  Cross validation}, in which a model is fit to the second of two data sets using the best fitting parameter values from fitting the first data set, is a fit-based approach to quantifying this sensiti
vity.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Global Quantitative Methods}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The drawback to local methods is the very fact that they are local. Each fit provides a view of the model's behavior at a particular point in its parameter space, but does not provide any information about how it behaves at the other parameter values. This
 can be particularly concerning if the behavior of the model is only sensible at a few points. Using only purely local methods leaves us only with a few \ldblquote snapshots\rdblquote  of model performance that are diffciut to piece together into a comprehensive understanding
 of the model. The task of comparing two models is even more arduous.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 For these reasons, we have been interested in developing {\i0 global analysis t}echniques. They are intended to complement local methods, not replace them. Under a global view, the goal is to learn something about the full range of a model's behavior. By doing s
o, we can gain a deeper understanding of model behavior and how it compares to competing models. Two of the most popular global methods are {\i Bayesian methods} (e.g., Myung & Pitt 1997; Kass & Raftery 1995) and {\i minimum description length} (MDL; e.g., Rissanen 
1996, 2001; Gr\'fcnwald 1998; Gr\'fcnwald, Myung, & Pitt, in press). In both cases, the important aspect of the method is to look at the predictions made by the model at all of its parameter values.  One thing that comes out of this perspective is the importance
 of a model's {\i complexity}, which refers to its inherent data-fitting ability (Myung 2000; Pitt, Myung, & Zhang, 2002). Bayesian methods and MDL are statistically rigorous, but a few requirements currently limit its application to the diverse range of models
 in psychology. One of the major limitations is that they assume, as do all quantitative methods, that the goal of modelling is to be able to approximate data or predict future events. As we will shortly discuss, this can be problematic in many psychologic
al applications. }
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 As a final note on global quantitative methods, Navarro et al (2004; Kim, Navarro, Pitt, & Myung, in press; Pitt & Navarro, in press; see also Wagenmakers et al, 2004) recently developed a global method called {\i landscaping. }{\i0 The essence of the landscaping id
ea is to see how well two models fit each other's data. As such, it is meant to measure the extent to which two models can mimic one another. The approach is attractive since }landscapes are relatively easy to create, requiring only a comparison of fits to 
data sets. In addition, landscapes can be used to assess the informativeness of data in distinguishing models by overplotting in a landscape data collected in past studies. In short, the landscape provides a global perspective from which to understand the 
relationship between two models and their fits to data. However, like MDL and Bayesian methods, landscaping makes the assumption that the models are intended to make quantitative predictions.}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\b0\ul\fs20\f2 Qualitative Methods}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\b0 As indicated previously, one serious drawback to the approaches discussed in the previous section is that they generally assume that the goal of modeling is to approximate empirical data as closely as possible. They are all {\i quantitative} methods. The proble
m with doing so is that one can get caught up in modeling the quantitiative minutiae of a data set while missing some theoretically important qualitative property. In the words of Box (1976, p. 792) \ldblquote {\cf1 Since all models are wrong the scientist must be alert t
o what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad\rdblquote . Therefore, we may be interested in the {\i qualitative} behavior of the model. }}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033
\par \pard\plain \ltrpar\s8\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 \ldblquote Yes or No\rdblquote : A Local Qualitative Method }
\par \pard\plain \ltrpar\s1\cf1{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af5\afs20\lang255\ltrch\dbch\af5\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 A method of model evaluation used quite frequently in practice is to look for theoretically relevant trends in the data, and see if a particular model can reproduce them. We refer to this as the {\i qualitative yes-no} method, and it is implicitly used in a gre
at many empirical investigations. One might, for instance, observe a preference reversal under some experimental conditions that only one model is able to reproduce. This is generally held to be strong evidence in favor of the model, and not without reason
. Psychological models are often intended to be illustrative of some underlying process rather than a precise description. Accordingly, the failure of a model to reproduce the \ldblquote fine grain\rdblquote  of a data set is not necessarily fatal to the theory on which the m
odel is based. It may merely indicate that some minor changes are required. As far as we are aware, no researcher discards a theory purely because of minor failings. On the other hand, if a model cannot capture the gross qualitative structure of the data, 
something is seriously amiss with the model, and perhaps with the theory as well. The yes-no method, therefore, is to look for important qualitative properties that can only be captured by one model. }
\par 
\par \pard\plain \ltrpar\s1\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Global Qualitative Methods}
\par \pard\plain \ltrpar\s1\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The yes-no method gets around the difficulty posed by quantitative methods, by relaxing the requirement that the model needs to capture {\i all} of the structure in the data. The problem is that it is also a local method, and so reintroduces some of the difficu
lties that go with local methods. In particular, there is the concern that a model might be able to produce all of the qualitative patterns that a particular experimental design could possibly elicit. In other words, there is an implicit problem pertaining
 to what we might call \ldblquote qualitative complexity\rdblquote .  The solution to the problem is the same as with quantitative methods: look at the behavior of the model across a broad range of parameter values. }
\par 
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\fs20\f2 The purpose of the current paper is to discuss a method for global qualitative analysis that we call{\i  parameter space partitioning}. Nevertheless, it is not the only method by which one can do global qualitative analysis. {\b0 Dunn and James\rquote  (2003) {\i signed differ
ence analysis} is another example, in which one seeks to identify all of the ordinal relationships between task performance measurements (e.g., response time, percent correct) a model allows by varying its parameter values. It offer a simple means of testin
g models based solely on their qualitative characteristics, but in its current state of development, it is based on the somewhat unrealistic premise of error-free measurement. However, while the method clearly requires some work, signed difference analysis
 seems to hold great promise.}}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ab\ltrch\dbch\af4\afs20\langfe255\ab\loch\f2\fs20\lang1033\b 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ab\ltrch\dbch\af4\afs20\langfe255\ab\loch\f2\fs20\lang1033\b {\loch\f2\fs20\lang1033\i0\b Parameter Space Partitioning: A Global Qualititative Method}
\par \pard\plain \ltrpar\s8\cf1{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af5\afs20\lang255\ltrch\dbch\af5\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The idea of looking for qualitative structure is appealing because it allows us to think about model predictions at the same level of granularity as experimental data. Most data sets that psychologists deal with are influenced by so many factors that it is
 often difficult to model them very precisely. As a result, we generally settle for a model that \ldblquote looks right\rdblquote : that is, a model that produces the right qualitative behavior. While this is probably unavoidable in many situations, it does raise the importan
t question of what it means to {\i0 capture} qualitative structure. There are two elements to this question. The first regards how to {\i define} a qualitative structure, while the second regards the notion of {\i capturing} such a structure.}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033
\par \pard\plain \ltrpar\s8\cf1{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af5\afs20\lang255\ltrch\dbch\af5\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Defining a qualitative structure is a complicated problem, and one that directly relates to the question of what one is interested in. Accordingly, there are no general answers to the question, since interest will vary from data set to data set, and indeed
 may vary within a data set depending on what a researcher hopes to learn from it. Although there is no general solution, the scientist usually knows what patterns should be found in order to support or falsify a model. Most of the time, ordinal prediction
s are being tested, though this need not always be true. We will discuss this in a little more detail shortly.}
\par 
\par \pard\plain \ltrpar\s1\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\cf1 The second element is the main focus of this paper. Assuming that we have an acceptable definition of a qualitative pattern, what does it mean for a model to have captured such a pattern? Clearly, if a model does not produce that pattern at any of it's par
ameter values, then it has not captured that pattern. On the other hand, a model that can produce any logically-possible pattern is not very impressive either. Overall, what we want to do is find all the patterns that a model can produce. We refer to this 
process as  {\i parameter space partitioning }(PSP), and it involves doing exactly what the name implies: A model\rquote s parameter space is literally partitioned into regions that correspond to the data patterns that the model could generated in a particular experim
ent. These partitions can then be studied to learn about a model\rquote s behavior and compared across models to assess their similarities. }
\par \pard\plain \ltrpar\s1\cf1{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af5\afs20\lang255\ltrch\dbch\af5\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s1\cf1{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af5\afs20\lang255\ltrch\dbch\af5\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 To provide a more concrete illustration of the idea, consider a visual word recognition experiment in which participants are asked to categorize stimuli as words or nonwords. The mean response time to words is then measured as the dependent variable across
  three experimental conditions, {\i A}, {\ulnone\i B} and {\i C}. In this situation, it may be reasonable to claim that the important theoretical property is the ordinal relationship (fastest to slowest) across conditions. In this case, there are 13 possible orderings (includi
ng equalities) that can be observed across the three conditions (e.g., {\i A > B > C}{\i0 ,}{\i  A > B = C}{\i0 ,}{\i  B > C = A, }etc). Each of these orderings defines a{\i  qualitative data pattern}. Suppose further that mean participant performance yielded the pattern {\i B > C > A}. }
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Now consider two hypothetical models,{\i  M}{{\*\updnprop5800}\dn5 1} and {\i M}{{\*\updnprop5800}\dn5 2}, of word recognition, each with two parameters. Using PSP, we can answer the following questions about the relationship between the models and the empirical data generated in the experiment: How many of the t
hirteen data patterns can each model produce? What part of the parameter space includes the empirical pattern? How much of the space is occupied by the empirical pattern? What data patterns are found in nearby regions as well as the rest of the parameter s
pace?}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Figure 2 shows the parameter space of each model partitioned into the data patterns it can generate. Model {\i M}{{\*\updnprop5800}\dn5 1} produces three, one of which is the emipirical pattern. Note how it is central to model performance, occupying the largest portion of the paramete
r space. Even though the model generates two other patterns, they are smaller and differ only minimally from the empirical pattern. In contrast, {\i M}{{\*\updnprop5800}\dn5 2} produces nine of the thirteen patterns. Although one is the empirical pattern, {\i M}{{\*\updnprop5800}\dn5 2}'s performance is not impre
ssive because it can mimic almost any pattern that could possibly have been observed in the experiment. Indeed, the fact that {\i M}{{\*\updnprop5800}\dn5 2} can mimic human performance seems almost incidental. Not only does the empirical pattern occupy a small region of the parameter
 space but larger regions are produced by patterns that do not human-like (e.g., {\i C>A>B}).}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\fs20\f2\cf1{\ulnone As the previous discussion indicates, PSP is essentially an extension of the yes-no method. Instead of looking to see if the model captures an empirically observed pattern, we look at all of the patterns that a model could potentially capture, to see if th
ey tell us anything useful. Of course, this idea of looking for patterns across the whole parameter space has been employed frequently in previous research.  For example, }}{\loch\f2\fs20\lang1033\fs20\f5\fs20\f5\fs20\f2\cf1Johansen and Palmeri (2002) used a grid search to look for predictions made by catego
rization models, as did Lee and Navarro (2002). The PSP method simply systematizes the idea.}
\par \pard\plain \ltrpar\s1\cf1{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af5\afs20\lang255\ltrch\dbch\af5\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Implementing PSP}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\fs20\f2 The preceding example illustrates the gist of the method and describes some of what can be learned with it. Implementation of the method requires solving two non-trivial problems. As mentioned above, the first problem is how to define a data pattern, and t
here is no general answer to this question. {\ulnone\cf1 Each set of a model\rquote s parameter values generates a model output, but not every model output is a distinct data pattern. How many qualitatively \ldblquote different\rdblquote  outputs can a model produce? Answering this question, whi
ch is what PSP enables us to do, depends critically on how a data pattern is defined. This is something which will vary from experiment to experiment, and indeed may vary within an experiment depending upon what a researcher wants to learn. In some cases, 
a \ldblquote natural\rdblquote  definition of a data pattern is an ordinal relationship between model outputs, as in the above example. Nevertheless, it is a good idea to try out a couple of different definitions and to perform sensitivity analyses to ascertain if and to what
 extent conclusions obtained under one definition hold across others. An example of this is presented later in the paper.}}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The Search Algorithm}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\ulnone Once a data pattern is defined, the next challenge is to find all patterns a model can simulate. The data space by definition is made up of all patterns that can be observed given an experimental design, however improbable. The space may contain a huge num
ber of patterns, only a small fraction of which correspond to a model\rquote s predictions, so it is essential to use an efficient search algorithm that finds all of the patterns the model can generate in a reasonable amount of time.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0{\ulnone The set of all data-patterns forms a partition on the parameter space, in the sense that each point in the space can correpond to only one pattern. When we come up with some definition of a data pattern, we have in effect created an unknown partition on th
e parameter space. Accordingly, the problem to be solved is to search a multi-dimensional parameter space in such a way that we visit each part of the partition at least once}}{\loch\f2\fs20\lang1033. As the number of parameters in a model increases, the space becomes higher dimen
sional, and the search problem can become very hard. The consequence of this is that brute force search methods to find all regions, such as Simple Monte Carlo (SMC; a random search procedure) will not work, or take far too long to succeed, precisely becau
se the search is random. Markov Chain Monte Carlo (MCMC; Gilk et al, 1996) is a much more sophisticated sampling method that we encorporated into an algorithm that efficiently finds all regions.}
\par 
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Application of the PSP algorithm begins with a starting set of parameter values at which the model can generate a valid data pattern (i.e., one that satisfies the definition). This initial set can be supplied by the modeler or from an exploratory run using
 SMC. Given the parameter set and the corresponding data pattern generated by the model, the algorithm samples nearby points in the parameter space to map the region that defines the data pattern. MCMC is used to approximate quickly and accurately the shap
e of this region. The process then begins anew by sampling a nearby point just outside of this region.}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Figure 3 illustrates how the algorithm works in the space of a two-parameter model. The process begins with the initial parameter set serving as the current point in the parameter space (solid point in panel a). A {\i candidate} sample point (shaded point) is d
rawn from a small, predefined region, called a  jumping distribution, centered at the current point. The model is then run with the candidate parameter values and its output is evaluated to determine if the data pattern is the same as that generated by the
 initial point. If so, the candidate point is accepted as the next point from which another candidate point is drawn. If the new candidate point does not yield the same data pattern as the initial one, it is rejected as belonging to the current region. Ano
ther jump from the initial point is attempted, accepting those points that yield the same data pattern. The sequence of all accepted points recorded across all trials is called the {\i Markov chain }corresponding to the current data pattern. This sample of poin
ts is used to estimate the size of the region occupied by the data pattern (panel b). The theory of MCMC guarantees that the sample of accepted points will eventually be distributed uniformly over the region. This feature of MCMC allows us to estimate accu
rately the volume occupied by the region, regardless of its size. }
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 3 here}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Every rejected point (shaded point in panel b), which must be outside the current region, is checked to see if it generates a new valid data pattern. If so, a new Markov chain corresponding to the newly discovered pattern is started to define the new regio
n. In effect, accepted points are used to shift the jumping distribution around inside the current region to map it completely, whereas rejected ones are used initiate new MCMC search processes to map new regions. Over time, as many search processes as the
re are unique data patterns will be run. Additional details about the algorithm are described in Appendix A.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Algorithm Evaluation}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 We tested the accuracy of the PSP algorithm by measuring its ability to find all of the data patterns defined for a particular model. The difficulty of the search problem was varied by manipulating the definition of a pattern (i.e., number of patterns) and
 the number of parameters in the model. The extent of both (see Table 1) was deliberately made large to make the test  challenging The efficiency of the algorithm was measured by comparing its performance to SMC (random search).}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 The model was a hypercube whose dimensionality {\i d} (i.e., number of parameters) was 5, 10, or 15. To illustrate the evaluation method, a two-dimensional model ({\i d}=2) is depicted in Figure 4 that contains twenty data regions (outlined in bold) that the algorit
hm had to find. Note that a large portion of the space does not produce any valid data patterns. Also note that the sizes of the data regions vary a great deal. Some are elicited by a wide range of parameter values whereas others can be produced only by a 
small ranges of values. This contrast grows exponentially as the dimensionality of the model increases, and was purposefully introduced into the test to make the search difficult and approximate the complexities (i.e., nonlinearities) in cognitive models. 
Ten independent runs of each search method were carried out to assess the reliability of algorithm performance. }
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figures 4 and 5 here}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Figure 5 shows performance of the PSP algorithm for a search problem in which there were one hundred regions embedded in a 10-dimensional hypercube ({\i d}=10). The PSP algorithm found all regions and did so in nine minutes. SMC found only about 23 patterns in 
nine minutes, and given its sluggish performance, it seems doubtful that SMC would find all of them in anything close to a reasonable amount of time.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Table 1 summarizes results from the complete test. The mean proportion of patterns found is listed in each cell. Results are clear and consistent. The PSP algorithm almost always found all of the patterns whereas SMC failed to do so in every condition. Mos
t noteworthy is the success of the PSP algorithm in the toughest situation, when there were 15 parameters and 500 data patterns. Its near perfect success suggests it is likely to perform admirably in other testing situations. In the remainder of this paper
, we describe its applications to analyzing performance of a single model and to comparing design differences between models.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Table 1 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ab\ltrch\dbch\af4\afs20\langfe255\ab\loch\f2\fs20\lang1033\b {\loch\f2\fs20\lang1033\i0\b Application 1: Evaluating the Qualitative Performance of ALCOVE}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 In standard model-fitting analyses, a model\rquote s ability to fit (or simulate) the data is taken as evidence that it approximates the underlying cognitive process. In a PSP analysis, the definition of \ldblquote fit\rdblquote  is relaxed to be a qualitative, ordinal relation on t
he same scale as the experimental predictions themselves. Model performance is then evaluated by determining how many of the possible orderings in the experiment can it produce and and how central is the empirical pattern among them (Figure 2). In this sec
tion, we examined the behavior of ALCOVE (Kruschke 1992), a highly successful exemplar-based account of human category learning, in the context of the seminal Shepard, Hovland, and Jenkins (1961) experiment. While there are some category learning effects t
hat it does not capture without extension or modification (e.g, Kruschke & Erikson 1995, Lee & Navarro 2002), ALCOVE remains a simple and powerful account of a broad range of phenomena.}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Background to the Analysis}
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The ALCOVE  Model}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 In some category learning experiments, participants are shown a sequence of stimuli, each of which possesses some unknown category label. The task is to learn which labels go with which stimuli, using the feedback provided after responses are made. ALCOVE 
solves this problem in the following way (for a detailed description, see Kruschke 1992). When stimulus {\i i} is presented to ALCOVE, its similarity to each of the previously stored exemplars, {\i s{{\*\updnprop5801}\dn8 ij}}, is calculated. Following Shepard (1987), similarity is assumed
 to decay exponentially (with a width parameter {\i c}) as a function of the attention-weighted city-block distance between the two stimuli in an appropriate psychological space. After estimating these similarities, ALCOVE forms response strengths for each of t
he possible categories. These are calculated using associative weights maintained between each of the stimuli and the categories. The probability of choosing the {\i k}-th category follows the choice rule (Luce, 1963) with parameter {\i \u966 ?}.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Having produced probabilities for each of the various possible categorization responses, ALCOVE is provided with feedback from an external source. This takes the form of a \ldblquote humble teacher\rdblquote  vector, in which learning is only required in cases where the wrong
 response was made.  Two learning rules are then applied, both derived by seeking to minimize the sum-squared error between the response strengths and the teaching vector, using a simple gradient descent approach to optimization. Using these rules, ALCOVE 
updates the associative weights (with parameter {\i{ \u951 ?}}{\i\i\i{\field{\*\fldinst ADVANCE \\d 3}{\fldrslt }}{{\*\updnprop5801}\dn8 w}{\field{\*\fldinst ADVANCE \\u 3}{\fldrslt }} }for the learning rate) and the attention weights (with a learning rate parameter {\i{ \u951 ?}}{\i\i\i{\field{\*\fldinst ADVANCE \\d 3}{\fldrslt }}{{\*\updnprop5801}\dn8 a}{\field{\*\fldinst ADVANCE \\u 3}{\fldrslt }}}) prior to observing the next stimulus.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The Shepard, Hovland and Jenkins Task}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 In a classic experiment, Shepard, Hovland and Jenkins (1961) studied human performance in a category learning task involving eight stimuli divided evenly between two categories. The stimuli were generated by varying exhaustively three binary dimensions suc
h as color (black vs. white), size (small vs. large) and shape (square vs. triangle). They observed that, if these dimensions are regarded as interchangeable, there are only six possible category structures across the stimulus set, illustrated in Figure 6a
. This means, for example, that the category structure that divides all squares into one category, and all triangles into the other is regarded as equivalent to the category structure that divides small shapes from large ones, as shown in the lower right.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Empirically, Shepard et al. (1961) found robust differences in the way in which each of the six fundamental category structures was learned. In particular, by measuring the mean number of errors made by subjects in learning each type of category structure,
 they found that Type I was learned more easily than Type II, which in turn was learned more easily than Types III, IV and V (which all had similar error measures), and that Type VI was the most difficult to learn.  More recently, Nosofsky, Gluck, Palmeri,
 McKinley and Glauthier (1994), replicated Shepard et al.'s (1961) task using many more subjects, and reported detailed information relating to the learning curves.  Figure 6b shows the mean proportion of errors for each category type. Consistent with the 
conclusions originally drawn by Shepard et al. (1961), it is generally held that the theoretically important qualitative trend in these data is the finding that there is a natural ordering on these curves, namely that I<II<(III, IV, V)<VI. This kind of pat
tern is called a weak order, since the possibility of ties is allowed.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 6 here}
\par \pard\plain \ltrpar\s1\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The psychological importance of this weak order structure is substantial. Suppose we had two models of the category learning process, {\i M{{\*\updnprop5801}\dn8 1}} and {\i M{{\*\updnprop5801}\dn8 2}}, of roughly equal complexity. {\i  M{{\*\updnprop5801}\dn8 1} }provides a reasonably good quantitative fit to the data, by assuming that all 
types are learned at the same rate, and closely approximates the average of the six empirical curves. In contrast, {\i M{{\*\updnprop5801}\dn8 2}} reproduces the ordering I<II<(III, IV, V)<VI, but learns far too slowly and, as a result, fits the data much worse than {\i M{{\*\updnprop5801}\dn8 1} }. Since the mod
els are of equivalent complexity, a classical model selection analysis would prefer model {\i M{{\*\updnprop5801}\dn8 1}}. However, while clearly both models have some flaws, most psychologists would prefer {\i M{{\*\updnprop5801}\dn8 2}}, because it captures the{\i  theoretically relevant }property of the data. This 
discrepancy arises because statistical criteria tend to assume that all properties of the data are equally relevant. In many psychological applications, this is not the case. In what follows, we assume that the weak order structure is the important theoret
ical property of the empirical data, and use the PSP method to ask how effectively ALCOVE captures this structure.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The PSP Analysis}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 As with any parameterized model, ALCOVE makes different predictions at different parameter values. When applied to the Shepard et al. (1961) task, ALCOVE will sometimes produce curves that have the same qualitative ordering as the empirical data, but at ot
her times they will look quite different. It would be nice to know something about the {\i other} orders that ALCOVE can produce, since it seems that we might learn something about the model itself. A PSP analysis can provide such information. Using the \ldblquote weak o
rder\rdblquote  definition of a pattern of curves, there are 4,683 different data patterns that a model could produce. One would hope that ALCOVE generates only a small proportion of these, and that the extra patterns it does produce are interpretable in terms of hu
man performance. }
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Preliminaries}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 We now have a model, a data set, and an intuitive definition of a data pattern. To perform PSP analyses, a formal method of associating a set of learning curves with a particular pattern must be defined. The judgement that I<II<(III, IV, V)<VI is the appro
priate empirical pattern has generally been based on visual inspection of the curves. It is possible to be more precise about this, allowing us to uniquely associate a set of learning curves with a qualitative ordering to yield a data pattern. The details 
of this procedure, which is essentially a clustering analysis, are provided in Appendix B. }
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 For the PSP analysis of ALCOVE, we constrained the parameter vectors ({\i c},{\i  {\u966 ?}},{\i   {\u951 ?}{\field{\*\fldinst ADVANCE \\d 3}{\fldrslt }}{{\*\updnprop5801}\dn8 w}{\field{\*\fldinst ADVANCE \\u 3}{\fldrslt }}}, {\i{ \u951 ?}}{\i\i\i{\field{\*\fldinst ADVANCE \\d 3}{\fldrslt }}{{\*\updnprop5801}\dn8 a}{\field{\*\fldinst ADVANCE \\u 3}{\fldrslt }}}) to lie between (0, 0, 0, 0) and (20, 6, 0.2, 0.2), and disallowed any parameter combination that did not produce monotonic curves. A technical complication is introduc
ed by the fact that ALCOVE\rquote s predictions are slightly dependent on the order in which stimuli are observed. Each stimulus has a different effect on ALCOVE, so variations in order of presentation produce slight perturbations in the response curves. However,
 even these minor perturbations can violate the continuity assumptions that underlie the PSP algorithm. In order to deal with these order effects, we chose 20 random stimulus orders, and ran the PSP algorithm 10 times for each stimulus order, yielding a to
tal of 200 runs. As it turns out, the important properties of ALCOVE are invariant under stimulus reordering, but some unimportant properties are not. }
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 How Many Data Patterns can ALCOVE Produce?}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 After running the PSP algorithm 200  times, we observed that each run produced a different number of patterns, ranging from a minimum of 32 to a maximum of 122. Although this range is substantial, it reflects an inherent variability in ALCOVE moreso than t
he PSP algorithm itself. The mean number of patterns recovered for a particular stimulus order ranged from 46.5 to 102.8, while the range in the number of patterns recovered within an order was minimal: the smallest range was a mere 7 patterns, while the l
argest was 36. Moreover, there was an important amount of redundancy across the 200 runs, with 17 patterns being found on every occasion, which included the empirical pattern I<II<(III,IV,V)<VI. We will refer to these 17 patterns as \ldblquote universal\rdblquote  patterns, a
nd the other 183 patterns as \ldblquote particular\rdblquote  patterns. }
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Compared to the set of all 4683 possible data patterns, even the largest count of 17+183=200 patterns encompassed by ALCOVE is quite a small number. In a sense, this is quite a success for the model, because the empirical pattern suddenly looks far less un
likely if we assume humans do something rather ALCOVE-like. Even in the scenario where we allow {\i all} 200 recovered patterns to be treated as a genuine ALCOVE prediction, the empirical pattern is one 1 pattern in 200, rising from the much less satisfying bas
e rate of 1 in 4683. If the substantive predictions are restricted to the set of universal patterns, the empirical pattern is now 1 in 17. Either way, ALCOVE provides a reasonably good qualitative account of these data.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 This initial analysis demonstrates that ALCOVE passes a basic sufficiency test, in that it can account for the qualitative structure of the observed data without \ldblquote going overboard\rdblquote , and producing every possible pattern. Of course, as psychologists, we are i
nterested in more than just how many patterns that ALCOVE, and this analysis is just the tip of the iceberg as far as what can be learned from PSP. For example, it would be useful to know what kinds of category-type orderings are generally preserved across
 all 200 data patterns. One crude method for determining this is to find the average position (i.e., its rank among the six curves) of each category type across all patterns. This is illustrated in Figure 7, which plots the mean rank for each of the six ty
pes across all patterns for both universal and particular patterns. It is clear that  rank tends to increase as the index of the type increases. The main difference between the two types of patterns is that the universals do not really distinguish between 
Types III and IV, whereas the particulars do. Nevertheless, it seems to be the case that, on average, rank does not decrease with index. This is encouraging, because both empirically and algebraically, the difficulty of the task either increases or stays c
onstant as the index increases (see Feldman 2000). This analysis demonstrates that on average, the set of 200 patterns that make up ALCOVE\rquote s entire set of qualitative predictions roughly preserve an important property of the empirical data: A monotonic inc
rease in learning difficulty across category type.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 7  here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Which Patterns Matter?}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The very that some patterns are frequent (universals) and others rare (particulars) suggests that some may be more representative of model behavior than others. However, the frequency with which a pattern is found strikes us as an unsatisfying definition o
f its importance to a model\rquote s behavior, since it confounds the model properties with the robustness of the search algorithm. It was therefore necessary to make some assumptions that allow us to identify the major patterns that are responsible for most of A
LCOVE\rquote s behavior. If we accept the notion that ALCOVE's parameters are interpretable and psychologically well-founded (see Kruschke 1993), then it makes sense to treat the parameter space itself as an important source of information about the model. Specif
ically, if a pattern can be produced only within a tiny region in the parameter space, then it is probably safe to dismiss it as largely irrelevant to the model. Using this information, we can estimate the proportion of the parameter space that is taken up
 by each pattern, and then use these quantities to identify the most prevalent patterns.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Note that the outcome of such an analysis depends on the manner in which ALCOVE\rquote s parameters are formalized. In statistical terms, the conclusions are no longer invariant under reparametrization. This is not necessarily a bad thing, so long as we have some
 principled reason for using the current parametrization. Arguably, the nature of the exemplar theory on which ALCOVE is based, and the manner in which the model captures Kruschke\rquote s (1993) \ldblquote three principles\rdblquote , provide exactly this kind of justification. }
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 As it turns out, there is a strong relationship between the size (i.e., volume) of the region occuppied by a pattern and the frequency with which it was discovered across the 200 runs. In Figure 8, the log of the average volume for each of the 200 patterns
 discovered is plotted against the frequency with which they were discovered. Patterns shown on the far left are the ultimate particulars, having been discovered only once, while patterns on the far right are the universals, having been discovered on every
 occasion. Noting that the scale is logarithmic, we observe that the universals are by far the largest patterns. The empirical pattern, indicated by the circle, is one of the larger patterns, and is a universal.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 8 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 The data in Figure 8 let us refine the answer to the question, \ldblquote To what extent does ALCOVE {\i predict} the empirical pattern?\rdblquote  The fact that the empirical pattern is among the 17 universals is encouraging, as is the regularity suggested by Figure 7. However, b
y considering the size of the various regions, we can take this analysis a step further. We could, for instance, exclude all patterns that do not reach some minimum average size. This approach is illustrated by the horizontal threshold shown in Figure 8: o
nly patterns that occupy more than 1% of the parameter space on average lie above this line. This is a pretty stringent test, given that the parameter space is four-dimensional. Indeed, the empirical pattern occupies only about 2% of the space. }
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 In total, only twelve patterns (all universals) occupy more than 1% of the space, as shown in Table 2, and some general properties of ALCOVE\rquote s behavior emerge when examined together. Looking across patterns, it is clear that Types III and IV are always (12
 of 12) predicted to be learned at about the same rate, and Type V is usually (10 of 12) also about the same. Type VI, on the other hand, is mostly learned slower than III, IV and V (7 of 12). Type I is usually (9 of 12) faster than III-VI, as is Type II (
8 of 12). So, not only is the empirically-observed pattern I<II<(III,IV,V)<VI among the largest patterns (it is the eighth largest), but the other large patterns generally preserve the pairwise relations found in the empirical data. They are, in short, \ldblquote cl
ose\rdblquote  to the empirical pattern. The exception to this claim regards the relationship between Types I and II. Their ordering is ambiguous. It might be that I<II (5 of 12), or I=II (4 of 12), or even II<I (3 of 12). In this case, ALCOVE does not make a strong
 qualitative prediction.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Table 2 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Summary}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The interesting aspect of these PSP analyses is that they both confirm a number of well-known properties of ALCOVE, as well as revealing some unexpected properties. Few category learning researchers would be surprised to hear that ALCOVE captures the I<II<
(III,IV,V)<VI ordering in a robust manner, or that the various pairwise relations that the ordering implies are almost always satisfied. On the other hand, it is somewhat surprising that it is even possible for ALCOVE to predict II<I. It may be that this o
nly happens at odd choices of parameters. However, this is certainly something that would be interesting to look into in the future. Similarly, it is obvious that ALCOVE is subject to stimulus ordering effects. However, the extent of these effects is somew
hat surprising, though perhaps not terribly interesting.}
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ab\ltrch\dbch\af4\afs20\langfe255\ab\loch\f2\fs20\lang1033\b 
\par {\loch\f2\fs20\lang1033\i0\b Application 2: Comparing the Architectures of Merge and TRACE}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 In addition to learning about the behavior of a single model, PSP analyses can inform us about the behavioral consequences of design differences between models. We present two different applications of PSP to two localist connectionist models of speech per
ception, TRACE (McClelland & Elman, 1986) and Merge (Norris, McQueen, & Cutler, 2000), both of which are able to simulate key experimental findings of how word knowledge influences phoneme perception. In this section, we use the subcategorical mismatch stu
dy by Marslen-Wilson and  Warren (1994) to assess the consequences of splitting phoneme processing into separate input and decision stages. In the next section, the indirect inhibition experiment of Frauenfelder, Segui, and Dijkstra (1990) is used to exami
ne the contribution of the \ldblquote bottom-up priority rule\rdblquote  to model performance.}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Background to the Analysis}
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The Architectural Difference Between TRACE and Merge}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The two models are illustrated schematically in Figure 9. In both cases, information is fed upwards from a phonemic input layer into a lexical knowledge layer. In TRACE, word knowledge can directly affect sensory processing of phonemes, because information
 can be fed back from the word layer to the phoneme layer. In short, the two layers {\i interact}. In Merge, there are separate layers for phonemic input and phoneme decision. As a result,  information goes from the word layer to the phoneme decision layer, but
 not to the the input layer. Thus, phoneme input and word knowledge are {\i integrated} at a later decision stage. We refer to this as the {\i architectural difference }between the models, and it is this difference that will be examined in this section.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 9 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 In order to compare the two models, it was necessary to equate them in every way possible to ensure that differences in performance were attributable only to design differences, not other factors, such as the size of the lexicon. In essence, we wanted to c
ompare the fundamental structural and functional properties that define the models. We did this by first implementing the version of Merge described in Norris et al (2000), and then making the necessary changes to Merge to turn it into TRACE. In the end TR
ACE required four fewer parameters than MERGE. The names of the parameters, along with other model details, are in Appendix C.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The Subcategorical Mismatch Experiment}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The first comparison of the two models was performed in the context of the subcategorical mismatch experiment of Marslen-Wilson and Warren (1994, Experiment 1; McQueen, Norris, & Cutler, 1999, Experiment 3).  It is attractive because of the large number of
 conditions and response alternatives, which together permitted detailed analyses of model behavior at both the phonemic and lexical levels. In the experiment, listeners heard one-syllable utterances and then had to classify them as words or nonwords (lexi
cal decision task) or categorize the final phoneme (phonemic decision task). The stimuli were made by appending a phoneme (e.g., /b/ or /z/) that was excised from the end of a word (e.g.,{\i  job}) or nonword (e.g., {\i joz}) to three preceding contexts, to yield si
x experimental conditions (listed in Table 3). The first context was a new token of those same items but with the final consonant removed (e.g., {\i jo}), to create cross-spliced versions of  {\i job }and {\i joz}. The second consisted of equivalent stretches of speech f
rom a word that differed only in the final consonant (e.g., {\i jo }from {\i jog }in both cases). The third was the same as the second except that the initial parts were excised from two nonwords (e.g., {\i jo }from {\i jod }and {\i jo }from {\i jov}). }
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Table 3 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Because cues to phoneme identity overlap in time (due to coarticulation in speech), a consequence of cross-splicing is that cues to the identity of the final consonant will conflict when the first word ends in a consonant different from the second. For exa
mple,  {\i jo }from {\i jog} contains cues to /g/ at the end of the vowel, which will be present in the resulting stimulus when combined with the /b/ from {\i job} (Condition 2 in Table 3). Marslen-Wilson and Warren (1994) were interested in how such stimuli affect phone
me and lexical processing. As the results in Table 3 show (taken from McQueen et al, 1999), in the lexical decision task, reaction times slowed when listeners heard cross-spliced stimuli, but responding was not affected by the source of the conflicting cue
s (i.e., Conditions 2 and 3 are equivalent).  Phoneme categorization, in contrast, was sensitive to the subtle variation in phonetic detail, but only when the stimulus itself formed a nonword (e.g., {\i joz}; Conditions 5 and 6). Importantly, the use of cross-s
pliced stimuli nullifies the bottom-up priority rule in Merge (discussed later) , which otherwise might have contributed to any differences (see Norris et al, 2000, for details).  To the extent that differences are found across models, they are more than l
ikely a result of their different architectural properties.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The PSP Analysis}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The subcategorical mismatch experiment contains two dependent measures of performance, classification decisions and the speed with which those decision were made (response time). In our analysis we treat classification performance as a qualitative pattern,
 which we investigate using PSP. We then perform a focused investigation of the RT predictions.}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Preliminaries}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Classification in connectionist models is usually defined in terms of the activation state of the network when specific decision criteria are met, such as a phoneme node exceeding an activation threshold. Because there are multiple conditions in the experi
ment, a data pattern is really a profile of classifications across these conditions. In this experiment there are six conditions, with a phoneme and lexical response in each, for a total of 12 categorization responses that together yield a single data patt
ern. With four possible phoneme responses and three possible lexical responses, there were a total of 2,985,984 (4{{\*\updnprop5800}\up5 6} x 3{{\*\updnprop5800}\up5 6}) patterns. Of interest is how many and which of these patterns TRACE and Merge produce.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 We applied two different decision rules in our application. The effect of these rules is that they establish the necessary mapping between the continuous space of network states to the discrete space of data patterns, making it possible to associate each d
ata pattern with a region in parameter space. Because any one rule could yield a distorted view of model performance, the use of two rules enabled us to assess the generality of the results. In addition, we found that some model properties that are not evi
dent with one criterion emerged when performance was compared across the rules. The first rule, labeled {\i weak threshold}, was a fixed activation threshold, with values of 0.4 for phoneme nodes and 0.2 for lexical nodes. It is the same rule used by Norris et 
al (2000), and was adopted to maintain continuity across studies.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The second rule, called the {\i stringent threshold}, required classification to be more decisive, by requiring the activation level of competing nodes to be significantly lower than the winning node. Two thresholds were used, the higher of which is the lower b
ound for the chosen node and the lower of which is the upper bound for the nearest competitor. These values were 0.45 and 0.25 for phoneme classification, and 0.25 and 0.15 for lexical decision. Two other constraints were also enforced as part of the strin
gent threhold. There had to be a minimum difference in activation between the winning node and its closest competitor of 0.3 for phoneme classification and 0.15 for lexical decision. Finally, for nonword responses in lexical decision, the difference in act
ivation between the two lexical items, {\i jog} and {\i job}, could not be more than 0.1.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The models were designed as depicted in Figure 9. The only lexical nodes were  {\i job} and {\i jog. }{\i0 All} necessary phoneme nodes were included, with  /b/, /z/, /g/, /v/ being of  primary interest. The PSP algorithm was run for both models and both decision rules. T
o obtain a data pattern, all six stimuli were fed into the model and both phoneme and lexical classification responses assessed. The algorithm and models were run in Matlab  on a Pentium IV computer. The time required to find all patterns varied greatly, t
aking as little as 22 minutes (TRACE, stringent threshold) and as long as 24 hours (Merge, stringent threshold).  The consistency of results was ascertained using five multiple runs for each model and threshold.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 How Many Patterns do the Models Produce?}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Looking first at the weak threshold data, both models generate 22 common patterns, with TRACE producing only a few unique patterns compared to Merge (3 vs 29). This is shown by the Venn diagram in the top left of Table 4. The filled dot represents the empi
rical pattern, which both models can produce. The nature of the overlap in the diagram reflects the fact that TRACE is virtually nested within Merge, with 22 of its 25 patterns also being ones that Merge produces. However, Merge can produce an extra 29 pat
terns, suggesting that in the  subcategorical mismatch design, Merge is more flexible (i.e., complex) than TRACE. That said, the difference is it not all that great, although it does not disappear under further scrutiny (see below). Perhaps most impressive
ly, both models are highly constrained in their performance, generating fewer than 60 of the some 3 million patterns that are possible in the design. When the stringent threshold is imposed, both models generate fewer patterns, with TRACE producing 7 and M
erge 32, as indicated in the lower half of Table 4. Nevertheless, the relationship between the models remains unchanged: TRACE is almost nested within Merge. However, the one unique TRACE pattern turns out to be the empirical data, which Merge no longer ge
nerates. }
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Table 4 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Interestingly, the change in threshold primarily caused a drop in the number of shared patterns, indicating that the models are more distinct under the stringent threshold. To understand why, notice that Table 4 also indicates that the proportion of the pa
rameter space occupied by valid data patterns is much smaller in TRACE than in Merge. For TRACE, this region is occupied almost entirely by the 22 common patterns (99%). Under the stringent threhold, this value drops slightly to 84%. This is because 15 of 
the common patterns occupied such tiny regions in TRACE\rquote s parameter space under the weak threshold that application of the stringent threshold eliminated them. A similar situation occurred with Merge (column 6), with 12 of the 16 common patterns (of which 
the empirical pattern was one) disappearing due to a change in threshold. Four became unique to Merge. A few patterns unique to each model also failed to satisfy the stringent threshold (3 for TRACE and 7 for Merge).}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Is the Variation Across Patterns Sensible?}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 To the extent that a model produces data patterns other than the empirical one, a mark of a good model is for its performance to degrade gracefully. In the previous example, ALCOVE's performance degraded gracefully because all of its patterns looked simila
r to the empirical one. In this case, we measured deviation by counting the number of mismatches (different decisions) between a particular pattern and the empirical one. Since a pattern consists of 12 decisions, there are maximum of 12 possible mismatches
 (6 phonemic and 6 lexical). Histograms of the mismatch counts were created for both models, and are shown in Figure 10 alongside a histogram showing the base rate for each count (i.e., there are thousands of possible patterns with six mismatches, but only
 12 patterns with one mismatch).}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 10 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Both models show a remarkable proclivity to produce human-like data. The distributions are positioned near the zero endpoint (empirical pattern) with peaks between two and four mismatches. The probability is virtually zero that a random model (i.e., a rand
om sample of patterns) would display such a low mismatch frequency. The weak threshold distributions are slightly to the right of stringent threshold distributions, and Merge distributions are slightly to the right of TRACE distributions. In both cases, th
is is a base rate effect: allowing more patterns increases the likelihood of more mismatches. {\b [Footnote??]}}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 An equally desirable property of a model is that patterns that mismatch across many conditions occupy a much smaller region in the parameter space than those that mismatch by only one or two conditions. That is, larger regions should correspond to patterns
 that are more similar to the empirical pattern. This is generally true for both models, and to a similar extent. When region volume is correlated with mismatch distance, there is a modest relationship between the measures ({\i r}=0.35 for TRACE and {\i r}=0.34 for 
Merge).}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The PSP analyses raise an interesting question: when the models make classification errors, what types of things to they get wrong? To answer this, the proportion of mismatches in each of the 12 conditions was computed for each model and are plotted in Fig
ure 11. The profile of mismatches across conditions reveals more similarities than differences between the models. For the most part the models performed similarly. The major  difference is that Merge produced more phoneme misclassifications (conditions 3 
and 6) and TRACE produced a greater proportion of lexical misclassifications. In the four phoneme conditions in which errors were made, the final phoneme was created by cross-splicing two different phonemes, creating input that specified one weakly and the
 other strongly. The errors are a result of misclassifying the phoneme as the more weakly specified (lowercase) alternative (e.g.,  {\i g} instead of {\i B}). By design, Merge\rquote s bottom-up only architecture hightens its sensitivity to sensory information, which in th
e present simulation made it a bit more likely than TRACE to misclassify cross-spliced phonemes.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Lexical misclassification errors in TRACE are due to a bias to respond nonword, which is a bit mysterious. If anything, one would expect the excitatory loop between the phoneme and lexical levels to give TRACE a lexical bias. Note that not all misclassific
ations are due to responding \ldblquote nonword.\rdblquote   In condition 5, they are entirely due to classifying the stimulus as {\i jog}. This also occurred in condition 2, but much less often. Misclassifications as {\i jog} constituted 8% of the mismatches for TRACE and 3% for Merge
.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 11 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Which Patterns Matter?}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Although some misclassifications can be legitimized on many grounds (e.g., humans make such errors, perception of ambiguous stimuli will not be constant), it is important to determine whether they are characteristic behaviors of the model or idiosyncratic 
patterns, rather like the \ldblquote particulars\rdblquote  defined in the ALCOVE analysis. That is, it is useful to distinguish between unrepresentiative and representative behaviors. To do so, we measured the volumes of all regions identified by the PSP algorithm. As in the
 ALCOVE analysis, a threshold of 1% of the valid volume was adopted to define a meaningful pattern. When this is done, many patterns turn out to be noise and the set of representative patterns is reduced to a handful. For TRACE, 21 of its patterns (3 uniqu
e and 18 common) do not meet this criterion. Four patterns, all common, dominate in volume, together accounting for 99% of the volume (range 3.8 % - 45.2%).  For Merge, the set of dominant patterns is larger, and is split equally between common and unique 
patterns. Thirty six patterns (21 unique and 15 common) fail to reach the 1% criterion. Seven common (range 1.3% - 21.1%) and eight unique (1.4% - 15.7%) patterns do so and make up 96% of the valid volume. Even with a threshold that eliminated 75% of all p
atterns, the asymmetry in pattern generation between the models is still present (TRACE=4; Merge=15).}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The volumes of the representative common patterns are graphed in Figure 12. The numerals in the legend refer to the mismatch distance of each region from the empirical pattern. Most obvious is the fact that the empirical pattern is much larger in TRACE tha
n in Merge (33.1% and 6.8%) and that one mismatching pattern (filled black) dominates in both models (45.2% and 21.1%). This pattern turns out to be one in which there is a bias to classify all stimuli as nonwords. As a group, the eight unique Merge patter
ns mismatch the empirical pattern more than the common patterns. The largest pattern occupies a region of 15.7% (six mismatches), nearly twice the next largest region (8.9%). In this pattern, not only did Merge exhibit the same nonword response bias, but i
t also categorized cross-spliced phonemes as the competing (remnant) phoneme.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 12 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Response Time Analyses}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Up to this point in the analysis we have compared only the decisions made by the models, but the time course of processing is equally important, as experimental predictions often hinge on differences in response times between conditions. Connectionist mode
ls are generally evaluated on their ability to classify stimuli at a rate (e.g., number of cycles) that maintains the same ordinal relations across conditions found with RTs. To assess the robustness of simulation performance, parameters can be varied slig
htly and additional simulations can be run to ensure that the model does not violate this ordering (e.g., by producing a reversal of the RT pattern). In their tests of Merge and TRACE, Norris et al (2000) found neither produced an RT reversal. Our test was
 a more exhaustive version of theirs.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The PSP analysis defines for us the region in the parameter space of each model that corresponds to the empirical pattern. When assessing RT performance, what we want to know is whether there are any points in this region (i.e., parameter sets) that yield 
invalid RT patterns, as defined by the ordinal relations among conditions in the experiment (i.e., the RT pattern in the six phonemic and three lexical conditions in Table 3). To perform this analysis, 10,000 points were sampled over the uniform distributi
on of the empirical region. Simulations were then run with each sample and the cycle time at which classification occurred was measured and compared across all conditions. Violations were defined as reversals in cycle times between adjacent conditions (e.g
., condition 1 vs. condition 2) in phoneme classification and lexical decision.  Just as Norris et al (2000) reported, we did not find a single reversal between conditions for either model.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Summary}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The PSP analyses reveal that the consequences of splitting the phoneme level in two, which effectively softens lexical influences, produces a slightly more flexible model, one that can produce more data patterns. This was found no matter how the analyses w
ere performed. Accordingly, the nature of this additional flexibility is of some interest. By splitting the phoneme level in two, Merge does not appear to undergo significant transformations. Rather, the model\rquote s behavior is expanded, as the nested relation
ship between TRACE and Merge shows. Merge retains many of TRACE\rquote s behaviors (producing most of its patterns) and has also acquired new ones. A consequence of this expansion is that the representativeness of these behaviors is considerably different across 
models, as shown in Figure 12.. Indeed, it is because these differences are quantitative moreso than qualitative that the models are so similar on other dimensions (e.g., frequency and types of mismatches, response time relations between conditions).}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ab\ltrch\dbch\af4\afs20\langfe255\ab\loch\f2\fs20\lang1033\b {\loch\f2\fs20\lang1033\i0\b Application 3: The Bottom-Up Priority Rule in Merge and TRACE}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The previous analysis of TRACE and Merge was restricted to a discussion of architectural differences. However, there is another important difference between the models, relating to the way different sources of information are prioritized. Once again, we wa
nt to use PSP to get a sense of the way in which this difference influences the qualitative behavior of the two models.}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Background to the Analysis}
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The Bottom-Up Priority Rule}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\f2{\ulnone The second major difference between T}}{\loch\f2\fs20\lang1033\f2RACE and Merge relates to a principle called the  {\i bottom-up priority }{\i0 (BUP) rule. }In keeping with the belief that bottom-up (sensory) information must take priority in guiding perception early in processing, the BUP rule
 states that the activation of a phoneme decision node must be initiated by phoneme input {\i before }excitatory lexical activation can affect phoneme decision making. {\i0 The BUP rule did not come into play in the previous comparison, due to the cross-splicing of 
the stimuli (Norris et al. 2000), but in other situations it is important. }}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\fs20\f2 In their original forms, Merge has a BUP rule, but TRACE does not. In TRACE, the initial activation of phoneme units can come from top-down connections from the word stage. For example, having been presented with {\i jo} in {\i job}, excitation from the {\i job }node wil
l feedback and excite the {\i b} node. Inhibitory connections between phoneme nodes makes it possible to observe indirect word-to-phoneme inhibition, because the activated {\i b} node will in turn inhibit competing phoneme nodes (e.g., {\i g)}. Thus, word nodes have the 
ability to excite directly and inhibit indirectly phoneme nodes. This violates the idea of  bottom-up priority. Of course, one can easily incorporate a BUP rule into TRACE, simply by not allowing top-down feedback to influence a phoneme node until after th
at node has received some bottom-up input. Similarly, the BUP rule can be removed from Merge, by allowing phoneme decision units to be activated by word layer units, without first having to be activated by phonemic input units.}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 This observation suggests that an elegant analysis is possible, by running PSP analyses on both models with and without the BUP rule, in what amounts to the 2x2 factorial design shown in Table 5.  If the rule is primarily responsible for differences in mod
el behavior, then the results for both models should be quite similar when the rule is and is not operational. If differences still remain, then structural differences are also contributing to their diverse behaviors. In short, these analyses will tell us 
whether Merge, without its priority rule, behaves like TRACE, and whether TRACE, with the priority rule, mimics MERGE. However, we now need a new data set, since the subcategorical mismatch data are not affected by the BUP rule.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The Frauenfelder et al. Data}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f0\fs20\lang1033{\loch\f2\fs20\lang1033\i0\b0\fs20\f2{\ulnone If indirect word-to-phoneme activation is possible, then one would expect anomalous word-endings to slow down human performance. So, for example, a nonword like {\i habil} should be more confusing than a nonword like {\i mabil}, because {\i habit} is a word, and listener
s would be expecting to hear /t/ rather than /l/. This idea was tested in Frauenfelder et al.'s (1990) experiment 3, which included conditions for words (e.g.,{\i  habit}), control nonwords (e.g., {\i mabil}) and inhibitory nonwords (e.g., {\i habil}), and RT was measure
d as the dependent variable. }}{\loch\f2\fs20\lang1033\fs20\fs20\fs20\f2However, the RT slowdown in the inhibition condition was small and not reliable, a null result which has been interpreted as arguing against word-to-phoneme excitation in TRACE. However, in simulations of indirect inhibition, T
RACE\rquote s behavior is not cut and dry, with inhibition being more likely with longer than shorter words (Norris et al, 2000).  In contrast, the BUP rule in Merge guarantees that it produces consistent performance that never yields inhibition. }
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Table 5 here}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The PSP Analysis}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The Frauenfelder et al. data raise the  interesting question of whether the difference in performance is solely due to the BUP rule, or whether the architectural differences matter as well. In what follows, we use PSP to provide some insight on this matter
. }
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Preliminaries}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The design of the indirect inhibition experiment is much simpler than the subcategorical mismatch experiment. There are only three conditions and only a single response decision (phonemic). To simulate the experiment, the combination of so few conditions a
nd a simple model design (one lexical and two critical phoneme nodes) would yield so few potential data patterns that the analysis might not provide satisfying answers to our question. However, since we know that human listeners can correctly classify thes
e stimuli as words or nonwords, we added a lexical decision response to the design, so as to allow a stronger comparison between the models.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Input to the models consisted of three utterances: {\i habit}, {\i mabil}, {\i habil}. They were selected to be moderately long (five phonemes) so that indirect inhibition would have a chance to emerge. Both models were modified from the previous test to consist of only 
one lexical node ({\i habit}) and the appropriate phoneme input/decision nodes, with {\i0 /t/ and /l/ }of most interest because their activation functions were used to test for inhibition. With two classification response, each with two alternatives, and three stimul
us conditions, there were a total of 64 possible data patterns (2{{\*\updnprop5800}\up5 3} x 2{{\*\updnprop5800}\up5 3}).}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 To examine the effect of the BUP rule on model performance, phoneme activation parameters were adjusted accordingly in each model prior to running the PSP algorithm. The same two decision rules were again used to assess the generality of results. With two 
decision rules and two priority rules, the algorithm was run four times on each model. The consistency of the results for each analysis was ascertained by rerunning the algorithm five times. The averaged data are presented below. No more than seven minutes
 were required to find all patterns in any run.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Classification Analyses}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 The classification data are shown in column 3 of Table 6, with the first row containing the comparison of the models as originally designed (TRACE without BUP and Merge with it). The Venn diagram shows that the relationship between the models is opposite o
f that in the subcategorical mismatch experiment (Table 4), with Merge now embedded in TRACE, and TRACE producing three times as many data patterns as Merge (12 vs. 4). Both models area again highly constrained in their behavior, producing nowhere near the
 64 possible data patterns in the experimental design.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Table 6 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 TRACE\rquote s extra flexibility in this experimental setup agrees with Norris et al\rquote s (2000) observation of TRACE\rquote s variable behavior in producing indirect inhibition. That this flexibility is due to the absence of a BUP rule can be seen by examining the venn di
agrams in the remaining rows. When the priority rules are swapped between models (row 2), their relationship reverses. TRACE shrinks from twelve to four pattners while Merge grows from four to twelve, embedding TRACE in Merge.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 This reversal is not merely in terms of the number of data patterns, but extends to the actual data patterns themselves. That is, even though the models trade places in terms of their relationship, the common and unique patterns remain the same. To see tha
t this is the case, look at the venn diagrams in rows 3 and 4, where the priority rule is either on or off at the same time in the two models. In both situations TRACE and Merge overlap completely, producing only common patterns, four when the priority rul
e is on and 12 when it is off, which matches exactly what was found in rows 1 and 2. Comparison of the diagrams in column 3, one can see that when the priority rule was on, the models always generated four patterns. When it was off, they generated 12.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The isomorphism of the models models with and without the priority rule is surprising. Within this experimental design, their qualitative behaviors are identical in terms of the patterns they can generate, and point to the the rule itself as a primary dete
rminer of behavioral differences. As will be discussed shortly, this interchangeability shows up in the RT analyses.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Columns 4-7 in Table 6 contain volume measurements whose relationship between models is similar to that found in the subcategorical mismatch analyses. TRACE\rquote s valid region of the parameter space is much smaller than Merge\rquote s. This relationship changes littl
e as a function of the priority rule. When common data patterns occupy only a portion of the valid volume (rows 1 and 2), the region is larger for TRACE (.75) than Merge (.56), indicating that the unique patterns occupy a smaller region in TRACE\rquote s paramete
r space. This difference foreshadows what will happen under the stringent threshold.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The last column in Table 6 shows the relationship between the models when the stringent threshold was used. A quick glance at all venn diagrams shows the isomorphism no longer holds. This can be seen most clearly in the top cell, where the embedding is the
 reverse of that found with the weak threshold. TRACE produced only three of the 12 patterns, two of which are shared by Merge. In contrast, Merge produced the same four patterns. In rows 2-4, TRACE remains embedded in Merge, producing the same few pattern
s (never the empirical one). It appears that under the stringent threshold, TRACE generated so few patterns to begin with that there was little opportunity for the priority rule to alter model behavior. In contrast, Merge behaved just as it did under the w
eak thresold, generating more patterns without the rule (rows 2 and 3) than with it (rows 1 and 3). Notice that what changes most down this colum of venn diagrams is number of unique patterns generated by Merge.[FN]}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The reason for this asymmetric effect of the stringent threshold on the two models can be understood by examining the volume measurements under the weak threshold (Figure 13). Region sizes are shown for both models with and without the priority rule. Each 
section of a vertical bar represents a different data pattern, except those denoted by gray shading, which represents the combined area of eight regions, most of which are visible for Merge but not for TRACE. In the first bar (TRACE, No BUP) these eight, w
hich together total less than 1% of the volume, plus the human region are so small that they disappear when the stringent threshold is applied, which results in the sharp drop in the number of data patterns TRACE can generate. These same regions are much l
arger in Merge (only one is less than 1% of the volume), and although they shrink in size, they do not disappear when the stringent threshold is applied.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 13 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Further insight into the effects of the priority rule on model behavior can be obtained by comparing the types of misclassification errors made by the models when the rule was on and off. As in Figure 12, each section of bar is filled with a graphic patter
n that denotes the mismatch distance of the data pattern from the empirical data, which is the bottom section and contains no pattern. In TRACE, the empirical pattern itself occupies a small region in the parameter space that changes imperceptibly in size 
when bottom-up information is given priority. In Merge, however, not only is this region much larger, but it enlarges even more with the priority rule, occupying 29% of the valid volume. Although TRACE can produce the empirical pattern, it is clearly a mor
e central pattern in Merge. Whatismore, the priority rule enhances this centrality.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Most of the mismatching patterns, especially those that occupied the largest regions, rarely veered far from the empirical pattern, differing by one or two responses out of a possible six. Comparison of the bars between models shows that the same biases ex
hibited by the models in the subcategorization experiment are present here as well. A few patterns dominate the parameter space in TRACE whereas the space is split among patterns more equitably  in Merge. This is most evident in the No BUP conditions, wher
e a pattern with two mismatches occupies half (.506) of TRACE\rquote s volume. The errors in this instance are due to lexical misclassifications in which the two nonwords ({\i mabit }and {\i habil}) were categorized as words. Merge exhibits a much weaker tendency to do thi
s (.068). Instead, the pattern occupying the largest volume in Merge does just the opposite: {\i habit} is classified as a nonword. TRACE does this as well. Application of the priority rule (bars 2 and 4) increases these tendencies in both models (i.e., the reg
ions increase in size).}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Reaction Time Analyses}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 RT analyses were perfromed only on the weak threshold data because TRACE failed to generate the human pattern under the stringent threshold. The region occupied by the empirical pattern was probed for parameter sets that violated the ordering of mean parti
cipant response times across the three conditions ({\i habit }< {\i mabil }= {\i habil}). Model behavior with and without the priority rule was also examined.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The procedure was the same as that used in the first comparison of the models. Samples (10,000) were drawn from the uniform distribution over the region of the empirical pattern and the model was run with each set of parameter values. The cycle at which ph
oneme classification occurred was recorded across condtions. Because performance violations amount to a reversal in cycle classification time between adjacent conditions, we calculated the difference in cycle time of neighboring conditions, {\i habit} vs. {\i mabil}
, and {\i mabil }vs. {\i habil}. Distributions of these difference scores (over all samples) are plotted in Figure 14. Cycle times to {\i mabil }were subtracted from those in the other two conditions. The top graphs contain the comparison of the models as originally form
ulated (TRACE without BUP, Merge with BUP). In the bottom pair of graphs the priority rules were swapped across models. The left-hand graphs contain the {\i habit-mabil }comparison and the right-hand graphs the more theoretically important, {\i mabil }vs {\i habil}, comp
arison.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 14 here}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 In the upper left graph, both models produce the correct ordering, with phoneme classification always being faster in a word ({\i habit}) than nonword ({\i mabil}) context.  What differs between the models are the shapes of the distributions, with TRACE\rquote s being shar
ply peaked and Merge\rquote s more diffuse. Data from the {\i mabil }- {\i habil} comparison are in the right graph. Here we see the effects of the priority rule, with Merge always generating the correct prediction of identical classification times in the two conditions (z
ero RT difference), but TRACE showing indirect inhibition, with recognition times slightly longer to /l/ in {\i habil }than in {\i mabil}, hence the negative difference score. Although this effect is usually small, choice of parameter settings could lead to differen
t conclusions, where occasionally the effect would be large.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 That equivalent classification times in the {\i mabil }and {\i habil }conditions are due to the priority rule can be seen in the lower right graph, where TRACE was run with the BUP rule and Merge without it. Just as in the classification data in Table 6, their perfo
rmance reverses: TRACE now produces no difference across conditions whereas Merge displays indirect inhibition. The two left-hand graphs are quite similar despite the priority rule shifting between models. This indicates that the word-nonword difference in
 phoneme processing is mostly determined by model architecture.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Scanning across all four graphs, it is clear that the interactive architecture of TRACE constrains classification time more so than the integrative architecture of Merge. Regardless of priority rule, both models show a tendency to produce small rather than
 large differences scores, but TRACE more so than Merge given the height and location of the peaks of the distributions.}
\par 
\par \pard\plain \ltrpar\s8\cf0\ul\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Summary}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 This second comparison of TRACE and Merge was undertaken to learn how the bottom-up priority rule can differentiate the two models. The PSP analyses under the weak and stringent threholds showed that the rule reduces model flexibility and confirmed its nec
essity for simulating the correct pattern of response times. By turning the rule on and off across models, we discovered that they behave identically, producing qualitiatively indistinguishable data patterns. Only when the volumes of these patterns were in
spected did performance differences due to model architecture emerge. Merge\rquote s integrative architecture is somewhat better suited for mimicking human behavior in this experimental design. Not only is the empirical pattern more central in Merge, but this was
 true regardless of whether the priority rule was in place. Such findings and conclusions are possible because of the global perspective on model behavior that a PSP analysis provides.}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ab\ltrch\dbch\af4\afs20\langfe255\ab\loch\f2\fs20\lang1033\b {\loch\f2\fs20\lang1033\i0\b General Discussion}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 {\loch\f2\fs20\lang1033\i0\b0 Models are becoming increasingly useful tools for psychologists. They help us make sense out of mounds of data, provide insights and new conceptual understanding, and they can even point the way in a research program. To use them most productively, we need
 to understand their behavior at a level of granularity that matches what is generated in experiments, namely qualitative differences among conditions. PSP was developed with this goal in mind. A model\rquote s ability to mimic human behavior is evaulated in the 
context of the other data patterns that it can generate in an experimental design, thereby providing a rich context in which to understand what it means for a model to account for an experimental finding. }
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 The three example application in this paper were meant to demonstrate some of the uses of this versatile tool. In the case of ALCOVE, PSP assisted in evaluating the model\rquote s soundness in capturing an empirical result. At a qualitative level, the PSP analysi
s supplies the same information about behavior found in LMA, that the model can mimic human performance. Much more importantly, by studying the partitioned parameter space, we learn how representative this behavior is of the model and how many other data p
atterns the model can produce. Some of these might be plausible alternative response patterns. Others might occupy such a small region in the parameter space that they should be considered spurious and  uncharacteristic of the model.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 In the second and third examples, PSP was used to study the behavioral consequences of slight design differences between two localist connectionist models, Merge and TRACE. To do this, data patterns and their corresponding volumes in parameter space were c
ompared across models in two experimental settings that were intended to highlight design differences. Many more similarities than differences were were found in classification behavior. RT analyses of the region occupied by the empirical data led to the s
ame conclusion. Qualitatively, TRACE and Merge were indistinguishable. Only in the volume analyses did differences emerge. These took the form of biases in emphasizing some data patterns over others. The volume of the empirical region can be interpreted as
 reflecting how well the model accounts for the result. By this measure, TRACE does the better job with the subcategorical mismatch data, and Merge with the indirect inhibition data.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Although one might not be surprised by the models\rquote  similarities, the PSP anaysis provides the evidence to support such a claim. Furthermore, the analyses suggest how difficult it might be to find a situation in which they make different qualitative predict
ions. However, PSP might be just the tool to assist in this endevour, by enabling the researcher to essentially pretest an experiment to learn whether it could discriminate between the models. Once an experiment is designed, the models can be run through t
he PSP algorithm to determine which data patterns are shared and unique. To the extent that  participants are likely to produce those in the latter category, the experiment has the potential to discriminate between the models.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Of course, the meaningfulness of any such analysis depends on how a data pattern is defined. Conclusions may be specific to a definition, which is why it can be worthwhile to use more than one, as was done when comparing TRACE and Merge. Because ordinal pr
edictions dominate in most experimental work, the task of defining a data pattern is relatively straight forward. Indeed, this is in part what makes PSP widely applicable. It does not depend on quantitive fits. Another reason PSP is applicable to a wide ra
nge of models is because of how the model interfaces with the the algorithm itself. In a sense, the model is just a module in the algorithm, which makes it possible to insert models of almost any type, be they algebraic, algorithmic, connectionist, etc. In
deed, this feature of the algorithm makes it possible to compare models across these types, something that in the past has been difficult to do. }
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 PSP can also be of considerable use in model development by making the modeler aware of the broader consequences of the design choices made in model creation. For example, there may be four key experimental results that any model of categorizatoin must cap
ture to be taken seriously. By running the algorithm in each experimental setting, a comprehensive analysis of the model\rquote s design can be undertaken. Desirable and undesirable behaviors can be readily identified. The centrality of the human pattern(s) can b
e assessed. The model can then be modified on the basis of this knowledge and the entire process repeated. From successive iterations, one can develop a clear understanding of how or whether parameters contribute to model behavior, such as whether there ar
e redundancies across paramters, the appropriateness of their ranges, and even optimal values.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Figure 15 provides a glimpse of what can be learned. Shown are distributions of parameter values for three of TRACE\rquote s parameters that were obtained within the empirical region of the parameter space in the indirect inhibition experiment (no BUP). Each dist
ribution contains 10,000 values and gives a sense of how the parameter contributes in capturing the experimental result. For the phoneme excitation parameter, the distribution of possible values is fairly broad, although there is a clear optimum. The phone
me-to-word excitation parameter is much more tightly constrained, so much so that one might wonder whether its range should cut in half, from 0 to 0.5. The distribution for the phoneme-to-phoneme inhibition parameter is essentially flat, suggesting a high 
degree of context sensitivity, which goes along with Norris et al\rquote s (2000) observations about the stability of indirect inhibition. Although insights such as these can sometimes be garnered after extensive experience with a model, PSP analyses not only con
firm these intuitions, but also expose their extent as well as introduce the modeler to additional tendencies and behaviors that might not be anticipated.}
\par 
\par \pard\plain \ltrpar\s8\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\rtlch\af7\afs20\lang255\ai\ltrch\dbch\af4\afs20\langfe255\ai\loch\f2\fs20\lang1033\i {\loch\f2\fs20\lang1033\i\b0 Figure 15 here}
\par \pard\plain \ltrpar\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par {\loch\f2\fs20\lang1033\i0\b0 Our analyses in the three applications focused on the regions in the parameter space that generate valid data patterns, those that meet the decision threshold. The invalid regions of the space can be of interest in some circumstances, such as during model 
development. Analysis of these regions might also be useful for model comparison. For example, models\rquote  likeness or distinctiveness might be further confirmed by comparing patterns in these regions.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Throughout the paper, the global perspective on model behavior that PSP provides has been examined from the standpoint of the model to determine its flexibility or scope (Cutting, 2000). It can be equally productive to view the situation from the vantage p
oint of the experiment, by asking how much the experiment is likely to challenge the model. To the extent that it will, the experiment has the potential to be a good, meaningful test of the model. In this regard, one lesson that comes out of our applicatio
ns of PSP is that the fewer the number of conditions and the fewer the possible relationships between them, the less informative the test may be.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Because PSP is a search problem, it is subject to many of the same difficulties found in nonlinear function optimization and integration, which work under a set of regularity conditions about the target function, such as continuity, smoothness, stationarit
y, existence of a solution within a finite limit and so on. For this reason, the algorithm is currently limited in scope. For example, a probabilistic (e.g., stochastic) model can be analyzed by PSP only if its simulational component can be replaced with a
 closed-form probability density function. Another requirement is that the range of parameters must be finite. If some unconstrained parameters are unavoidable (i.e., plausible data patterns could still be generated from their extreme values), it is recomm
ended to reparameterize the model utilizing log, inverse logistic, or other transformations.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 As currently implemented, volume estimation of a region is performed using a multi-dimensional ellipsoid. This is satisfactory for models that generate regions that are not oddly shaped (sharply bowed) or discontinuous. Exploratory work ensured that these 
conditions were satisfied for the current simulations. However, the algorithm would be more powerful, precise, and useful were it could be applied to these situations. Work is underway to extend its capabilities.}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 Another obvious extension of PSP is to continuous value data. [Jay, finish this paragraph]}
\par 
\par {\loch\f2\fs20\lang1033\i0\b0 In sum, PSP fills a void in the arsenal of tools available for studying the behavior of computational models. It is intended to complement local approaches by focusing on the global behavior of the model while at the same time operating at the qualitative 
level of description and prediction found in most experimental work. The wealth of information that can be learned with the tool is enormous, as the examples presented here attest, and bodes well for its successful application in other modeling domains and
 content areas.}
\par \pard\plain \ltrpar\s8\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\aspalpha\ql\rtlch\af7\afs20\lang255\ltrch\dbch\af4\afs20\langfe255\loch\f2\fs20\lang1033 
\par }