{\rtf1\ansi\deff0\adeflang1025
{\fonttbl{\f0\froman\fprq2\fcharset0 Times New Roman;}{\f1\froman\fprq2\fcharset0 Times New Roman;}{\f2\froman\fprq2\fcharset0 Times New Roman Greek Greek;}{\f3\froman\fprq2\fcharset0 Times New Roman Greek;}{\f4\froman\fprq2\fcharset0 Times New Roman CE;}{\f5\fmodern\fprq1\fcharset0 Courier New;}{\f6\froman\fprq2\fcharset0 Times New Roman;}{\f7\fswiss\fprq2\fcharset0 Arial;}{\f8\fnil\fprq2\fcharset0 Lucida Sans Unicode;}{\f9\fnil\fprq0\fcharset0 Tahoma;}{\f10\fnil\fprq2\fcharset0 Tahoma;}}
{\colortbl;\red0\green0\blue0;\red128\green128\blue128;}
{\stylesheet{\s1\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\snext1 Default;}
{\s2\sa120\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon1\snext2 Text body;}
{\s3\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af9\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon2\snext3 List;}
{\s4\cf0{\*\tlswg8236}\tqc\tx4818{\*\tlswg8236}\tqr\tx9637{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon1\snext4 Header;}
{\s5\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon2\snext5 Table Contents;}
{\s6\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ab\ltrch\dbch\af0\afs24\langfe1033\ai\ab\loch\f0\fs24\lang1033\i\b\sbasedon5\snext6 Table Heading;}
{\s7\sb120\sa120\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af9\afs20\lang1033\ai\ltrch\dbch\af0\afs20\langfe1033\ai\loch\f0\fs20\lang1033\i\sbasedon1\snext7 Caption;}
{\s8\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af9\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon1\snext8 Index;}
{\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033\sbasedon1\snext9 Normal;}
{\s10\cf0\qj{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 QuickFormat1;}
{\s11\cf0\qj{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _26;}
{\s12\li1440\ri0\lin1440\rin0\fi-720\cf0\qj{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _25;}
{\s13\li2160\ri0\lin2160\rin0\fi0\cf0\qj{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _24;}
{\s14\li2880\ri0\lin2880\rin0\fi0\cf0\qj{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _23;}
{\s15\li3600\ri0\lin3600\rin0\fi0\cf0\qj{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _22;}
{\s16\li4320\ri0\lin4320\rin0\fi0\cf0\qj{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _21;}
{\s17\li5040\ri0\lin5040\rin0\fi0\cf0\qj{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _20;}
{\s18\li5760\ri0\lin5760\rin0\fi0\cf0\qj{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _19;}
{\s19\li6480\ri0\lin6480\rin0\fi0\cf0\qj{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _18;}
{\s20\cf0\qj{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _17;}
{\s21\li1440\ri0\lin1440\rin0\fi-720\cf0\qj{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _16;}
{\s22\li2160\ri0\lin2160\rin0\fi0\cf0\qj{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _15;}
{\s23\li2880\ri0\lin2880\rin0\fi0\cf0\qj{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _14;}
{\s24\li3600\ri0\lin3600\rin0\fi0\cf0\qj{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _13;}
{\s25\li4320\ri0\lin4320\rin0\fi0\cf0\qj{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _12;}
{\s26\li5040\ri0\lin5040\rin0\fi0\cf0\qj{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _11;}
{\s27\li5760\ri0\lin5760\rin0\fi0\cf0\qj{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _10;}
{\s28\li6480\ri0\lin6480\rin0\fi0\cf0\qj{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _9;}
{\s29\cf0\qj{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _8;}
{\s30\li1440\ri0\lin1440\rin0\fi-720\cf0\qj{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _7;}
{\s31\li2160\ri0\lin2160\rin0\fi0\cf0\qj{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _6;}
{\s32\li2880\ri0\lin2880\rin0\fi0\cf0\qj{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _5;}
{\s33\li3600\ri0\lin3600\rin0\fi0\cf0\qj{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _4;}
{\s34\li4320\ri0\lin4320\rin0\fi0\cf0\qj{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _3;}
{\s35\li5040\ri0\lin5040\rin0\fi0\cf0\qj{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _2;}
{\s36\li5760\ri0\lin5760\rin0\fi0\cf0\qj{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _1;}
{\s37\li6480\ri0\lin6480\rin0\fi0\cf0\qj{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon9\snext9 _;}
{\s38\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon2\snext38 Frame contents;}
{\s39\sb240\sa120\keepn\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af10\afs28\lang1033\ltrch\dbch\af8\afs28\langfe1033\loch\f7\fs28\lang1033\sbasedon1\snext2 Heading;}
{\*\cs41\cf0\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 Footnote Characters;}
{\*\cs42\cf0\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 Endnote Characters;}
{\*\cs43\cf0\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033\sbasedon44 Default Para;}
{\*\cs44\cf0\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033 Normal;}
}
{\info{\creatim\yr2004\mo8\dy9\hr12\min25}{\revtim\yr2004\mo8\dy10\hr10\min42}{\printim\yr1601\mo1\dy1\hr0\min0}{\comment StarWriter}{\vern6450}}\deftab720
{\*\pgdsctbl
{\pgdsc0\pgdscuse195\pgwsxn12240\pghsxn15840\marglsxn1440\margrsxn1440\margtsxn1440\margbsxn1440\headery0{\*\headeryb0\headerxl0\headerxr0\headeryh283}{\header \pard\plain \s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033 
\par }
\pgdscnxt0 Default;}
{\pgdsc1\pgdscuse195\pgwsxn12240\pghsxn15840\marglsxn1134\margrsxn1134\margtsxn1134\margbsxn1134\pgdscnxt1 Endnote;}
{\pgdsc2\pgdscuse195\pgwsxn12240\pghsxn15840\marglsxn1440\margrsxn1440\margtsxn900\margbsxn1440\headery0{\*\headeryb0\headerxl0\headerxr0\headeryh283}{\header \pard\plain \s9\cf0\qr{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033{\field{\*\fldinst \\page\\* ARABIC}{\fldrslt 27}}
\par 
\par }
\pgdscnxt2 Convert 1;}}
{\*\pgdscno0}\paperh15840\paperw12240\margl1440\margr1440\margt1440\margb1440\sectd\sbknone\pgwsxn12240\pghsxn15840\marglsxn1440\margrsxn1440\margtsxn1723\margbsxn1440\headery1440{\header \pard\plain \s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033 
\par }
\ftnbj\ftnstart1\ftnrstpg\ftnnar\aenddoc\aftnrstcont\aftnstart1\aftnnar

\pard\plain\pard\plain \absw380\absh23\nowrap\pvmrg\posyt\phmrg\posxr{\*\flymaincnt64\flyvert5489\flyhorz5121\flyanchor2\flypage1}\ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\field{\*\fldinst \\page\\* ARABIC}{\fldrslt 27}}
\par \pard

\pard\plain\pard\plain \absw440\absh23\nowrap\pvmrg\posyt\phmrg\posxr{\*\flymaincnt64\flyvert5489\flyhorz5121\flyanchor2\flypage1}\ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033{\field{\*\fldinst \\page\\* ARABIC}{\fldrslt 27}}
\par \pard
\pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\field{\*\fldinst SEQ CHAPTER \\h \\r 1}{\fldrslt }}{\loch\f0\fs20\lang4105\i0\b0{\lang4105 }}{\loch\f0\fs20\lang1033{\fs24 Imagine you are a medical doctor. How comfortable would you be prescribing a newly developed medication for which there is minimal knowledge of how it works or of its side effects? An analogous situation exists today in the psychological sciences in model
ing human behavior. Quantitative modeling has evolved into an influential and increasingly popular research tool and method of inquiry, yet our understanding of model behavior is too often quite limited (e.g., why the model behaves in a particular way and 
the range of other behaviors it exhibits). In fact, model behavior can be down right mysterious (Dawson & Shamanski, 1994; McCloskey, 1991). This observation is not a criticism about models or modeling, but a comment about the absence of methods for studyi
ng them.}}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Modeling will be most fruitful when the consequences of a modeler\rquote s design choices are understood. A similarly thorough understanding is also necesasry to make knowledgable choices in model selection. Curent model analysis tools are not capable of providin
g this insight. The purpose of this paper is to fill this gap by introducing a powerful, general-purpose tool for model analysis and selection.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b Current Methods of Model Analysis}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Much of present-day model evaluation and comparison assess what we have recently dubbed Local Model Analysis (LMA; Navarro, Pitt, & Myung, 2004). The term refers to evaluation in a specific experimental context The most frequent form of LMA is testing mode
l performance against human performance, for example, by fitting a mathematical model to the data or simulating the human pattern with a connectionist model. Because data are a reflection of the psychological process under study, a good fit to the data is 
a necessary condition a model must satisfy to be taken seriously. A good fit determines how well a model passes the sufficiency test of mimicking  human performance. It is especially useful in the early stages of model development as a quick and easy check
 on sufficiency. Quantitative measures of fit include percent variance accounted for, root mean square deviation, and maximum likelihood. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 As with any quantitative measure, it is important to be aware of their limitations of goodness-of-fit (GOF) measures. For example, they can be biased when assumptions are violated (normality in the case of root mean square deviation). Although a good fit m
akes a model a member of the class of possible contenders, this class will almost always be quite large. If two models that one is comparing fit the data similarly well, other analysis methods are needed to choose between them.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Another LMA method that is useful for probing model behavior more deeply is a sensitivity analysis, in which a model\rquote s parameters are varied around its best-fitting values to learn how robust model behavior is to slight variations of those parameters. If a
 good fit reflects a fundamental property of the model, then this behavior should be stable across reasonable parameter variation. Another reason a model should satisfy this criterion is that human data are noisy. A model should not be so sensitive that it
s behavior changes noticeably when noise is encountered. Cross validation, in which a model is fit to the second of two data sets using the best fitting parameter values from fitting the first data set, is a fit-based approach to quantifying this sensitivi
ty.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 A drawback of all LMA methods is that they are local.  Each fit provides a view of model performance in a specific experimental design (e.g., conditions, stimuli). Each view is also always in relative to how humans performed, which not only limits what we 
learn about model performance (e.g., whatelse can it fit?) but also makes it difficult to get a sense of overall model behavior when these views are combined. We are left with snapshots of model performance that are diffciut to piece together into a compre
hensive understanding of the model. The task of comparing two models is even more arduous.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 For these reasons, we have been interested in developing Global Model Analysis (GMA) techniques. They are intended to complement LMA, not replace it. The idea is that by stepping back from a particular data sample and obtaining a broader view of how a mode
l performs, we can gain a deeper understanding of model behavior and how it compares to competing models. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 One measure of GMA is a model\rquote s complexity, which quantifies its inherent data-fitting ability in an experimental design (Grunwald et al, in press; Pitt, Myung, & Zhang, 2002; Rissanen, 1996; 2001).  The method is mathematically rigorous, but a few require
ments currently limit its application to the diverse range of models in psychology. One is that it is only applicable to statistical (parametric, algebraically formulated) models. Perhaps more serious from the standpoint of GMA, it generates only a single 
value regarding model adequacy. Although a useful piece of information, what is needed is are ways to learn about the the general and atypical behavioral characteristics of a model.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 [Insert a few paragraphs describing grid search and Dunn\rquote s signed-difference analysis as methods of GMA. Describe how they work, what they are good for. The following paragraphs might need to be revised slightly to reference them or encoporate into the dis
cussion. Does this belong elsewhere? For example, we could save this to the GD, where a more extended comparison could be made.]}
\par 
\par \sect\sbknone\cols1\ltrsect
{\loch\f0\fs24\lang1033\i0\b0 Recently, Navarro et al (2004; Navarro et al, 2003; Pitt & Navarro, in press; see also Wagenmakers et al, 2004) developed a GMA method called {\i Landscaping} that attempts to achieve the same goals as grid search and signed-difference analysis while still main
taining ties to measures of model complexity. A landscape is a plot of the relative fits of two models to a range of data sets generated by the models in a particular experimental design. Two examples are shown in Figure 1. In each, the fit of each model i
s along an axis, with better moving outward from the origin. The diagonal line represents the location in this \ldblquote fit space\rdblquote  of equal fit by the two models. Points above the line indicate a better fit by model A, and points below it by model B. The lightly s
haded points were data sets generated by model A (with noise added). The reverse was the case for the dark points. The high degree of overlap of the distributions in the left panel indicates both models fit both data sets simiarly well. In this experimenta
l setup, the models are indiscriminable. The separation of the distributions in the plot on the right indicates the models are discriminable, most decisively if data were generated in the regions furthest from the criterion line, where one model provides a
 vastly superior fit than the other.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 1 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Landscaping is attractive because landscapes are relatively easy to create, requiring only a comparison of fits to data sets. In addition, landscapes can be used to assess the informativeness of data in distinguishing models by overplotting in a landscape 
data collected in past studies. In short, the landscape provides a global perspective from which to understand the relationship between two models and their fits to data.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Landscaping has two drawbacks, however. Like the complexity measure mentioned above, it too is restricted to statistical models. More importantly, comparison of only fits cannot tell us much about the functional relationship between models. For example, ar
e the models virtually isomorphic in a particular experimental setup, generating the same data patterns across the entire parameter space? Or do models generate only one or two common data patterns, one of which happens to be the empirical pattern? Whichev
er is the case, how central or peripheral is the human pattern in model performance? Questions like these, which are at the heart of GMA, cannot be addressed by GOF measures alone.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 We introduce a general-purpose version of landscaping that overcomes these two hurdles. It is called {\i Parameter Space Partitioning }(PSP), and involves doing exactly what the name implies: A model\rquote s parameter space is literally partitioned into regions that 
correspond to the data patterns that could be generated in an experiment. These partitions can then be studied to learn about a model\rquote s behavior and compared across models to assess their similarities.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033{\loch\f0\fs24\lang1033\i0\b{\b Parameter Space Partitioning:}}{\loch\f0\fs24\lang1033 {\b Peering into the Black Box of Model Behavior}}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 To illustrate PSP, consider a visual word recognition experiment in which participants are asked to categorize stimuli as words or  nonwords and response time to words is measured as the dependent variable across  three experimental conditions, A, B and C.
 Suppose we are interested in the ordinal relationship (fastest to slowest) across conditions. In this case, there are 13 possible orderings (including equalities) that can be observed across the three conditions (e.g., A > B > C, A > B = C, B > C = A, etc
). Each of these orderings defines a data pattern. Suppose that mean participant performance yielded the pattern B > C > A. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Now consider two hypothetical models, M{{\*\updnprop5800}\dn6 1} and M{{\*\updnprop5800}\dn6 2}, of word recognition, each with two parameters. Using PSP, we can answer the following questions about the relationship between the models and the empirical data generated in the experiment: How many of the t
hirteen data patterns can each model produce? What part of the parameter space includes the empirical pattern? How much of the space is occupied by the empirical pattern? What data patterns are found in nearby regions as well as the rest of the parameter s
pace?}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Figure 2 shows the parameter space of each model partitioned into the data patterns it can generate. Model M{{\*\updnprop5800}\dn6 1} produces three, one of which is the emipircal pattern. Note how it is central to model performance, occupying the largest portion of the parameter
 space. Even though the model generates two other patterns, they are smaller and differ minimally from the empirical pattern. In contrast, M{{\*\updnprop5800}\dn6 2} produces nine of the thirteen patterns. Although one is the empirical pattern, M{{\*\updnprop5800}\dn6 2}'s performance is not impressive 
because it can mimic almost any pattern that could be generated in the experiment. Its predictive power is too great. Indeed, that M{{\*\updnprop5800}\dn6 2} can mimic human performance seems almost incidental. Not only does the empirical pattern occupy a small region of the para
meter space but larger regions are produced by patterns that do not human-like (e.g., C>A>B).}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 2 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \pard\plain \ltrpar\s9\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Challenges in Implementing PSP}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The preceding example illustrates the gist of the method and describes some of what can be learned with it. Implementation of the method requires solving two non-trivial problems. One is how to define a data pattern, and the other is how to devise an effic
ient search algorithm to find the data patterns.}
\par {\loch\f0\fs24\lang1033\i0\b0  \tab \tab \tab \tab \tab \tab \tab \tab \tab }
\par {\loch\f0\fs24\lang1033\i0\b0 Each set of a model\rquote s parameter values generates a model output, but not every model output is a distinct data pattern. How many qualitatively \ldblquote different\rdblquote  outputs can a model produce? Answering this question, which is what PSP enables us to do, depends cri
tically on how a data pattern is defined. This is something which will vary from experiment to experiment, and indeed may vary within an experiment depending upon what a researcher wants to learn. Although there is no general solution, the scientist usuall
y knows what patterns should be found in order to support or falsify a model. Most of the time, ordinal predictions are being tested. In this case, a \ldblquote natural\rdblquote  definition of a data pattern is the ordinal relationship of model outputs, as in the above examp
le. Nevertheless, it is a good idea to try out a couple of different definitions and to perform sensitivity analyses to ascertain if and to what extent conclusions obtained under one definition hold across others. An example of this is presented later in t
he paper.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Once a data pattern is defined, the next challenge is to find all patterns a model can simulate. The data space by definition is made up of all patterns that can be observed given an experimental design, however improbable. The space may contain a huge num
ber of patterns, only a small fraction of which correspond to a model\rquote s predictions, so it is essential to use an efficient search algorithm that finds all of the patterns the model can generate in a reasonable amount of time.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The PSP Algorithm}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The problem to be solved is to partition a multi-dimensional parameter space into an unknown number of regions, each of which by definition corresponds to a unique data pattern.. Given the dimensionality of the parameter space of most models is quite high,
 brute force search methods to find all regions, such as Simple Monte Carlo (SMC; a random search procedure) will not work (or take far too long to succeed), precisely because the search is random. Markov Chain Monte Carlo (MCMC; Gilk et al, 1996) is a muc
h more sophisticated sampling method that we encorporated into an algorithm that efficiently finds all regions.}
\par \pard\plain \ltrpar\s10\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Application of the PSP algorithm begins with a starting set of parameter values at which the model can generate a valid data pattern (i.e., one that satisfies the definition). This initial set can be supplied by the modeler or from an exploratory run using
 SMC. Given the parameter set and the corresponding data pattern generated by the model, the algorithm samples nearby points in the parameter space to map the region that defines the data pattern. MCMC is used to approximate quickly and accurately the shap
e of this region. The process then begins anew by sampling a nearby point just outside of this region.}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Figure 3 illustrates how the algorithm works in the space of a two-parameter model.  The process begins with the initial parameter set serving as the current point in the parameter space (solid point in panel a). A {\i candidate} sample point (shaded point) is 
drawn from a small, predefined region, called a  jumping distribution, centered at the current point. The model is then run with the candidate parameter values and its output is evaluated to determine if the data pattern is the same as that generated by th
e initial point. If so, the candidate point is accepted as the next point from which another candidate point is drawn. If the new candidate point does not yield the same data pattern as the initial one, it is rejected as belonging to the current region. An
other jump from the initial point is attempted, accepting those points that yield the same data pattern. The sequence of all accepted points recorded across all trials is called the {\i Markov chain }corresponding to the current data pattern. This sample of poi
nts is used to estimate the size of the region occupied by the data pattern (panel b). The theory of MCMC guarantees that the sample of accepted points will eventually be distributed uniformly over the region. This feature of MCMC allows us to estimate acc
urately the volume occupied by the region, regardless of its size. }
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 3 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Every rejected point (shaded point in panel b), which must be outside the current region, is checked to see if it generates a new valid data pattern. If so, a new Markov chain corresponding to the newly discovered pattern is started to define the new regio
n. In effect, accepted points are used to shift the jumping distribution around inside the current region to map it completely, whereas rejected ones are used initiate new MCMC search processes to map new regions. Over time, as many search processes as the
re are unique data patterns will be run. Additional details about the algorithm are described in Appendix A.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Algorithm Evaluation}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 We tested the accuracy of the PSP algorithm by measuring its ability to find all of the data patterns defined for a particular model. The difficulty of the search problem was varied by manipulating the definition of a pattern (i.e., number of patterns) and
 the number of parameters in the model. The extent of both (see Table 1) was deliberately made large to make the test  challenging The efficiency of the algorithm was measured by comparing its performance to SMC (random search).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The model was a hypercube whose dimensionality {\i d} (i.e., number of parameters) was 5, 10, or 15.To illustrate the evaluation method, a two-dimensional model (d = 2) is depicted in Figure 4 that contains twenty data regions (outlined in bold) that the algori
thm had to find. Note that a large portion of the space does not produce any valid data patterns. Also note that the sizes of the data regions vary a great deal. Some are elicited by a wide range of parameter values whereas others can be produced only by a
 small ranges of values. This contrast grows exponentially as the dimensionality of the model increases, and was purposefully introduced into the test to make the search difficult and approximate the complexities (i.e., nonlinearities) in cognitive models.
 Ten independent runs of each search method were carried out to assess the reliability of algorithm performance. }
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figures 4 and 5 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Figure 5 shows performance of the PSP algorithm for a search problem in which there were one hundred regions embedded in a 10-dimensional hypercube (d=10). The PSP algorithm found all regions and did so in nine minutes. SMC found only about 23 patterns in 
nine minutes, and given its sluggish performance, it seems doubtful that SMC would find all of them in anything close to a reasonable amount of time.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Table 1 summarizes results from the complete test. The mean proportion of patterns found is listed in each cell. Results are clear and consistent. The PSP algorithm almost always found all of the patterns whereas SMC failed to do so in every condition. Mos
t noteworthy is the success of the PSP algorithm in the toughest situation, when there were 15 parameters and 500 data patterns. Its near perfect success suggests it is likely to perform admirably in other testing situations. In the remainder of this paper
, we describe its applications to analyzing performance of a single model and to comparing design differences between models.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Table 1 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b Evaluating Global Model Behavior in Relation to Empirical Data: A Test of ALCOVE}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Differences between LMA and GMA are most apparent when methods of each type are used to evaluate a model in the context of a new empirical finding. In LMA, a model\rquote s ability to fit (or simulate) the data is taken as evidence that it approximates the underl
ying cognitive process. In GMA, the definition of \ldblquote fit\rdblquote  is relaxed to be a qualitative, ordinal relation on the same scale as the experimental predictions themselves. Model performance is then evaluated by determining how many of the possible orderings in 
the experiment can it produce and and how central is the empirical pattern among them (Figure 2). In essence, the local precision of LMA is replaced by knowledge of the model\rquote s inherent pattern-producing ability and the centrality or exclusivity of the emp
irical pattern.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 In this section, we examined the behavior of ALCOVE (Kruschke 1992), an exemplar-based account of human category learning, in the context of the seminal Shepard, Hovland, and Jenkins (1961) experiment. While there are some category learning effects that it
 does not capture without extension or modification (e.g, Kruschke & Erikson 1995, Lee & Navarro 2002), ALCOVE remains a simple and powerful account of a broad range of phenomena.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The ALCOVE  Model}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 In some category learning experiments, participants are shown a sequence of stimuli, each of which possesses some unknown category label. The task is to learn which labels go with which stimuli, using the feedback provided after responses are made. ALCOVE 
solves this problem in the following way (for a detailed description, see Kruschke 1992). When stimulus {\i i} is presented to ALCOVE, its similarity to each of the previously stored exemplars, {\i s{{\*\updnprop5800}\dn6 ij}}, is calculated. Following Shepard (1987), similarity is assumed
 to decay exponentially (with a width parameter {\i c}) as a function of the attention-weighted city-block distance between the two stimuli in an appropriate psychological space. After estimating these similarities, ALCOVE forms response strengths for each of t
he possible categories. These are calculated using associative weights maintained between each of the stimuli and the categories. The probability of choosing the {\i k}-th category follows the choice rule (Luce, 1963) with parameter {\i\f2 \u966 ?}.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Having produced probabilities for each of the various possible categorization responses, ALCOVE is provided with feedback from an external source. This takes the form of a \ldblquote humble teacher\rdblquote  vector, in which learning is only required in cases where the wrong
 response was made.  Two learning rules are then applied, both derived by seeking to minimize the sum-squared error between the response strengths and the teaching vector, using a simple gradient descent approach to optimization. Using these rules, ALCOVE 
updates the associative weights (with parameter {\i{\f2 \u951 ?}}{\i\i\i{{\*\updnprop5800}\dn6 w}} for the learning rate) and the attention weights (with a learning rate parameter {\i{\f2 \u951 ?}}{\i\i\i{{\*\updnprop5800}\dn6 a}}) prior to observing the next stimulus.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The Shepard, Hovland and Jenkins Task}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 In a classic experiment, Shepard, Hovland and Jenkins (1961) studied human performance in a category learning task involving eight stimuli divided evenly between two categories. The stimuli were generated by varying exhaustively three binary dimensions suc
h as color (black vs. white), size (small vs. large) and shape (square vs. triangle). They observed that, if these dimensions are regarded as interchangeable, there are only six possible category structures across the stimulus set, illustrated in Figure 6a
. This means, for example, that the category structure that divides all squares into one category, and all triangles into the other is regarded as equivalent to the category structure that divides small shapes from large ones, as shown in the lower right.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Empirically, Shepard et al. (1961) found robust differences in the way in which each of the six fundamental category structures was learned. In particular, by measuring the mean number of errors made by subjects in learning each type of category structure,
 they found that Type I was learned more easily than Type II, which in turn was learned more easily than Types III, IV and V (which all had similar error measures), and that Type VI was the most difficult to learn.  More recently, Nosofsky, Gluck, Palmeri,
 McKinley and Glauthier (1994), replicated Shepard et al.'s (1961) task using many more subjects, and reported detailed information relating to the learning curves.  Figure 6b shows the mean proportion of errors for each category type. Consistent with the 
conclusions originally drawn by Shepard et al. (1961), it is generally held that the theoretically important qualitative trend in these data is the finding that there is a natural ordering on these curves, namely that I<II<(III, IV, V)<VI. This kind of pat
tern is called a weak order, since the possibility of ties is allowed.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033{\loch\f0\fs24\lang1033\i\b0\i Figure 6 here}
\par \pard\plain \ltrpar\s1\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\loch\f0\fs24\lang1033\i0\b0\fs24 The psychological importance of this weak order structure is substantial. Suppose we had two models of the category learning process, {\i A} and {\i B}{\i0 , of roughly equal complexity. M}odel {\i A} provides a reasonably good quantitative fit to the data, by assuming that al
l types are learned at the same rate, which closely approximates the average across the six empirical curves. In contrast, model {\b0\i B} reproduces the ordering I<II<(III, IV, V)<VI, but learns far too slowly and, as a result, fits the data much worse than model
 {\i A}. Since the models are of equivalent complexity, a classical model selection analysis would prefer model {\i A}. However, while clearly both models have some flaws, most psychologists would prefer model B, because it captures the{\i  theoretically relevant }proper
ty of the data. This discrepancy arises because statistical criteria tend to assume that all properties of the data are equally relevant. In many psychological applications, this is not the case. In what follows, we will assume that the weak order structur
e is the important theoretical property of the empirical data, and will use the PSP method to ask how effectively ALCOVE captures this structure.}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033{\loch\f0\fs24\lang1033\i0\b0\ul The PSP Analysis}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 As with any parameterized model, ALCOVE makes different predictions at different parameter values {\b (CITATION: the paper that Tom Palmeri sent Jay??)} . When applied to the Shepard et al. (1961) task, ALCOVE will sometimes produce curves that have the same qu
alitative ordering as the empirical data, but at other times they will look quite different. It would be nice to know something about the {\i other} orders that ALCOVE can produce, since it seems that we might learn something about the model itself. A PSP analy
sis can provide such information. Using the \ldblquote weak order\rdblquote  definition of a pattern of curves, there are 4683 different data patterns that a model could produce. One would hope that ALCOVE generates only a small proportion of these, and that the extra pattern
s it does produce are interpretable in terms of human performance. }
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Preliminaries}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 We now have a model, a data set, and an intuitive definition of a data pattern. To perform PSP analyses, a formal method of associating a set of learning curves with a particular pattern must be rigorously defined. The judgement that I<II<(III, IV, V)<VI i
s the appropriate empirical pattern has generally been based on visual inspection of the curves. It is possible to be more precise about this, allowing us to uniquely associate a set of learning curves with a qualitative ordering to yield a data pattern. T
he details of this procedure, which is essentially a clustering analysis, are provided in Appendix B. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 For the PSP analysis of ALCOVE, we constrained the parameter vectors ({\i c},{\i{\f3  }}{\i\i\i{\f2 \u966 ?}},{\i{\f3   }}{\i\i\i{\f2 \u951 ?}{{\*\updnprop5800}\dn6 w}}, {\i{\f2 \u951 ?}}{\i\i\i{{\*\updnprop5800}\dn6 a}}) to lie between (0,0,0,0) and (20,6,.2,.2), and disallowed any parameter combination that did not produce monotonic curves. A technical complication is introduced by the fa
ct that ALCOVE\rquote s predictions are slightly dependent on the order in which stimuli are observed. Each stimulus has a different effect on ALCOVE, so variations in order of presentation produce slight perturbations in the response curves. However, even these 
minor perturbations can violate the continuity assumptions that underlie the PSP algorithm. In order to deal with these order effects, we chose 20 random stimulus orders, and ran the PSP algorithm 10 times for each stimulus order, yielding a total of 200 r
uns. As it turns out, the important properties of ALCOVE are invariant under stimulus reordering, but some unimportant properties are not. }
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 How Many Data Patterns can ALCOVE Produce?}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 After running the PSP algorithm 200  times, we observed that each run produced a different number of patterns, ranging from a minimum of 32 to a maximum of 122. Although this range is substantial, it reflects an inherent variability in ALCOVE moreso than t
he PSP algorithm itself. The mean number of patterns recovered for a particular stimulus order ranged from 46.5 to 102.8, while the range in the number of patterns recovered within an order was minimal: the smallest range was a mere 7 patterns, while the l
argest was 36. Moreover, there was an important amount of redundancy across the 200 runs, with 17 patterns being found on every occasion, which included the empirical pattern I<II<(III,IV,V)<VI. We will refer to these 17 patterns as \ldblquote universal\rdblquote  patterns, a
nd the other 183 patterns as \ldblquote particular\rdblquote  patterns. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Compared to the set of all 4683 possible data patterns, even the largest count of 17+183=200 patterns encompassed by ALCOVE is quite a small number. In a sense, this is quite a success for the model, because the empirical pattern suddenly looks far less un
likely if we assume humans do something rather ALCOVE-like. Even in the scenario where we allow {\i all} 200 recovered patterns to be treated as a genuine ALCOVE prediction, the empirical pattern is one 1 pattern in 200, rising from the much less satisfying bas
e rate of 1 in 4683. Even if the substantive predictions are restricted to the set of universal patterns, the empirical pattern is now 1 in 17. Either way, ALCOVE provides a reasonably good qualitative account of these data.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 This initial analysis demonstrates that ALCOVE passes a basic sufficiency test, in that it can account for the qualitative structure of the observed data without \ldblquote going overboard\rdblquote , and producing every possible pattern. Of course, as psychologists, we are i
nterested in more than just how many patterns that ALCOVE, and this analysis is just the tip of the iceberg as far as what can be learned from PSP. For example, it would be useful to know what kinds of category-type orderings are generally preserved across
 all 200 data patterns. One crude method for determining this is to find the average position (i.e., its rank among the six curves) of each category type across all patterns. This is illustrated in Figure 7, which plots the mean rank for each of the six ty
pes across all patterns for both universal and particular patterns. It is clear that  rank tends to increase as the index of the type increases. The main difference between the two types of patterns is that the universals do not really distinguish between 
Types II, III and possibly IV, whereas the particulars do. Nevertheless, it seems to be the case that, on average, rank does not decrease with index. This is encouraging, because both empirically and algebraically, the difficulty of the task either increas
es or stays constant as the index increases (see Feldman 2000). This analysis demonstrates that on average, the set of 200 patterns that make up ALCOVE\rquote s entire set of qualitative predictions roughly preserve an important property of the empirical data: A 
monotonic increase in learning difficulty across category type.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 7  here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Which Patterns Matter?}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The preceding analysis considered the collection of all 200 patterns as equally informative. However, the very fact that some are frequent (universals) and others rare (particulars) suggests that some may be more representative of model behavior than other
s. We explore this possibility here.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The frequency with which a pattern is found strikes us as an unsatisfying definition of its importance to a model\rquote s behavior, since it confounds the model properties with the robustness of the search algorithm. It was therefore necessary to make some assum
ptions that allow us to identify the major patterns that are responsible for most of ALCOVE\rquote s behavior. If we accept the notion that ALCOVE's parameters are interpretable and psychologically well-founded (see Kruschke 1993), then it makes sense to treat th
e parameter space itself as embodying not just its full range of behaviors, but those that best characterize its performance. Specifically, if a pattern can be produced only within a tiny region in the parameter space, then it is probably safe to dismiss i
t as largely irrelevant to the model. By doing this for all data patterns, we can estimate the proportion of the parameter space that is taken up by each, and then use these quantities to identify the most prevalent patterns.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Note that the outcome of such an analysis depends on the manner in which ALCOVE\rquote s parameters are formalized. In statistical terms, the conclusions are no longer invariant under reparametrization. This is not necessarily a bad thing, so long as we have some
 principled prior reason on how to parametrize the model. Arguably, the nature of the exemplar theory on which ALCOVE is based, and the manner in which the model captures Kruschke\rquote s (1993) \ldblquote three principles\rdblquote , provide exactly this kind of justification. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 As it turns out, there is a strong relationship between the size (i.e., volume) of the region occuppied by a pattern and the frequency with which it was discovered across the 200 runs. In Figure 8, the log of the average volume for each of the 200 patterns
 discovered is plotted against the frequency with which they were discovered. Patterns shown on the far left are the ultimate particulars, having been discovered only once, while patterns on the far right are the universals, having been discovered on every
 occasion. Noting that the scale is logarithmic, we observe that the universals are by far the largest patterns. The empirical pattern, indicated by the circle, is one of the larger patterns, and is a universal.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 8 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 The data in Figure 8 let us refine the answer to the question, \ldblquote To what extent does ALCOVE {\i predict} the empirical pattern?\rdblquote  The fact that the empirical pattern is among the 17 universals is encouraging, as is the regularity suggested by Figure 7. However, b
y considering the size of the various regions, we can take this analysis a step further. We could, for instance, exclude all patterns that do not reach some minimum average size. This approach is illustrated by the horizontal threshold shown in Figure 8: o
nly patterns that occupy more than 1% of the parameter space on average lie above this line. This is a pretty stringent test, given that the parameter space is four-dimensional. Indeed, the empirical pattern occupies only about 2% of the space. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 In total, only twelve patterns (all universals) occupy more than 1% of the space, as shown in Table 2, and some general properties of ALCOVE\rquote s behavior emerge when examined together. Looking across patterns, it is clear that Types III and IV are always (12
 of 12) predicted to be learned at about the same rate, and Type V is usually (10 of 12) also about the same. Type VI, on the other hand, is mostly learned slower than III, IV and V (7 of 12). Type I is usually (9 of 12) faster than III-VI, as is Type II (
8 of 12). So, not only is the empirically-observed pattern I<II<(III,IV,V)<VI among the largest patterns (it is the eighth largest), but the other large patterns generally preserve most of the pairwise relations found in the empirical data. They are, in sh
ort, \ldblquote close\rdblquote  to the empirical pattern. The exception to this claim regards the relationship between Types I and II. Their ordering is ambiguous. It might be that I<II (5 of 12), or I=II (4 of 12), or even II<I (3 of 12). In this case, ALCOVE does not make 
a strong qualitative prediction.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Table 2 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Summary}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 These analyses demonstrate that ALCOVE makes strong and accurate predictions about the qualitative structure of the data that should be found in the Shepard et al (1961) task.}
\par {\loch\f0\fs24\lang1033\i0\b0 [Review types of analyses that were performed and what was found.]}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b Understanding Design Differences Between Models: }
\par {\loch\f0\fs24\lang1033\i0\b A Comparison of Merge and TRACE}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 In addition to learning about the behavior of a single model, PSP analyses can inform us about the behavioral consequences of design differences between models. In this section, we demonstrate its application to comparing two localist connectionist models 
of speech perception. Connectionist models were chosen to show how PSP analyses can help make inroads into understanding the behaviors of highly nonlinear models that results from the interconnectedness among their parts. The localist variety was chosen be
cause of its popularity in some content areas (e.g., language, memory; Grainger & Jacobs, 1998). This particular pair of models,  TRACE (McClelland & Elman, 1986) and Merge (Norris, McQueen, & Cutler, 2000), was chosen to learn the extent to which their sl
ight architectural differences make them functionally distinct, and because we had previously worked with them (Brunsman, Myung, & Pitt, 1999). }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Schematic diagrams of the designs of each model are shown in Figure 9. They are similar in many ways. Both have a phonemic input stage and a word stage. There are excitatory connections from the phoneme input to the word stages, and inhibitory connections 
within the word stage. They differ in how prior knowledge is combined with phonemic input to yield a phonemic decision (percept). In TRACE, word (prior) knowledge can directly affect sensory processing of phonemes. This is represented by direct excitatory 
connections from the word stage back down to the phoneme stage. Also note that in TRACE the phoneme stages performs double duty, also serving as a phoneme decision stage. In Merge, these two duties are purposefully separated into two distinct stages to pre
vent word information from affecting sensory registration of phonemes. Instead, lexical knowledge affects phoneme identification via excitatory connections from the word to the phoneme-decision stage. In contrast to the direct interaction between phoneme a
nd word levels in TRACE, these two sources of information are integrated at a later decision stage in Merge.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 9 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Although not visible in the diagrams in Figure 9, the models differ in another important way. In keeping with the belief that bottom-up (sensory) information must take priority in guiding perception early in processing, activation of a phoneme decision nod
e in Merge must be initiated by phoneme input {\i before }excitatory lexical activation can affect phoneme decision making. TRACE contains no such constraint.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The goal of our investigation was to assess the impact of these two design differences on {\i global }model behavior. Norris et al (2000) proposed Merge as an alternative to TRACE because they felt the evidence from the experimental literature did not warrant a
 conclusion as strong as direct word-to-phoneme feedback (i.e., interaction). Integration of phonemic and word information at a later decision stage is, in their view, more in line with the data.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The adequacy of the Merge architecture was demonstrated in simulation tests in which it performed just as well as, if not slightly better than, TRACE in reproducing key experimental findings of how word knowledge affects phoneme processing.  PSP analyses o
f the models\rquote  behaviors were performed in two of these experimental settings. In the first, the subcategorical mismatch study by Marslen-Wilson and  Warren (1994), the consequences of splitting  phoneme processing into separate input and decision stages wa
s evaluated. In the second, the indirect inhibition experiment of Frauenfelder, Segui, and Dijkstra (1990), the contribution of the bottom-up priority rule to model performance was examined.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 To compare the two models, it was necessary to equate them in every way possible to ensure that differences in performance were attributable only to design differences, not other factors, such as the size of the lexicon. In essence, we wanted to compare th
e fundamental structural and functional properties that define the models. We did this by first implementing the version of Merge described in Norris et al (2000), and then making the necessary changes to Merge to turn it into TRACE (Norris et al essential
ly did this). When finished, the source code for the two models, written in Matlab, was identical except for the sections that corresponded to their design differences. TRACE required 4 fewer parameters than Merge (8 vs. 12) because the phoneme input and d
ecision stages were combined. The names of the parameters, along with other model details, are in Appendix C.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Interaction vs Integration}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The first comparison of the two models was performed in the context of the subcategorical mismatch experiment of Marslen-Wilson and Warren (1994, Experiment 1; McQueen, Norris, & Cutler, 1999, Experiment 3).  It is attractive because of the large number of
 conditions and response alternatives, which together permitted detailed analyses of model behavior at both the phonemic and lexical levels.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 In the experiment, listeners heard one-syllable utterances and then had to classify them as words or nonwords (lexical decision task) or categorize the final phoneme (phonemic decision task). The stimuli were made by appending a phoneme (e.g., /b/ or /z/) 
that was excised from the end of a word (e.g.,{\i  job}) or nonword (e.g., {\i joz}) to three preceding contexts, to yield six experimental conditions (listed in Table 3). The first context was a new token of those same items but with the final consonant removed (e.
g., {\i jo}), to create cross-spliced versions of  {\i job }and {\i joz}. The second consisted of equivalent stretches of speech from a word that differed only in the final consonant (e.g,, {\i jo }from {\i jog }in both cases). The third was the same as the second except that the 
initial parts were excised from two nonwords (e.g., {\i jo }from {\i jod }and {\i jo }from {\i jov}). }
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Table 3 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Because cues to phoneme identity overlap in time (due to coarticulation in speech), a consequence of cross-splicing is that cues to the identity of the final consonant will conflict when the first word ends in a consonant different from the second. For exa
mple,  {\i jo }from {\i jog} contains cues to /g/ at the end of the vowel, which will be present in the resulting stimulus when combined with the /b/ from {\i job} (Condition 2 in Table 3).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Marslen-Wilson and Warren (1994) were interested in how such stimuli affect phoneme and lexical processing. As the results in Table 3 show (taken from McQueen et al, 1999), in the lexical decision task, reaction times slowed when listeners heard cross-spli
ced stimuli, but responding was not affected by the source of the conflicting cues (i.e., Conditions 2 and 3 are equivalent).  Phoneme categorization, in contrast, was sensitive to the subtle variation in phonetic detail, but only when the stimulus itself 
formed a nonword (e.g., {\i joz}; Conditions 5 and 6).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Merge can simulate this complex pattern of results. TRACE can as well if the number of processing cycles is increased from one to 15 (Norris et al, 2000). As with ALCOVE, a PSP analysis will reveal what other data patterns (i.e., lexical/phoneme classifica
tion responses) these models can generate in the experimental design. By comparing their frequency, similarity, and the centrality of the empirical data among all patterns, the consequences of splitting the phoneme level in two should begin to emerge. This
 comparison can also be extended to the reaction time data to determine if ordinal relations across conditions are maintained or violated.}
\par {\loch\f0\fs24\lang1033\i0\b0 To the extent that differences are found across models, they are more than likely a result of their different structural properties (one phoneme stage vs two). The use of cross-spliced stimuli nullifies the bottom-up priority rule in Merge, which otherwise
 might have contributed to any differences (see Norris et al, 2000, for details).}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Details of the PSP analysis}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Recall that a PSP analysis requires the researcher to define a data pattern in the experimental setting. Frequently this will take the form of ordinal relations among conditions. In the subcategorical mismatch experiment, there were two dependent measures 
of performance, classification (What was the final phoneme? Was the utterance a word?) and the speed with which this decision was made (response time). Rather than define a data pattern using both measures simultaneously, we first compared classification b
ehavior of the models within their parameter spaces and then performed a focused investigation of the RT predictions within the region occupied by the empirical data pattern (i.e., listener performance).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Classification in connectionist models is usually defined as the activation state of the network when specific decision criteria are met, such as a phoneme node exceeding an activation threshold. Because there are multiple experimental conditions in the ex
periment, as with the ALCOVE analysis, a data pattern is really a profile of model classifications across these conditions. In the subcategorical mismatch experiment, there are six conditions, with a phoneme and lexical response in each, for a total of 12 
categorization responses that together yield a single data pattern.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Given the large number of conditions and multiple response alternatives in each task, the total number of possible data patterns is enormous. To understand why, take a look at the right half of Table 3, which lays out the conditions and response categories
 for each task in the model simulations. There were four possible phoneme responses, /b/, /g/, /z/, /v/.  In lexical decision, there were three categories, a nonword response and two word responses, \ldblquote job\rdblquote  and \ldblquote jog.\rdblquote  The reason for distinguishing between th
e words was that just as listeners would perceive two different words, we were interested in whether the models, if given the opportunity, would differentiate between them as well. The cells of this 6x7 response matrix that contain asterisks denote human c
lassification performance and constitute the empirical data pattern. A change in any of these responses produces a new data pattern. With four phoneme and 3 lexical responses, there were a total of 2,985,984 (4{{\*\updnprop5800}\up6 6} x 3{{\*\updnprop5800}\up6 6}) patterns. Of interest is how many and 
which of these patterns TRACE and Merge produce.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Not one but two decision rules were used to define a categorization response by the models. The effect of these rules is that they establish the necessary mapping between the continuous space of network states to the discrete space of data patterns, making
 it possible to associate each data pattern with a region in parameter space. Because any one rule could yield a distorted view of model performance, the use of two enabled us to assess the generality of the results. In addition, we found that some model p
roperties that are not evident with one criterion emerged when performance was compared across the rules. The first rule, labeled {\i weak threshold}, was a fixed activation threshold, with values of .4 for phoneme nodes and .2 for lexical nodes. It is the same
 rule used by Norris et al (2000), and was adopted to maintain continuity across studies.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The second rule, dubbed {\i stringent threshold}, required classification to be more decisive by requiring the activation level of competing nodes to be significantly lower than the winning node. Two threholds were used, the higher of which is the lower bound f
or the chosen node and the lower of which is the upper bound for the nearest competitor. These values were .45 and .25 for phoneme classification, and .25 and .15 for lexical decision. Two other constraints were also enforced as part of the stringent threh
old. There had to be a minimum difference in activation between the winning node and its closest competitor of .3 for phoneme classification and .15 for lexical decision. Finally, for nonword responses in lexical decision, the difference in activation betw
een the two lexical items, {\i jog} and {\i job}, could not be more than .10.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The models were designed as depicted in Figure 9. {\i job} and {\i jog }were the only two lexical nodes and there were the necessary phoneme nodes, with  /b/, /z/, /g/, /v/ being of  primary interest. The PSP algorithm was run using each model and decision rule. It 
is important to understand that when a model simulation was performed with a set of parameter values, to obtain a data pattern, all six stimuli (Table 3) had to be fed to the model and both phoneme and lexical classification responses assessed. The algorit
m and models were run in Matlab  on a Pentium IV computer. The time required to find all patterns varied greatly, taking as little as 22 minutes (TRACE, stringent threshold) and as long as 24 hours (Merge, stringent threshold).  The consistency of results 
was ascertained using five multiple runs for each model and threshold.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Classification Analyses}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Adoption of almost any decision rule means that not every model simulation with a given set of parameter values will yield a response pattern that satisfies the rule. Failure to do so can be for a variety of reasons, such as no node reaching threshold or m
ultiple nodes exceeding threshold. As a result, there can be sizable regions of the parameter space that contain what can be considered invalid (no-response or strange-response) patterns. Athough the study and comparison of such patterns across models coul
d be informative, we restricted our analyses to those regions that yielded patterns that satistifed one of the two rules (i.e., valid patterns).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The analysis began by comparing classification (asymmtopic) performance of the two models. Table 4 contains a number of measures that define their relationship under the weak and stringent thresholds. The second column contains venn diagrams that depict th
e similarity relation between the models when measured in terms of the data patterns each can generate. Looking first at the weak threshold data, both models generate 22 common patterns, with TRACE producing only a few unique patterns compared to Merge (3 
vs 29). The filled dot represents the empirical pattern. Its location in the intersection indicates that both models can produce it.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Table 4 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 The nature of the overlap in the diagram reflects the fact that TRACE is virtually nested within Merge, with 22 of its 25 patterns also being ones that Merge produces. This nested relationship, combined with the 29 extra Merge patterns, suggests that in th
e subcategorical design, Merge is a more flexible model than TRACE. That is, Merge\rquote s ability to generate more data patterns makes it more complex than TRACE (Myung, 2000; Pitt et al, 2002). That said, the difference is it not all that great, although it do
es not disappear under further scrutiny (see below). Perhaps most impressively, both models are highly constrained in their performance, generating fewer than 60 of the some 3 million patterns that are possible in the design.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 As might be expected when the stringent threshold is imposed, both models generate fewer patterns, with TRACE producing 7 (72% fewer) and Merge 32 (27% fewer). Despite the differential effects of the change in threshold, the relationship between the models
 remains unchanged, with TRACE still nested within Merge. One surprise is that the lone unique TRACE pattern turns out to be the empirical data, which Merge no longer generates. What stands out most in a comparison of the venn diagrams is that the change i
n threshold primarily caused a drop in the number of common patterns, indicating that the models are more distinct under the stringent threshold. To understand why, the regions in parameter space occcupied by these patterns must be examined.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Columns 3 and 4 contain estimates of the proportion of the parameter space occupied by the valid data patterns. Far less of TRACE\rquote s parameter space is used than Merge\rquote s. Predictably, the regions shrink when the stringent threshold is applied, although the 
shrinkage is much more dramatic for Merge than TRACE. If one then examines how much of each valid volume is occupied by common and unique patterns, an answer to the above question presents itself. For TRACE (column 5), this region is occupied almost entire
ly by the 22 common patterns (.99). Under the stringent threhold, this value drops slightly to .84, but because there is only one unique pattern in this case, its value must be .16, the size of the region occupied by the empirical pattern. The remaining 15
 common patterns occupied such tiny regions in TRACE\rquote s parameter space under the weak threshold that application of the stringent threshold eliminated them. A similar situation occurred with Merge (column 6), with 12 of the 16 common patterns (of which the
 empirical pattern was one) disappearing due to a change in threshold. Four became unique to Merge. A few patterns unique to each model also failed to satisfy the stringent threshold (3 for TRACE and 7 for Merge).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Although a change in threshold brings out differences in the models, analysis of their common patterns and their behaviors in other ways reveals an impressive degree of similarity. For example, under the weak threshold, the regions in parameter space of al
l 22 patterns are comparable in size across models. To measure this, we correlated the rank orderings (from smallest to largest) of the volumes in the two models. Use of the actual volume estimates themselves is inappropriate because of differences in mode
l structure and  parameterization. With rho= .79, the correlation is high. For the stringent threshold it is even higher,  rho=1.0, but keep in mind that there were only six data points in this analysis. Such strong associations indicates that the overlap 
between models is not just nominal in terms of shared data patterns, but those regions are similar in relative size with all other common regions, making them functionally highly similar at a qualitative level.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 To the extent that a model produces data patterns other than the empirical one, a mark of a good model is for its performance to degrade gracefully. In the present context, this means that deviations from the empirical pattern should be minor, not severe. 
We measured the degree of deviation by comparing each mismatching pattern (common and unique) with the empirical pattern and counting the number of mismatches. Because predictions across 12 conditions constitute a data pattern, there were a maximum of 12 m
ismatches (6 phonemic and 6 lexical). Histograms of the mismatch distances were create for both models and are show in Figure 10, along with a histogram of the frequency of all possible mismatches.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 10 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Both models show a remarkable proclivity to produce human-like data. The distributions are positioned near the zero endpoint (empirical pattern) with peaks between 2 and 4 mistmatches. The probability is virtually zero that a random model (i.e., a random s
ample of patterns) would display such a low mismatch frequency. Changes in threshold shift the distributions ever so slightly righward. Both Merge distributions are shifted slightly to the right of the TRACE distributions. This latter result is likely due 
to Merge\rquote s unique patterns given that most of TRACE\rquote s patterns are nested within Merge\rquote s. [Footnote]}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 An equally desireable property of a model is that patterns that mismatch across many conditions occupy a much smaller region in the parameter space than those that mismatch by only one or two conditions. That is, mismatching patterns on the right side of t
he histograms in Figure 10 should have smaller volumes than those on the left. This turns out to be the case for both models and to a similar extent. When pattern volume is correlated with mismatch distance, there is a modest relationship between the measu
res (r=.35 for TRACE and .34 for Merge).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The data in Figure 10 raise questions such as what types of classification errors were made and whether they were the same across models? To answer them, the proportion of mismatches in each of the 12 conditions was computed for each model and are plotted 
in Figure 11. The profile of mismatches across conditions reveals more similarities than differences between the models. In 10 of the 12 conditions, both models performed similarly. Neither model produced a single mismatch in the same four conditions, and 
both models produced mismatches in the same six conditions. Looking across all conditions, model behavior differs in two global ways. Merge produced more phoneme misclassifications (conditions 3 and 6) and TRACE produced a greater proportion of lexical mis
classifications. In the four phoneme conditions in which errors were made, the final phoneme was created by crosss-splicing two different phonemes, creating input that specified one weakly and the other strongly. The errors are a result of misclassifying t
he phoneme as the more weakly specified (lowercase) alternative (e.g.,  {\i g} instead of {\i B}). By design, Merge\rquote s bottom-up only architecture hightens its sensitivity to sensory information, which in the present simulation made it a bit more biased than TRACE to
 misclassify cross-spliced phonemes.}
\par 
\pard\plain\pard\plain \absw440\absh23\nowrap\pvmrg\posyt\phmrg\posxr{\*\flymaincnt64\flyvert5489\flyhorz5121\flyanchor2\flypage17}\ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033{\field{\*\fldinst \\page\\* ARABIC}{\fldrslt 27}}
\par \pard
{\loch\f0\fs24\lang1033\i0\b0 Lexical misclassification errors in TRACE are due to a bias to respond nonword, which is a bit mysterious. If anything, one would expect the excitatory loop between the phoneme and lexical levels to give TRACE a lexical bias. Note that not all misclassific
ations are due to responding \ldblquote nonword.\rdblquote   In condition 5, they are entirely due to classifying the stimulus as {\i jog}. This also occurred in condition 2, but much less often. Misclassificaitons as jog constituted .08 of the mistmaches for TRACE and .03 for Mer
ge.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 11 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 Although some misclassifications can be legitimized on many grounds (e.g., humans make such errors, perception of ambiguous stimuli will not be constant), it is important to determine whether they are characteristic behaviors of the model or ideosyncratic 
patterns, rather like the \ldblquote particulars\rdblquote  defined in the ALCOVE analysis. That is, it is useful to distinguish between unrepresentiative and representative behaviors. To do so, we measured the volumes of all regions identified by the PSP algorithm. As in the
 ALCOVE analysis, a threshold of 1% of the valid volume was adopted to define a meaningful pattern. When this is done, many patterns turn out to be noise and the set of representative patterns is reduced to a handful. For TRACE, 21 of its patterns (3 uniqu
e and 18 common) do not meet this criterion. Four patterns, all common, dominate in volume, together accounting for 99% of the volume (range 3.8 % - 45.2%).  For Merge, the set of dominant patterns is larger, and is split equally between common and unique 
patterns. Thirty six patterns (21 unique and 15 common) fail to reach the 1% criterion. Seven common (range 1.3 - 21.1) and eight unique (1.4 - 15.7) patterns do so and make up 96% of the valid volume. Even with a threshold that eliminated 75% of all patte
rns, the asymmetry in pattern generation between the models is still present (TRACE=4; Merge=15).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The volumes of the representative common patterns are graphed in Figure 12. The numerals in the legend refer to the mismatch distance of each region from the empirical pattern. Most obvious is the fact that the empirical pattern is much larger in TRACE tha
n in Merge (33.1% and 6.8%) and that one mismatching pattern (filled black) dominates in both models (45.2% and 21.1%). This pattern turns out to be one in which there is a bias to classify all stimuli as nonwords. As a group, the eight unique Merge patter
ns mismatch the empirical pattern more than the common patterns. The largest pattern occupies a region of 15.7% (six mismatches), nearly twice the next largest region (8.9%). In this pattern, not only did Merge exhibit the same nonword response bias, but i
t also categorized cross-spliced phonemes as the competing (remnant) phoneme.}
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Figure 12 here}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Reaction Time Analyses}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Up to this point in the analysis we have compared asymmptotic performance of the models, but the time course of processing is equally important, as experimental predictions often hinge on differences in response times between conditions. Connectionist mode
ls are generally evaluated on their ability to classify stimuli at a rate (e.g., number of cycles) that maintains the same ordinal relations across conditions found with RTs. To assess the robustness of simulation performance, parameters can be varied slig
htly and additional simulations can be run to ensure that the model does not violate this ordering (e.g., by producing a reversal of the RT pattern). In their tests of Merge and TRACE, Norris et al (2000) found neither produced an RT reversal. Our test was
 a more exhaustive version of theirs.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The PSP analysis defines for us the region in the parameter space of each model that corresponds to the empirical pattern. When assessing RT performance, what we want to know is whether there are any points in this region (i.e., parameter sets) that yield 
invalid RT patterns, as defined by the ordinal relations among conditions in the experiment (i.e., the RT pattern in the six phonemic and three lexical conditions in Table 3). To perform this analysis, 10,000 points were sampled over the uniform distributi
on of the empirical region. Simulations were then run with each sample and the cycle time at which classification occurred was measured and compared across all conditions. Violations were defined as reversals in cycle times between adjacent conditions (e.g
., condition 1 vs. condition2) in phoneme classification and lexical decision.  Just as Norris et al (2000) reported, we did not find a single reveral between conditions for either model.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Summary}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The PSP analyses reveal that the consequences of splitting the phoneme level in two, which effectively softens lexical influences, produces a slightly more flexible model, one that can of producing more data patterns. This was found when all patterns were 
considered and when those that were deemed most representative were considered. Of more interest is the nature of this additional flexibility; that is, the relationship between the models as a result of the structural change. By splitting the phoneme level
 in two, Merge did not undergo significant transformations. Rather, the model\rquote s behavior was expanded, as the nested relationship shows. Merge retained  many of TRACE\rquote s behaviors (producing most of its patterns) plus acquired new ones. A consequence of thi
s expansion is that the representativeness of these behaviors is considerably different across models, as shown in Figure 12. It is because these are quantitative more so than qualitative that the models are so similar on other dimensions (e.g., frequency 
and types of mismatches, response time relations between conditions).}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The Bottom-up Priority Rule}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 This second comparison of TRACE and Merge was performed to determine how the addition of a bottom-up priority rule in Merge distinguishes it from TRACE. Recall that the two models differ in the sources of information that can initiate phoneme decision maki
ng. In Merge, for a phoneme decision node to become activated (Figure 9), excitation must be initiated from the phoneme input stage (i.e., evidence for the phoneme must be in the acoustic signal). In TRACE, this is not required. Initial activation via top-
down connections from the word stage is possible. For example, having been presented with {\i jo} in {\i job}, excitation from the {\i job }node will feedback and excite the {\i b} node. Inhibitory connections between phoneme nodes makes it possible to observe indirect word-t
o-phoneme inhibition, because the activated {\i b} node will in turn inhibit competing phoneme nodes (e.g., {\i g)}. Thus, word nodes have the ability to excite directly and inhibit indirectly phoneme nodes.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Frauenfelder et al (1990, Experiment 3) tested TRACE\rquote s prediction of indirect word-to-phoneme inhibition. Listeners had to monitor for phonemes that occurred late in multisyllabic words and nonwords. Of interest was whether reaction times to the target pho
neme would slow when listeners heard stimuli that should cause indirect phoneme inhibition. Three conditions are of interest in the present analysis. A word condition (e.g., {\i habi{\ul t}}, with {\i /t/} as the target phoneme) served as a lower bound on responding becau
se lexical and phonemic information should combine to yield fast RTs. A control nonword condition (e.g., {\i mabi{\ul l}}, with {\i /l/ }as the target phoneme) served as a reference against which to measure inhibition. Because {\i mabil }is not a word, there should be no top-d
own lexical facilitation or inhibition when responding to /l/; responses should be based on sensory input alone. In the third, inhibitory condition, listeners heard nonwords like {\i habil}, with {\i l }as the target phoneme. A slowdown in RT relative to the control
 nonword condition is expected if there is in fact inhibition. This is because the first part of the stimulus, {\i habi}, will excite {\i habit}, whose activation should then feed back down and excite /t/, which will then inhibit /l/.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The RT slowdown in the inhibition condition was small and not reliable, a null result which has been interpreted as arguing against word-to-phoneme excitation in TRACE. However, in simulations of indirect inhibition, TRACE\rquote s behavior is not cut and dry, wi
th inhibition being more likely with longer than shorter words (Norris et al, 2000).  In contrast, the bottom-up priority rule in Merge guarantees that it produces consistent performance that never yields inhibition.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 A PSP analysis was performed on TRACE and Merge using the aforementioned experimental setup to gain a deeper understanding of how the priority rule distinguishes Merge from TRACE. TRACE\rquote s variable behavior is a sign that it can generate more data patterns 
than Merge. If this is the case, is the absence of the priority rule the main cause, or is it also due to structural differences between the models?}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 To address this question, we ran PSP analyses on both models with and without the priority rule in what amounted to a 2x2 factorial design, shown in Table 5. The lower left and upper right cells represent the models as originally formulated. This compariso
n serves as a reference from which to understand the contribution of the priority rule and the models\rquote  structures in affecting behavior. Comparisons of results between columns (i.e., models) neutralizes the effects of the priority rule. If the rule is prim
arily responsible for differences in model behavior, then the results for both models should be quite similar when the rule is and is not operational. If differences still remain, then structural differences are also contributing to their diverse behaviors
. In short, these analyses will tell us whether Merge, without its priority rule, behaves like TRACE, and whether TRACE, with the priority rule, mimics MERGE. }
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 Table 5 here}
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Details of the PSP Analysis}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The design of the indirect inhibition experiment is much simpler than the subcategorical mismatch experiment. There are only three conditions and only a single response decision (phonemic). To simulate the experiment, the combination of so few conditions a
nd a simple model design (one lexical and two critical phoneme nodes) would yield so few potential data patterns that the analysis might not provide satisfying answers to our question. We therefore added a lexical decision response (word or nonword) to the
 design on the grounds that listeners would, if asked, accurately categorize each stimulus as a word or nonword. The models should perform similarly. This additional response permitted a more fine-grained analysis and comparison of the models.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Input to the models consisted of three utterances: {\i habit}, {\i mabil}, {\i habil}. {\i habit }was selected to be moderately long (five phonemes) so that indirect inhibition would have a chance to emerge. Both models were modified from the previous test to consist of only 
one lexical node (h{\i abit}) and the appropriate phoneme input/decision nodes, with /t/ and /l/ of most interest because their activation functions were used to test for inhibition. With two classification response, each with two alternatives,  and three stimu
lus conditions, there were a total of 64 possible data patterns (2{{\*\updnprop5800}\up6 3} x 2{{\*\updnprop5800}\up6 3}).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 To examine the effect of the bottom-up priority rule on model performance, phoneme activation parameters were adjusted accordingly in each model prior to running the PSP algorithm. The same two decision rules were again used to assess the generality of res
ults. With two decision rules and two priority rules, the algorithm was run four times on each model. The consistency of the results for each analysis was ascertained by rerunning the algorithm five times. The averaged data are presented below. No more tha
n seven minutes were required to find all patterns in any run.}
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Classification Analyses}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 # of TRACE patterns shrinks across thresholds. # of Merge patterns drops only minimally. This is what was found in the subcat analysis.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Is the reason that both TRACE and Merge produced the human pattern because the strings were not long enough to produce inhibition in TRACE?}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 model equivalence in this testing situation (recognition threshold dependent).}
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par \pard\plain \ltrpar\s9\cf0\ul{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Reaction Time Analyses}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par 
\par \pard\plain \ltrpar\s9\cf0\qc{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b General Discussion}
\par \pard\plain \ltrpar\s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 In the discussion, point out that PSP analyses like this can us determine the generality of a particular behavior. In the case of TRACE and Merge, findings generated in the past have been shown not to generalize fully to new situations, be they new words o
r a scaled-up version of the model. PSP analyses can assess the robustness of findings, and thereby provide researchers with a clearer understanding of what a model can and cannot do, identifying incidental as well as central patterns of the model.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Need to cite Roberts and Pashler (2000; p. 384) as a 2D example of parameter space partitioning. Insert this is as a simple case, easy to solve because of the low dimensionality.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Discuss usefulness of parameter analyses in RT data.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Mention usefulness of a consistency analysis as measure of model behavior.}
\par {\loch\f0\fs24\lang1033\i0\b0 One of the great advantages of PSP is that it allows us to learn what kinds of data patterns a model can produce. Inspection and further analysis of these patterns provides a wealth of information about model behavior.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The problem of PSP is similar in many ways to the problem of nonlinear function optimization or integration. As with PSP, any numerical optimization or integration algorithm works under a set of regularity conditions about the target function, such as cont
inuity, smoothness, stationarity, existence of solution within finite limit and within machine precision, and so on. For this reason, the algorithm is currently limited in scope. For example, a probabilistic model can be analyzed by PSP only if its simulat
ional component can be replaced with a closed-form probability density (or mass) function, so that data patterns are defined on the space of probability distributions. As for the requirement that the range of parameters must be finite, if some unconstraine
d parameters are unavoidable (i.e., plausible data patterns could still be generated from their extreme values), it is recommended to reparameterize the model utilizing log, inverse logistic, or other transformations.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Extend to continuous value data, models.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Extensive experience with a model can, without a doubt, provide one with good intuitions about model behavior. PSP analyses confirm these intuitions, expose their extent, as well as introduce the modeler to additional tendencies and behaviors.}
\par 
\par 
\par \page\pard\plain \ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b References}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Feldman, J. (2000). Minimization of Boolean complexity in human concept learning. {\i Nature, 407}, 630-633.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Kontkanen, P., Myllym{\f4 \'e4}ki, P., Buntine, W., Rissanen, J. & Tirri, H. (in press). An MDL framework for data clustering. To appear in P. Gr{\f4 \'fc}nwald, I. J. Myung & M. A. Pitt (Eds.) {\i Advances in Minimum Description Length: Theory and Applications}. Cambridge, MA: 
MIT Press.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Kruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist model of category learning. {\i Psychological Review}, {\i 99}, 22-44.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Kruschke, J. K. (1993). Three principles for models of category learning. {\i The Psychology of Learning and Motivation 29}, 57\endash 90.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Kruschke, J. K. & Erikson, M. A. (1995). Six principles for models of category learning. Talk presented at the {\i 36th Annual Meeting of the Psychonomic Society}, 10 November 1995, Los Angeles, CA.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Lee, M. D., & Navarro, D. J. (2002). Extending the ALCOVE model of category learning to featural stimulus domains. {\i Psychonomic Bulletin and Review}, {\i 9}, 43-58.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Luce, R. D. (1963). Detection and recognition. In R. D. Luce, R. R. Bush, & E. Galanter (Eds.), {\i Handbook of Mathematical Psychology }(Vol. 1, p. 103-190). New York, NY: Wiley.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Nosofsky, R. M., Gluck, M. A., Palmeri, T. J., McKinley, S. C. & Glauthier, P. (1994). Comparing models of rule-based classification learning: A replication and extension of Shepard, Hovland, and Jenkins. {\i Memory & Cognition 22}, 352\endash 369.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Shepard, R. N. (1987). Toward a universal law of generalization for psychological science. {\i Science}, {\i 237}, 1317-1323.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Shepard, R. N., Hovland, C. L. & Jenkins, H. M. (1961). Learning and memorization of classification. {\i Psychological Monographs 75 }(13), Whole No. 517.}
\par 
\par \sect\sectd\sbknone\pgwsxn12240\pghsxn15840\marglsxn1440\margrsxn1440\margtsxn1723\margbsxn1440\headery1440{\header \pard\plain \s9\cf0{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033 
\par }

{\*\pgdscno2}\sect\sectd\pgwsxn12240\pghsxn15840\marglsxn1440\margrsxn1440\margtsxn1447\margbsxn1440\headery900{\header \pard\plain \s9\cf0\qr{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033{\field{\*\fldinst \\page\\* ARABIC}{\fldrslt 27}}
\par 
\par }
\pard\plain \ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b Appendix A: Additional Details about How the PSP Algorithm Works}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The goal of PSP is to partition a multi-dimensional space of model parameters into an unknown number of regions, each of which by definition corresponds to a unique data pattern the model can produce. Five assumptions are made in its application: (1) regio
ns are contiguous with one another in the sense that a path between any two regions exists {\i within} the partition; (2) the size of the regions changes smoothly in the parameter space, so small regions tend to cluster together; (3) model behavior must be stat
ionary in the sense that a given parameter set always generates a single, fixed data pattern. This means that the boundaries of the region are fixed, not varying every time the model generates a data pattern; (4) the range of the parameter space to be sear
ched must be finite. (5) the data space has been discretized in such a way that the total number of data patterns to be discovered must be finite to the extent that they can be found within a reasonable amount of computing time.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The algorithm begins with a given set of parameters and its corresponding data pattern generated by the model. This initial parameter set serves as the current point in the parameter space from which a nearby point is sampled. The sampling is performed usi
ng a probability distribution, called a jumping distribution, defined over a small hyper-sphere centered at the current point (panel a of Figure 3). Once a candidate sample point is drawn from the jumping distribution, the model is run with the parameter v
alues, and its output is evaluated to determine if the same data pattern as the current one was generated. If so, the candidate point is accepted as the next point of the process. That is, a jump is made successfully, so the jumping distribution moves to a
 new location, being ready to sample another candidate point. If the candidate point does not yield the same pattern as the current one, it is rejected as belonging to the current region. That is, the jump trial is unsuccessful and the jumping distribution
 stays at the current location to sample again on the next trial. The sequence of such locations of the jumping distribution, recorded over {\i all }trials, is called the Markov chain corresponding to the current data pattern. The theory of MCMC guarantees that
 the Markov chain will eventually yield a uniformly distributed sample over the region, regardless of its size or shape.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 While the Markov chain runs inside the current region, every rejected point, which must be outside the region, is checked to see if it generates a new valid data pattern. If it does, the point is taken to initiate a new Markov chain corresponding to the ne
wly discovered data pattern. In effect, accepted points are used to shift the jumping distribution inside the current region whereas rejected points are used to discover new regions contiguous with the current one. Each time a new data pattern is found, a 
new MCMC-based search process is spawned. Over time, as many Markov chains run simultaneously as there are data patterns found. Consequently, the search takes place along the boundaries of all the discovered regions, to find yet more regions (i.e., additio
nal data patterns).}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The size of the jumping distribution (i.e., the radius of the hyper-sphere) in each region must adapt to its size and shape. If it is too small, almost all candidate points will be accepted, but every jump will be so small that it will take too many jumps 
for an exhaustive search of a region. Also, rejected points will rarely be generated. In contrast, if the size of the jumping distribution is too large, candidate points will be rejected too often, and the granularity of the jumps will not be small enough 
to define the edges of a region, which requires a properly sized jumping distribution to succeed. Unless one is dealing with a normal distribution, no theory exists that defines the optimal jumping distribution. With the PSP algorithm, we have found it bes
t to use an adaptive jumping distribution, which on average accecpts 20% of sample points.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The PSP algorithm can be summarized roughly:}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\loch\f5\fs24\lang1033\i0\b0{\f5{\fs24 \tab }}}{\f5\f5\f5{\loch\f5\fs20\lang1033{\fs22 Given{\field{\*\fldinst ADVANCE \\d 7}{\fldrslt }}}{\*\flyinpara
\pard\plain\absw162\absh244\pvpara\posyt{\*\flyvert20385\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_65bcc57f.jpg}}{\fldrslt }}}
\pard
}{\fs24 }}}{\fs24\fs24\fs24{\loch\f0\fs24\lang1033{\field{\*\fldinst ADVANCE \\u 7}{\fldrslt }} and Pattern 1,}{\fs22\f5  set {\i m} = {\i i} = 1.}}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\li2160\ri0\lin2160\rin0\fi-3600\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\loch\f5\fs24\lang1033\i0\b0{\f5{\fs24 \tab \tab }}}{\f5\f5\f5{\loch\f5\fs20\lang1033{\fs22{\i Step 1.\tab }}{\fs22\fs22\fs22Establish {\field{\*\fldinst ADVANCE \\d 6}{\fldrslt }}}{\*\flyinpara
\pard\plain\absw462\absh210\pvpara\posyt{\*\flyvert20417\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m2340bd3f.jpg}}{\fldrslt }}}
\pard
}{\fs24 }}}{\loch\f0\fs20\lang1033{\field{\*\fldinst ADVANCE \\u 6}{\fldrslt }}{\fs22  by adapting the size of its hyper-spherical domain. Go to Step 2.}}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\li2160\ri0\lin2160\rin0\fi-2880\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\loch\f0\fs22\lang1033\i0\b0\fs22  \tab {\f5{\i \tab Step 2.\tab }}{\f5\f5\f5Set {\i i }= mod({\i i}, {\i m}) + 1. Go to Step 3.}}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\li2160\ri0\lin2160\rin0\fi-2160\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\loch\f5\fs22\lang1033\i\b0{\ulnone{\f5{\fs22{\i Step 3.\tab }}}}}{\fs22\fs22\fs22{\f5\f5\f5{\ulnone{\loch\f5\fs22\lang1033Sample {\field{\*\fldinst ADVANCE \\d 8}{\fldrslt }}}}}}{\f5\f5\f5{\ulnone{\loch\f5\fs20\lang1033{\*\flyinpara
\pard\plain\absw189\absh257\pvpara\posyt{\*\flyvert27201\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m1ddabed.jpg}}{\fldrslt }}}
\pard
}{\fs24 }}}}{\ulnone{\loch\f0\fs20\lang1033{\field{\*\fldinst ADVANCE \\u 8}{\fldrslt }}{\fs22  from {\field{\*\fldinst ADVANCE \\d 6}{\fldrslt }}}}}{\loch\f0\fs20\lang1033{\*\flyinpara
\pard\plain\absw503\absh224\pvpara\posyt{\*\flyvert38433\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_6c8d1ab5.jpg}}{\fldrslt }}}
\pard
}{\fs24 {\field{\*\fldinst ADVANCE \\u 6}{\fldrslt }}. If {\field{\*\fldinst ADVANCE \\d 8}{\fldrslt }}{\*\flyinpara
\pard\plain\absw189\absh257\pvpara\posyt{\*\flyvert27201\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m1ddabed.jpg}}{\fldrslt }}}
\pard
}{\field{\*\fldinst ADVANCE \\u 8}{\fldrslt }}}{\f5{\fs22  generates a new valid pattern, set {\i m} = {\i m} + 1, {\field{\*\fldinst ADVANCE \\d 7}{\fldrslt }}}}{\f5\f5\f5{\*\flyinpara
\pard\plain\absw517\absh231\pvpara\posyt{\*\flyvert35745\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m24f7a881.jpg}}{\fldrslt }}}
\pard
}{\fs24 }}{\fs24\fs24\fs24{\field{\*\fldinst ADVANCE \\u 7}{\fldrslt }}}{\f5{\fs22 , and record the new pattern as Pattern {\i m}, and then go to Step 1. If {\field{\*\fldinst ADVANCE \\d 8}{\fldrslt }}}}{\f5\f5\f5{\*\flyinpara
\pard\plain\absw189\absh257\pvpara\posyt{\*\flyvert27201\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m1ddabed.jpg}}{\fldrslt }}}
\pard
}{\fs24 }}{\fs24\fs24\fs24{\field{\*\fldinst ADVANCE \\u 8}{\fldrslt }} generates Pattern }{\f5{\fs22{\i i}}}{\fs22\fs22\fs22{\f5\f5\f5, set {\field{\*\fldinst ADVANCE \\d 8}{\fldrslt }}}}{\f5\f5\f5{\*\flyinpara
\pard\plain\absw475\absh257\pvpara\posyt{\*\flyvert27201\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m404acc02.jpg}}{\fldrslt }}}
\pard
}{\fs24 }}{\fs24\fs24\fs24{\field{\*\fldinst ADVANCE \\u 8}{\fldrslt }} and go to Step 2.}{\fs22\f5  Otherwise, go to step 2.}}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1033{\loch\f0\fs24\lang1033\i0\b0\fs24{\ulnone Note that {\field{\*\fldinst ADVANCE \\d 7}{\fldrslt }}{\*\flyinpara
\pard\plain\absw528\absh244\pvpara\posyt{\*\flyvert20385\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_323d5a3a.jpg}}{\fldrslt }}}
\pard
}{\field{\*\fldinst ADVANCE \\u 7}{\fldrslt }} is the jumping distribution of the region corresponding to Pattern {\i i}, whose center is at {\field{\*\fldinst ADVANCE \\d 7}{\fldrslt }}{\*\flyinpara
\pard\plain\absw162\absh244\pvpara\posyt{\*\flyvert20385\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m7f68876f.jpg}}{\fldrslt }}}
\pard
}{\field{\*\fldinst ADVANCE \\u 7}{\fldrslt }}, and that {\i  i {\field{\*\fldinst ADVANCE \\d 6}{\fldrslt }}{\*\flyinpara
\pard\plain\absw705\absh216\pvpara\posyt{\*\flyvert40929\flyanchor1\flycntnt}{{\field\fldpriv{\*\fldinst{\\import intro2_rtf_m245744cf.jpg}}{\fldrslt }}}
\pard
}}}}{\loch\f0\fs24\lang1033\fs24\fs24\fs24{\field{\*\fldinst ADVANCE \\u 6}{\fldrslt }}indexes the region from which we are currently sampling, and {\i m} represents the number of regions (or number of data patterns) found so
 far.}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par {\loch\f0\fs24\lang1033\i0\b0 The search process terminates if the following two conditions are met. First, a certain preset size of MCMC samples is obtained for each of the discovered regions. To compensate for the fact that regions discovered early in the search process are likely to
 be sampled more than regions discovered later, the algorithm concentrates more on the newly discovered regions in such a way that the total number of trials in the search history will eventually be the same for all discovered regions. Second, if a new pat
tern is not discovered after a set number of trials (or time), the algorithm terminates.}
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The PSP algorithm also has the desirable property of focusing on each region in equal proportion, irrespective of its size. An equal number of search trials is performed in each region. As a consequence, closer attention is paid to the small regions. This 
means that the resulting sampling distribution over the whole parameter space is essentially a mixture distribution that gives higher density to points known to lie near many regions.}
\par 
\par 
\par \page\pard\plain \ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b Appendix B}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 The weak order that defines a data pattern in the ALCOVE example can be decomposed into a set of equivalence relations, and a strong order on the equivalence classes. The strong order part is easy. We simply rank them in terms of mean learning rate. The eq
uivalence relations are more difficult to derive, requiring that we partition the six curves using a suitable clustering procedure. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 The clustering procedure we employed was a minor variant on the clustering technique introduced by Kontkanen, Myllym{\f4 \'e4}ki, Buntine, Rissanen, and Tirri (in press). The essence of the technique is to view a clustering solution as a probabilistic model for the
 data. In the current application, the likelihood function for the data takes the form of a mixture of binomials, with a single multivariate binomial for each cluster. The clustering procedure now reduces to a statistical inference problem, which is solved
 by choosing the set of clusters that optimizes a Minimum Description Length statistic. The six learning curves reported by Nosofsky et al. (1994) are averaged across 40 subjects over the first 16 blocks, consisting of 16 stimulus presentations each. Each 
data point is thus pooled across 40 x 16 = 640 trials. Using this technique it is possible to infer that I<II<(III, IV, V)<VI is indeed the natural structure for these data. }
\par 
\par {\loch\f0\fs24\lang1033\i0\b0 Lastly, we need to be able to associate a set of {\i predicted} learning curves with a data pattern, which is not the same thing as associating a set of {\i observed} learning curves with a data pattern. Nevertheless, it is not difficult to do. A set of response pro
babilities is first discretized to the same resolution as the empirical data. This is straightforward, by finding the expected values for the data, given by {\i np}, where {\i n} is the sample size and {\i p} is the average response probability predicted for some categor
y type across any given block of trials, and then rounding to the nearest integer. While the rounding error is a nuisance, it is negligible for {\i n}=640. The discretized curves are then mapped onto a qualitative data pattern by using the same clustering techn
ique used to classify the empirical data.}
\par 
\par \page\pard\plain \ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ab\ltrch\dbch\af0\afs24\langfe1033\ab\loch\f0\fs24\lang1033\b {\loch\f0\fs24\lang1033\i0\b Appendix C: Parameters used for each model}
\par \pard\plain \ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 
\par \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx5513\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx6795\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx8077\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Parameter}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 TRACE}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Merge}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Range}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx5513\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx6795\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx8077\clbrdrt\brdrs\brdrw1\brdrcf1\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Phoneme excitation}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Phoneme to word excitation}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Phoneme to word inhibition}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Phoneme to target excitation}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Phoneme decay}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Wored to target excitation}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Word to phoneme excitation}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Word to word inhibition}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Word decay}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Target to target inhibition}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Target decay}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Target momentum}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clvertalc\cellx5513\clvertalc\cellx6795\clvertalc\cellx8077\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Phoneme to phoneme inhibition}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 -}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 (0,1)}
\cell\row\pard \trowd\trql\trleft1440\trpaddft3\trpaddt0\trpaddfl3\trpaddl28\trpaddfb3\trpaddb0\trpaddfr3\trpaddr28\clbrdrb\brdrs\brdrw1\brdrcf1\clvertalc\cellx5513\clbrdrb\brdrs\brdrw1\brdrcf1\clvertalc\cellx6795\clbrdrb\brdrs\brdrw1\brdrcf1\clvertalc\cellx8077\clbrdrb\brdrs\brdrw1\brdrcf1\clvertalc\cellx9360
\pard\intbl\pard\plain \intbl\ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\ql\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Cycles per input slice}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ai\ltrch\dbch\af0\afs24\langfe1033\ai\loch\f0\fs24\lang1033\i {\loch\f0\fs24\lang1033\i\b0 V}
\cell\pard\plain \intbl\ltrpar\s9\cf0\qc{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\sb12\sa6\rtlch\af0\afs24\lang1033\ltrch\dbch\af0\afs24\langfe1033\loch\f0\fs24\lang1033 {\loch\f0\fs24\lang1033\i0\b0 Fixed}
\cell\row\pard \pard\plain \ltrpar\s9\cf0\qj{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\rtlch\af0\afs20\lang1033\ltrch\dbch\af0\afs20\langfe1033\loch\f0\fs20\lang1042 
\par {\loch\f0\fs20\lang1042\i0\b0 Note: word to word inhibition does not apply to models in Frauenfelder design. Cycles per input slice were fixed to 15 for models in subphonemic mismatch design, and 3 for models in Frauenfelder design.}
\par \pard\plain \ltrpar\s9\cf0{\*\tlswg8236}\tx0{\*\tlswg8236}\tx720{\*\tlswg8236}\tx1440{\*\tlswg8236}\tx2160{\*\tlswg8236}\tx2880{\*\tlswg8236}\tx3600{\*\tlswg8236}\tx4320{\*\tlswg8236}\tx5040{\*\tlswg8236}\tx5760{\*\tlswg8236}\tx6480{\*\tlswg8236}\tx7200{\*\tlswg8236}\tx7920{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\ql\rtlch\af7\afs20\lang1033\ltrch\dbch\af7\afs20\langfe1033\loch\f7\fs20\lang1042 
\par }