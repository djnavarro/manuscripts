% !TEX root = DataDrivenBayes.tex

\section*{Appendix A: Details for the Bayesian gamblers problems}

In the main text we describe a gambling contest to determine which of three Bayesian agents holds ``better'' beliefs about a simple binary prediction task (e.g., whether the next card drawn from a deck will be black or white). The {\it veridical Bayesian} assumes---correctly, as it turns out---that arrivals are independent, and that there is some unknown probability $\theta = P(B)$ with which any given card will be black. Moreover, this Bayesian decides that they have no knowledge about $\theta$ and---again, correctly, as it turns out---places a uniform prior over this quantity $P(\theta) \propto 1$. The {\it misinformed Bayesian} also assumes that outcomes are independent, but adopts a prior that favours black  $P(\theta) \propto \theta$. Finally, the {\it miscalibrated Bayesian} correctly adopts a uniform prior over $\theta$ but incorrectly assumes that the arrivals will be ``streaky'', with successive arrivals tending to be of the same color. Specifically, the probability that the next card is black is judged to be $(\theta+1)/2$ if the last card was also black, but only $\theta/2$ if the last card was white. Formally, this likelihood arises from a first order Markov chain in which the marginal probability of black is fixed at $\theta$, but the probability that successive cards will be of different colors is only $\theta(1-\theta)$ rather than $2\theta(1-\theta)$ as would be expected if the outcomes were independent Bernoulli trials with probability $\theta$.

To convert this scenario into a gambling problem we suppose that every agent offers bets that they believe to be fair, and places a \$1 bet every time another agent offers a bet that they believe to be favorable. Given a starting stake of \$100, Figure~\ref{fig:threeBayes} tracks the relative fortunes of all three Bayesians, averaged across 10,000 repeats of the betting game. Critically, the game is structured to match the assumptions made by the first Bayesian: The true probability of a black card $\theta$ is generated uniformly at random on each repeat of the game, and the outcomes on every trial are generated independently on each trial. As the figure illustrates, the three Bayesians perform very differently on this problem: The misinformed Bayesian fares very poorly, and quickly loses money to the other two. The streaky Bayesian initially does well despite the miscalibrated likelihood but the veridical model tends to win out in the long run by capitalizing on the streaky model's tendency to expect too many repetitions and not enough alternations among the outcomes.  

\section*{Appendix B: Details for the coincidences example}

In the original binary-data model for the coincidences task presented by GT1, the learner is told about a sample containing $n$ binary observations, of which $k$ are ``successes''. If the learner assumes the data represent the outcomes of $n$ independent Bernoulli trials, then the model described by Equations~\ref{eq:gt1-likelihood} and~\ref{eq:gt1-posterior} results. In our ``conservative'' version of the model the learner acts as if the effective sample size consisted of only $n^\prime$ observations, where $n^\prime \leq n$, and similarly assumes that there were only $k^\prime$ succeses where $k^\prime \leq k$. Thus the predictions of our model can be obtained by applying Equations~\ref{eq:gt1-likelihood} and~\ref{eq:gt1-posterior} to a smaller sample in which $k^\prime$ successes from $n^\prime$ trials are observed.

To formalize this in terms of a probabilistic model, we imagine that the learner ``retains'' only a subset of the original observations, where the probability that a specific observation is retained is denoted $\theta$. This model implies that the number of success observations retained $k^\prime$ and the number of non-success observations $n^\prime - k^\prime$ are both binomially distributed:
\begin{equation}
\begin{array}{rcl}
k^\prime & \sim & \mbox{Binomial}(\theta,k) \\
n^\prime - k^\prime &\sim& \mbox{Binomial}(\theta,n-k)
\end{array}
\end{equation}
Thus the full model has two free parameters to describe the response curve for an individual participant: $\theta$ captures the degree of conservatism (i.e., the extent to which data causes the learner to adjust his or her beliefs), and as per the original GT1 model, $\phi$ captures the prior degree of belief that the learner places in the alternative hypothesis (i.e., that a real effect exists). More precisely $\phi = \log P(h_1)/P(h_0)$ denotes the logarithm of the prior odds for the alternative hypothesis over the null.

The output of this model is the posterior probability $P(h_1 | n, k, \theta, \phi)$, the probability that the alternative hypothesis is true (according to the learner) given that observations $k$ out of $n$ successes were observed, given the learner's priors $\phi$ and likelihood $\theta$. If we let $e = P(h_1 | n, k, \theta, \phi)$ denote the extent of this evidence, then we assume that the actual response $r$ given by the participant is equal to this evidentiary value $e$ plus some normally distributed error: 
\begin{equation}
r \sim \mbox{Normal}(e,\sigma_1^2)
\end{equation}
In our applications we defined the researcher prior over the response variability in terms of the precision (i.e., $\tau_1=1/\sigma_1^2$) and placed a diffuse prior over it, namely a $\mbox{Gamma}(.001,.001)$. 

In order to specify a model that allows us to capture indvidual differences, we adopt a hierarchical Bayesian approach. We assume that each participant has unique value of $\theta$ and $\phi$, where these parameters are sampled from group level distributions: 
\begin{equation}
\begin{array}{rcl}
\theta & \sim & \mbox{Beta}(a,b) \\
\phi & \sim & \mbox{Normal}(\mu,\sigma_2^2) 
\end{array}
\end{equation} 
The researcher prior over $a$ and $b$ is an exponential distribution with scale parameter 1. The prior over $\mu$ was a normal distribution with mean 0 and standard deviation 100, and the prior over the variability parameter $\sigma_2^2$ was again defined in terms of the precision, $\tau_2 = 1/\sigma_2^2$, where our prior over $\tau_2$ was again a $\mbox{Gamma}(.001,.001)$ distribution. We assume that different group level distributions exist for each of the cover stories. We implemented this as a graphical model in JAGS, allowing us to estimate the posterior distribution over $\theta$ and $\phi$ for each subject, as well as obtaining estimates of the group level parameters $a$, $b$, $\mu$, and $\sigma_2$. It is this model reported in the main text.

\section*{Appendix C: Details for the optimal predictions example}

The descriptive Bayesian model in the second case study uses the same likelihood function as the original model from GT2, but treats the participant prior as an unknown variable to be inferred from the data. To that end we assume that the learner's prior could be a normal distribution, an erlang distribution or a pareto distribution, and place a uniform (researcher) prior across these three possibilities:
\begin{equation}
t \sim \left\{ 
  \begin{array}{l l}
    \mbox{Normal}( \mu , \sigma ) & \quad \text{if $c=1$}\\
    \mbox{Erlang}( \beta ) & \quad \text{if $c=2$}\\
    \mbox{Pareto}( \gamma) & \quad \text{if $c=3$}
  \end{array} \right.
\end{equation}
where we (as researchers) place a uniform prior over $c$ and diffuse priors over the parameters for all three distributional families. Specifically our priors over the parameters are given by $\mu \sim \mbox{HalfNormal}(.0001)$ and $\sigma \sim \mbox{Uniform}(0,1000)$ for the normal distribution; $\beta \sim  \mbox{Uniform}(0,1000)$ for the erlang model; and $\gamma \sim \mbox{Gamma}(.1,.1)$ for the pareto distribution. 

As noted in the main text, for any specific choice of paricipant prior the Bayesian cognitive model (both the descriptive model and the original optimal model from GT2) produces a paricipant posterior $P(t|x)$ corresponding to the learner's belief about the likely duration/extent of the unknown quantity. In order to convert this to a full probabilistic model for the participant response $r$ we assume that people sample the response from their posterior distribution. We implemented this model with a custom sampler in PyMC in order to estimate the posterior predictive estimates for the responses to each condition reported in Figure~\ref{fig:emp_vs_mink_agg_predictive}, as well as for the participant priors themselves (in Figure~\ref{fig:predictions-priors-subjective-vs-empirical}). 

The Noisy \mink model is probabilistic extension of the \mink heuristic. The only difference in the new generative model is that instead of assuming that people report the value of $t$ that is produced by the \mink heuristic, it suggests that the participant samples their response from a normal distribution that is centered on that value. If $t^*$ denotes the value predicted by the deterministic \mink model, the Noisy \mink model predicts that people sample from 
\begin{equation}
t \sim \mbox{Normal}(t^*,\sigma^2).
\end{equation}
However, since people never report values of $t$ that are below the observed value $x$, the actual response distribution is truncated at $x$. Moreover, instead of assuming that the experimenter guesses the value of the multiplier $g$, we fold it into the experimenter's model. That is, we specify a prior that captures our actual prior beliefs about the value of $g$, 
\begin{equation}
g \sim \mathrm{Uniform}(0,3)
\end{equation}
and seek to infer the actual value of $g$ from the empirical data. As with the two Bayesian cognitive models, we implemented the model in PyMC and applied a custom sampler to infer posterior distributions over the model parameters $g$ and $\sigma$ as well as the specific exemplars people used to estimate $t^*$.

\section{Appendix D: Details for the generalization example}

As outlined in the main text, the Bayesian generalization model specified by TG1 specifies a {\it generalization probability}. When told that a set of items possesses some property, the generalization probability is the chance that a novel item shares that property. Given a binary matrix of category assignments $\mathbf{C}$---such that $c_{ix} = 1$ if item $i$ belongs to category $x$ and $c_{ix}=0$ if it does not---the simplest version of the model assumes that the extension of the novel property is to a single category. Under this model, the probability of generalizing a property from item $j$ to item $i$ is denoted $g(i|j)$, 
\begin{equation}
\begin{array}{rcl}
g(i|j) &=& \sum_x P(c_{ix}=1) P(x | j)  \\
&=& \sum_{x | c_{ix}=1} P(x | j)
\end{array}
\end{equation}
where $P(x|j)$ is the posterior probability that category $x$ is the true extension of the novel property given that the item $j$ is known to possess the property, and 
\begin{equation}
P(x|j) \propto P(j|x) P(x)
\end{equation}
The prior distribution $P(x)$ is captured by a vector of weights $\mathbf{w}$ such that $w_x \geq 0$ and $\sum_x w_x = 1$. As discussed in the main text, the likelihood function is a weighted mixture of the strong sampling model and the weak sampling model, so the probability that item $j$ would have been generated if $x$ were the true hypothesis is given 
\begin{equation}
P(j|x) = (1-\theta) \frac{1}{n} + \theta \frac{c_{jx}}{n_x}
\end{equation}
where $n_x = \sum_j c_{jx}$ counts the number of item is the $x$-th category, $n$ is the total number of items in the domain, and $\theta$ is a model parameter that specifies which sampling model the learner relies upon: $\theta=0$ implies weak sampling and $\theta=1$ is strong sampling.

The full model implemented in the paper extends this basic model in four respects. Firstly, following TG1 we allow generalization from multiple exemplars, so if the learner has been told that items $j$ and $k$ both possess the novel property, the posterior probability of category $x$ is given by
\begin{equation}
P(x|j,k) \propto P(j|x) P(k|x) P(x)
\end{equation}

Secondly, the ``base'' category matrix $\mathbf{C}$ is augmented by one ``universal'' category (to which all items belong) and $n$ ``singleton'' categories (each containing only one item). So if $\mathbf{C}$ is an $n \times m$ binary matrix specifying the memberships for $m$ categories, then the model actually has $m+n+1$ categories once the universal and singleton categories are added. As such the weights vector $\mathbf{w}$ that specifies the prior distribution over categories has length $m+n+1$.

The third extension allows the learner to construct a more elaborate hypothesis space $\mathcal{H}$ from the base representation defined by $\mathbf{C}$ and $\mathbf{w}$. Instead of assuming that the extension of the unknown property is necessarily restricted to a single category, the learner also consider the possibility that the property is possessed by the members of two categories (i.e., the intersection of two categories in $\mathbf{C}$. We operationalize this in terms of an expanded hypothesis matrix $\mathbf{H}$ that contains a copy of every element in $\mathbf{C}$ as well as an additional column for every pair of columns in $\mathbf{C}$, and whose elements are 1 if {\it either} of the original columns has a 1 in the corresponding location. The prior probability of any such ``composite'' hypothesis is computed from the weights vector $\mathbf{w}$ in the following way. If hypothesis $z$ is the union of categories $x$ and $y$ then
\begin{equation}
P(z) \propto \gamma w_x w_y
\end{equation}
and similarly if hypothesis $z$ is a ``primitive'' hypothesis that corresponds to category $x$ only then 
\begin{equation}
P(z) \propto (1-\gamma) w_x
\end{equation}
 
Finally, in order to assign probability to responses at an individual trial level we assume that the raw response is sample from a normal distribution whose mean corresponds to the model-predicted generalization probability, $g(i|j)$ or $g(i|j,k)$, and whose variance $\sigma^2$ is unknown.   
 
The full model is requires that we infer an $n \times m$ binary matrix $\mathbf{C}$ for the category assignments, a vector of $n+m+1$ non-negative weights $\mathbf{w}$ for the $m$ base categories, the $n$ singleton categories and the 1 universal categories (though this corresponds only to $n+m$ unknowns as these must sum to 1), the parameter $\theta$ that defines the sampling model and the parameter $\gamma$ that indicates the relative weights assigned to ``primitive'' versus ``composite'' hypotheses. To perform inference in this model we specify researcher priors for $\theta$ and $\gamma$ that are uniform across the unit interval, uniform priors across all possible binary matrices $\mathbf{C}$ (for a fixed value of $m$) and uniform across weight vectors $\mathbf{w}$. We used a simulated annealing algorithm to find the best fitting (i.e., maximum a posteriori) values for the theoretically relevant parameters $\mathbf{C}$, $\mathbf{w}$, $\theta$ and $\gamma$. Following \citeA{tenenbaum_learning_1996} we treated $\sigma^2$ as a nuisance parameter, and can be used as a de facto temperature parameter in a simulated annealing algorithm by initalizing $\sigma^2$ at a large value and gradually reducing it. The results reported in the main text were the result of an application of the simulated annealing procedure with $m=8$.

It should be noted that unlike the other two cases studies we did not do full Bayesian inference for this model. What we have reported is a point estimate (in effect the Bayesian MAP estimate) for the theoretically important variables, rather than estimating the full posterior distribution over all variables. The reason for this is partly that it is more tractable to compute the point estimate, though not impossible: We did also implement a fully Bayesian version of a restricted model that more closely resembles the approach in \citeA{navarro2008}, and found it worked reasonably well. The more important reason is that the results are somewhat more interpretable when we have a {\it single} set of categories $\mathbf{C}$, rather than a full posterior distribution over possible category assignment matrices. The former can be described in a table, the latter is difficult to summarize, though \citeA{navarro2008} do offer suggestions for how to do so.
